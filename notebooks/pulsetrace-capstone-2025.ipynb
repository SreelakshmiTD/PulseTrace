{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üîç PulseTrace ‚Äî Multi-Agent Root Cause Analysis for Data Pipelines\n\nPulseTrace automates how data engineers investigate failing pipelines using a **coordinated multi-agent system**.  \nInstead of manually scanning logs, schemas, lineage, and past incidents, PulseTrace orchestrates:\n\n- **Detector Agent** ‚Äî detects failures and triggers analysis  \n- **Diagnoser Agent** ‚Äî fetches logs, schema diffs, sample data, and builds incident signatures  \n- **History Analyzer Agent** ‚Äî matches incidents against the memory bank  \n- **Impact Analyzer Agent** ‚Äî determines downstream blast radius using lineage  \n- **Advisor Agent** ‚Äî synthesizes the final RCA report and fix recommendations  \n\n\n## üîß Technology Implemented Behind PulseTrace\n\nPulseTrace demonstrates:\n- a **multi-agent workflow**  \n- **asynchronous agent-to-agent (A2A) messaging** using an in-memory bus  \n- custom diagnostic tools  \n- observability via `TRACE_STORE`  \n- per-incident session state (`SESSIONS`)  \n- historical memory bank (`MEMORY_BANK`)  \n- hybrid deterministic + LLM reasoning  \n- human confirmation steps  \n","metadata":{}},{"cell_type":"markdown","source":"## üìò Notebook Roadmap\n\n- üîß Setup & environment checks (offline mode + optional Gemini)\n- ‚ñ∂Ô∏è How to Run This Notebook\n- üß≠ Architecture overview & how the agents interact  \n- ü§ñ Agent implementations (Diagnoser, History Analyzer, Impact Analyzer, Advisor)  \n- ‚ñ∂Ô∏è Orchestrator: run an end-to-end RCA demo using offline samples\n- üì° Observability: traces, sessions, memory bank \n- üé® Interactive UI (ipywidgets) ‚Äî upload logs, run RCA, approve & save reports  \n- üõ† Troubleshooting & diagnostics\n- ‚ö†Ô∏è Current Limitations\n- üöÄ What's Next steps for PulseTrace\n- üìù Conclusion  \n","metadata":{}},{"cell_type":"markdown","source":"## üöÄ How to Run PulseTrace  \n\nRunning PulseTrace is simple and intentionally structured to feel smooth end-to-end.  \nFollow these steps, and you‚Äôll have the full RCA pipeline running in minutes.\n\n### **1. Run the Setup Cells**  \nStart at the top of the notebook and execute each setup cell in order.  \nThese cells:  \n- Prepare the environment  \n- Register all agents  \n- Load synthetic log samples  \n- Initialize helper utilities  \n\nYou‚Äôll see clear confirmation messages as components are loaded.\n\n### **2. (Optional) Execute the Demo Flow**  \nFind the cell titled **‚ÄúRun demo & quick validation.‚Äù**  \nRunning it gives you a quick sanity check:  \n- A complete RCA workflow runs automatically  \n- Traces and sessions are generated  \n- A draft report is produced and validated  \n\nThis helps confirm everything is wired correctly.\n\n### **3. Use the Interactive UI**  \nScroll to the section titled **‚ÄúInteractive UI (ipywidgets)‚Äù**.  \nHere you can:  \n- Upload your own log file  \n- Or choose a demo sample  \n- Click **Run Diagnosis** to execute the full RCA pipeline  \n- Inspect **Traces**, **Sessions**, and **Memory Bank** live  \n- Approve and save reports as needed  \n\nThe UI is designed to be fast, clear, and beginner-friendly.\n\n\n### **4. Inspect Internal Activity (Optional)**  \nUse the **Observability** section to view:  \n- Agent-to-Agent event traces  \n- Active sessions  \n- Memory bank summaries  \n\nThese tools help verify‚Äîand showcase‚Äîhow the pipeline behaves under the hood.\n\n\n### **5. Explore Further Sections**  \nThe notebook ends with:  \n- Troubleshooting  \n- Current limitations  \n- Next steps  \n- A clean conclusion  \n\n**You're ready to run PulseTrace.  \nFollow the notebook from top to bottom, and the workflow will run smoothly end-to-end.**\n","metadata":{}},{"cell_type":"markdown","source":"## üîß Environment Setup & Mode Detection\n\nThis cell initializes the core environment for PulseTrace:\n- loads helper libraries  \n- prepares global state (sessions, trace store, memory bank)  \n- checks whether Gemini is available  \n- configures hybrid mode automatically (fallback-safe)  \n- detects UI support (ipywidgets)\n","metadata":{}},{"cell_type":"markdown","source":"## (1) Optional: Install ipywidgets (only if missing)\nThis cell checks whether `ipywidgets` is installed and installs it only if required.\n","metadata":{}},{"cell_type":"code","source":"# Cell 1 ‚Äî install ipywidgets if missing (with readable debug logs)\nprint(\"üîß Checking for ipywidgets...\")\n\ntry:\n    import ipywidgets\n    print(\"‚úî ipywidgets is already installed.\")\nexcept ImportError:\n    print(\"‚ö† ipywidgets not installed. Installing ipywidgets==7.7.1 ...\")\n    import sys\n    !{sys.executable} -m pip install ipywidgets==7.7.1\n\n    print(\"‚è≥ Installation attempted. A kernel restart may be required depending on the environment.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:46.249650Z","iopub.execute_input":"2025-11-26T10:59:46.249960Z","iopub.status.idle":"2025-11-26T10:59:46.332749Z","shell.execute_reply.started":"2025-11-26T10:59:46.249934Z","shell.execute_reply":"2025-11-26T10:59:46.331774Z"}},"outputs":[{"name":"stdout","text":"üîß Checking for ipywidgets...\n‚úî ipywidgets is already installed.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## (2) Optional: Google API key setup (Kaggle Secrets)\nTries to load `GOOGLE_API_KEY`. If found, saves it to the environment for Gemini.\n","metadata":{}},{"cell_type":"code","source":"# Cell 2 ‚Äî Kaggle secret loader (with readable status messages)\nprint(\"üîë Checking for GOOGLE_API_KEY in Kaggle Secrets...\")\n\nimport os\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n\n    key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n\n    if key:\n        os.environ[\"GOOGLE_API_KEY\"] = key\n        print(\"‚úî GOOGLE_API_KEY successfully loaded into environment.\")\n        print(\"üîí Key preview:\", key[:6] + \"******\")\n    else:\n        print(\"‚ö† GOOGLE_API_KEY not found in Kaggle Secrets. Continuing in offline mode.\")\n\nexcept Exception as e:\n    print(\"‚ö† Could not load GOOGLE_API_KEY from Kaggle Secrets.\")\n    print(\"   Reason:\", e)\n    print(\"   Proceeding in offline mode.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:46.335665Z","iopub.execute_input":"2025-11-26T10:59:46.335967Z","iopub.status.idle":"2025-11-26T10:59:46.412464Z","shell.execute_reply.started":"2025-11-26T10:59:46.335943Z","shell.execute_reply":"2025-11-26T10:59:46.411636Z"}},"outputs":[{"name":"stdout","text":"üîë Checking for GOOGLE_API_KEY in Kaggle Secrets...\n‚ö† Could not load GOOGLE_API_KEY from Kaggle Secrets.\n   Reason: Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 102460684 and label GOOGLE_API_KEY.'], 'error': {'code': 5}, 'wasSuccessful': False}.\n   Proceeding in offline mode.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## (3) Environment Summary & Mode Detection\nDetects:\n- Gemini availability  \n- ipywidgets UI availability  \n- Prints a clean summary block\n","metadata":{}},{"cell_type":"code","source":"# Cell 3 ‚Äî Environment summary & detection with detailed print messages\nprint(\"üåê Initializing environment summary...\")\n\n# Detect Gemini availability\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\nUSE_GEMINI = bool(GOOGLE_API_KEY)\n\n# Detect UI availability\ntry:\n    import ipywidgets as widgets\n    from IPython.display import display, clear_output\n    UI_AVAILABLE = True\nexcept Exception:\n    UI_AVAILABLE = False\n\n# Print summary (with fallbacks if rich is not installed)\nsummary_lines = [\n    \"\\n=== PulseTrace Environment Summary ===\",\n    f\"Gemini Mode: {'ON (API key detected)' if USE_GEMINI else 'OFF (no API key found)'}\",\n    f\"UI Mode: {'ENABLED (ipywidgets available)' if UI_AVAILABLE else 'UNAVAILABLE'}\",\n]\n\ntry:\n    from rich import print as rprint\n    for line in summary_lines:\n        rprint(line)\nexcept Exception:\n    for line in summary_lines:\n        print(line)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:46.413307Z","iopub.execute_input":"2025-11-26T10:59:46.413607Z","iopub.status.idle":"2025-11-26T10:59:46.484619Z","shell.execute_reply.started":"2025-11-26T10:59:46.413580Z","shell.execute_reply":"2025-11-26T10:59:46.483857Z"}},"outputs":[{"name":"stdout","text":"üåê Initializing environment summary...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\n=== PulseTrace Environment Summary ===\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n=== PulseTrace Environment Summary ===\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Gemini Mode: OFF \u001b[1m(\u001b[0mno API key found\u001b[1m)\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Gemini Mode: OFF <span style=\"font-weight: bold\">(</span>no API key found<span style=\"font-weight: bold\">)</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"UI Mode: ENABLED \u001b[1m(\u001b[0mipywidgets available\u001b[1m)\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">UI Mode: ENABLED <span style=\"font-weight: bold\">(</span>ipywidgets available<span style=\"font-weight: bold\">)</span>\n</pre>\n"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## (4) Configure Gemini Client (defensive)\nAttempts to import and configure `google-generativeai`.  \nFalls back safely if unavailable or misconfigured.\n","metadata":{}},{"cell_type":"code","source":"# Cell 4 ‚Äî Gemini client initialization with clear debugging output\nprint(\"ü§ñ Checking if Gemini client can be initialized...\")\n\nUSE_GEMINI = bool(os.getenv(\"GOOGLE_API_KEY\"))\n\nif USE_GEMINI:\n    print(\"üîç GOOGLE_API_KEY detected. Attempting to configure Gemini client...\")\n    try:\n        import google.generativeai as genai\n        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n        print(\"‚úî Gemini client configured successfully.\")\n    except Exception as e:\n        print(\"‚ùå Failed to initialize Gemini client.\")\n        print(\"   Falling back to deterministic offline mode.\")\n        print(\"   Reason:\", e)\n        USE_GEMINI = False\nelse:\n    print(\"‚ö† Gemini disabled ‚Äî no GOOGLE_API_KEY found. Running in offline deterministic mode.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:46.485804Z","iopub.execute_input":"2025-11-26T10:59:46.486152Z","iopub.status.idle":"2025-11-26T10:59:46.495443Z","shell.execute_reply.started":"2025-11-26T10:59:46.486124Z","shell.execute_reply":"2025-11-26T10:59:46.494386Z"}},"outputs":[{"name":"stdout","text":"ü§ñ Checking if Gemini client can be initialized...\n‚ö† Gemini disabled ‚Äî no GOOGLE_API_KEY found. Running in offline deterministic mode.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## üß© Core Runtime Structures  \nThis cell initializes all global stores, time helpers, hashing helpers, and idempotent guards used across the PulseTrace engine.  \nThese are required before tools, agents, or the router can function.\n","metadata":{}},{"cell_type":"code","source":"print(\"üîß Initializing core runtime structures...\")\n\nimport os, time, uuid, json, re, hashlib, traceback, pathlib, threading\nfrom collections import Counter, deque\nfrom IPython.display import display, Markdown\n\nif \"_PULSETRACE_FINAL\" not in globals():\n    _PULSETRACE_FINAL = True\n    DEMO_SEED = True\n\n    def now_ts(): \n        return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\n    def make_id(prefix=\"inc\"): \n        return f\"{prefix}-{uuid.uuid4().hex[:8]}\"\n\n    def sha256_hex(s: str): \n        return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n\n    TRACE_STORE = []\n    IN_MEMORY_BUS = deque()\n    SESSIONS = {}\n    MEMORY_BANK = []\n    DRAFT_MEMORY = []\n\nprint(\"‚úÖ Core runtime structures ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:46.496443Z","iopub.execute_input":"2025-11-26T10:59:46.496767Z","iopub.status.idle":"2025-11-26T10:59:46.508455Z","shell.execute_reply.started":"2025-11-26T10:59:46.496734Z","shell.execute_reply":"2025-11-26T10:59:46.507763Z"}},"outputs":[{"name":"stdout","text":"üîß Initializing core runtime structures...\n‚úÖ Core runtime structures ready.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## ‚¨áÔ∏è Adding PDF Export and Report Utilities\n\nThis section introduces helper functions necessary to provide a **\"Download as PDF\"** option for the final report. \n\nSince the Jupyter UI is rendering a mix of Markdown and HTML, we implement a file-download mechanism that is common in web applications:\n\n- It uses the pure-Python **`reportlab`** library to convert the Markdown report into a raw PDF file format in memory.\n- The PDF bytes are then encoded using **Base64**.\n- The Base64 string is embedded into an **HTML download link** (`<a download>`) that triggers the file save when clicked.\n\nThis ensures the **integrity and professional formatting** of the final RCA report artifact.","metadata":{}},{"cell_type":"code","source":"# --- INSTALLATION: Run this first if reportlab is not already installed ---\nprint(\"‚öôÔ∏è Checking for and installing 'reportlab' dependency...\")\n# The '!' prefix runs the command in the shell environment, installing the library.\n!pip install reportlab \n\n# --- IMPORTS ---\nimport base64\nfrom io import BytesIO\nfrom IPython.display import HTML, display\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\nfrom reportlab.lib.styles import getSampleStyleSheet\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\nimport re\n\nprint(\"‚úÖ 'reportlab' installed/verified. Loading PDF utility functions...\")\n\n# --- UTILITY FUNCTIONS ---\n\ndef create_simple_pdf_from_markdown(markdown_content, incident_id):\n    \"\"\"Creates a basic PDF document from the Markdown content using reportlab.\"\"\"\n    buffer = BytesIO()\n    doc = SimpleDocTemplate(\n        buffer,\n        pagesize=letter,\n        title=f\"PulseTrace RCA Report: {incident_id}\",\n        leftMargin=72, rightMargin=72, topMargin=50, bottomMargin=50\n    )\n    styles = getSampleStyleSheet() \n    Story = []\n    \n    for line in markdown_content.split('\\n'):\n        line = line.strip()\n        if not line:\n            Story.append(Spacer(1, 6))\n            continue\n            \n        # 1. Headings\n        if line.startswith('# Incident Report:'):\n            Story.append(Paragraph(line.replace('# Incident Report: ', ''), styles['Title']))\n        elif line.startswith('## '):\n            Story.append(Paragraph(line.replace('## ', ''), styles['h2']))\n        elif line.startswith('### '):\n            Story.append(Paragraph(line.replace('### ', ''), styles['h3']))\n        \n        # 2. Lists (Numbered/Bulleted)\n        elif re.match(r'^\\d+\\. ', line):\n            # Numbered list item\n            Story.append(Paragraph(line, styles['Normal'], bulletText=line.split('.')[0] + '.'))\n        elif line.startswith('- '):\n            # Bulleted list item (using Normal style for simplicity)\n            Story.append(Paragraph(line.replace('- ', ''), styles['Normal'], bulletText='‚Ä¢'))\n\n        # 3. Code Blocks/Tables/Metadata (Using Code style)\n        elif line.startswith('```') or line.startswith('|'):\n            Story.append(Paragraph(line, styles['Code']))\n        \n        # 4. Standard Text\n        else:\n            # FIX: Use re.sub to ensure **text** is converted to <b>text</b>\n            # This correctly handles the paired bold markers, preventing the Parse error.\n            styled_line = re.sub(r'\\*\\*(.*?)\\*\\*', r'<b>\\1</b>', line)\n            \n            Story.append(Paragraph(styled_line, styles['BodyText']))\n        \n        Story.append(Spacer(1, 4))\n\n\n    doc.build(Story)\n    buffer.seek(0)\n    return buffer.read()\n\ndef render_pdf_download_button(incident_id, report_md):\n    \"\"\"\n    Generates the PDF content and renders an HTML button to download it using IPython.display.\n    \"\"\"\n    pdf_bytes = create_simple_pdf_from_markdown(report_md, incident_id)\n    pdf_base64 = base64.b64encode(pdf_bytes).decode('utf-8')\n    filename = f\"PulseTrace_Report_{incident_id}.pdf\"\n\n    html_button = f\"\"\"\n    <a download=\"{filename}\" href=\"data:application/pdf;base64,{pdf_base64}\">\n        <button style=\"background-color: #2196F3; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; font-size: 16px; margin-top: 20px;\">\n            ‚¨áÔ∏è Download as PDF\n        </button>\n    </a>\n    \"\"\"\n    return html_button\n\nprint(\"‚ú® PDF utility functions (create_simple_pdf_from_markdown, render_pdf_download_button) loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:46.509268Z","iopub.execute_input":"2025-11-26T10:59:46.509658Z","iopub.status.idle":"2025-11-26T10:59:50.475500Z","shell.execute_reply.started":"2025-11-26T10:59:46.509627Z","shell.execute_reply":"2025-11-26T10:59:50.474462Z"}},"outputs":[{"name":"stdout","text":"‚öôÔ∏è Checking for and installing 'reportlab' dependency...\nRequirement already satisfied: reportlab in /usr/local/lib/python3.11/dist-packages (4.4.5)\nRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab) (11.3.0)\nRequirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from reportlab) (3.4.4)\n‚úÖ 'reportlab' installed/verified. Loading PDF utility functions...\n‚ú® PDF utility functions (create_simple_pdf_from_markdown, render_pdf_download_button) loaded.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## üß™ Demo Memory Seed + Synthetic Log Inputs  \nThis cell seeds the demo MEMORY_BANK (idempotent) and defines the synthetic log inputs used in offline RCA mode.\n","metadata":{}},{"cell_type":"code","source":"print(\"üß™ Seeding demo memory & loading synthetic logs...\")\n\ndef seed_demo_memory_once():\n    global MEMORY_BANK\n    if not DEMO_SEED: \n        print(\"  ‚Ü™ DEMO_SEED disabled.\")\n        return\n    if any(e.get(\"seeded\") for e in MEMORY_BANK): \n        print(\"  ‚Ü™ MEMORY_BANK already seeded.\")\n        return\n\n    MEMORY_BANK.clear()\n    seeded = [\n        {\n            \"incident_id\": \"past-inc-orders-2025-11-01\",\n            \"job\": \"orders\",\n            \"error_class\": \"TypeError\",\n            \"changed_fields\": [\"price\"],\n            \"sample_anomalies\": [{\"row\":10,\"field\":\"price\",\"value\":\"999.99\"}],\n            \"created_at\": \"2025-11-01 09:00:00\",\n            \"hint_snippet\": \"cannot cast '123.45' to INT\",\n        },\n        {\n            \"incident_id\": \"past-inc-pricing-2025-10-20\",\n            \"job\": \"pricing\",\n            \"error_class\": \"ValueError\",\n            \"changed_fields\": [],\n            \"sample_anomalies\": [{\"row\":400,\"field\":\"price\",\"value\":-5.0}],\n            \"created_at\": \"2025-10-20 14:30:00\",\n            \"hint_snippet\": \"negative value\",\n        }\n    ]\n\n    for e in seeded:\n        e[\"text_hash\"] = sha256_hex(\n            f\"{e['job']}|{e['error_class']}|{','.join(e.get('changed_fields',[]))}|{e.get('hint_snippet','')}\"\n        )\n        e[\"seeded\"] = True\n        e[\"source\"] = \"demo_seed\"\n        MEMORY_BANK.append(e)\n\n    print(\"  ‚úî MEMORY_BANK seeded:\", [m[\"incident_id\"] for m in MEMORY_BANK])\n\nseed_demo_memory_once()\n\nSYNTHETIC_LOGS = {\n    \"schema_drift\": [\n        \"2025-11-24 10:00:01 INFO job=orders ETL step=ingest files=3\",\n        \"2025-11-24 10:03:02 ERROR job=orders transform TypeError: cannot cast '123.45' to INT on column price\",\n    ],\n    \"missing_partition\": [\n        \"2025-11-24 11:00:05 ERROR job=reports No files found for partition dt=2025-11-24\",\n    ],\n    \"invalid_values\": [\n        \"2025-11-24 12:05:02 ERROR job=pricing transform ValueError: negative value found in price at row 524\",\n    ],\n}\n\nprint(\"‚úÖ Demo memory + synthetic logs loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.476758Z","iopub.execute_input":"2025-11-26T10:59:50.477138Z","iopub.status.idle":"2025-11-26T10:59:50.486194Z","shell.execute_reply.started":"2025-11-26T10:59:50.477106Z","shell.execute_reply":"2025-11-26T10:59:50.485312Z"}},"outputs":[{"name":"stdout","text":"üß™ Seeding demo memory & loading synthetic logs...\n  ‚úî MEMORY_BANK seeded: ['past-inc-orders-2025-11-01', 'past-inc-pricing-2025-10-20']\n‚úÖ Demo memory + synthetic logs loaded.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## üõ† Deterministic Diagnostic Tools  \nThis cell defines the lightweight deterministic tools used by agents:  \n- log_fetch  \n- schema_diff  \n- sample_data  \n- lineage_query  \n- history_query  \n- save_report  \nThese simulate a real pipeline environment for offline RCA.\n","metadata":{}},{"cell_type":"code","source":"print(\"üõ† Initializing deterministic diagnostic tools...\")\n\nclass LogFetch:\n    def run(self, params):\n        return {\"lines\": SYNTHETIC_LOGS.get(params.get(\"scenario\"), []), \"job\": None}\n\nclass SchemaDiff:\n    def run(self, params):\n        if params.get(\"scenario\") == \"schema_drift\":\n            return {\"diff\":[{\"field\":\"price\",\"old\":\"INT\",\"new\":\"FLOAT\"}]}\n        return {\"diff\":[]}\n\nclass SampleData:\n    def run(self, params):\n        scenario = params.get(\"scenario\")\n        if scenario == \"schema_drift\":\n            return {\n                \"rows\":[{\"id\":1,\"price\":\"123.45\"},{\"id\":2,\"price\":\"200.00\"}],\n                \"anomalies\":[{\"row\":1,\"field\":\"price\",\"value\":\"123.45\"}]\n            }\n        if scenario == \"invalid_values\":\n            return {\n                \"rows\":[{\"id\":524,\"price\":-12.5},{\"id\":100,\"price\":10.0}],\n                \"anomalies\":[{\"row\":524,\"field\":\"price\",\"value\":-12.5}]\n            }\n        return {\"rows\":[], \"anomalies\":[]}\n\nclass LineageQuery:\n    def run(self, params):\n        job = params.get(\"job\")\n        if job in (\"orders\",\"pricing\"):\n            return {\"downstreams\":[\n                {\"asset\":\"dashboard.sales_over_time\",\"type\":\"dashboard\",\"critical\":True},\n                {\"asset\":\"ml.revenue_forecast\",\"type\":\"model\",\"critical\":True}\n            ]}\n        if job == \"reports\":\n            return {\"downstreams\":[{\"asset\":\"dashboard.reports\",\"type\":\"dashboard\",\"critical\":False}]}\n        return {\"downstreams\":[]}\n\nclass HistoryQuery:\n    def run(self, params):\n        signature = params.get(\"signature\", {})\n        matches=[]\n        for past in MEMORY_BANK:\n            score = 0.0\n            if signature.get(\"job\") == past.get(\"job\"):\n                score += 0.4\n            cf=set(signature.get(\"changed_fields\",[])); pf=set(past.get(\"changed_fields\",[]))\n            if cf and pf:\n                overlap = len(cf & pf)/max(1,len(cf|pf))\n                score += 0.4*overlap\n            if signature.get(\"error_class\") == past.get(\"error_class\"):\n                score += 0.2\n            if score>0: \n                matches.append({\"past_incident\":past,\"score\":round(score,2)})\n        matches.sort(key=lambda x: x[\"score\"], reverse=True)\n        return {\"matches\": matches[:5]}\n\nclass SaveReport:\n    def run(self, params):\n        report_md = params.get(\"report_md\")\n        report_json = params.get(\"report_json\")\n        incident_id = params.get(\"incident_id\", make_id(\"rpt\"))\n        out_dir = params.get(\"out_dir\",\"submission_reports\")\n        os.makedirs(out_dir, exist_ok=True)\n        base = f\"{out_dir}/pulsetrace_report_{incident_id}\"\n        md_path = f\"{base}.md\"; json_path = f\"{base}.json\"\n        with open(md_path, \"w\") as f: f.write(report_md)\n        with open(json_path, \"w\") as f: json.dump(report_json, f, indent=2)\n        return {\"saved\": True, \"md_path\": md_path, \"json_path\": json_path}\n\nTOOLS = {\n    \"log_fetch\": LogFetch(),\n    \"schema_diff\": SchemaDiff(),\n    \"sample_data\": SampleData(),\n    \"lineage_query\": LineageQuery(),\n    \"history_query\": HistoryQuery(),\n    \"save_report\": SaveReport()\n}\n\nprint(\"‚úÖ Tools initialized.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.487105Z","iopub.execute_input":"2025-11-26T10:59:50.487329Z","iopub.status.idle":"2025-11-26T10:59:50.513233Z","shell.execute_reply.started":"2025-11-26T10:59:50.487308Z","shell.execute_reply":"2025-11-26T10:59:50.512264Z"}},"outputs":[{"name":"stdout","text":"üõ† Initializing deterministic diagnostic tools...\n‚úÖ Tools initialized.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## üîç Signature Builder & Existing Incident Lookup\n\nThese helpers generate the canonical incident signature and support\nidempotent behavior by checking whether the same incident signature\nalready exists in the current session or memory bank.\n\nThis ensures:\n- recurring incidents reuse past reports  \n- no duplicate work  \n- deterministic RCA behavior across runs  \n","metadata":{}},{"cell_type":"code","source":"print(\"üîß Loading: Signature builder & existing-entry lookup helpers...\")\n\n# -------------------------\n# Helpers: signature, find existing\n# -------------------------\ndef build_signature(incident, schema_diff_res, logs_res, samples_res):\n    job = incident.get(\"job\")\n    err = incident.get(\"error_snippet\",\"\")\n    m = re.search(r\"(TypeError|ValueError|No files found|timeout|Missing|PermissionError)\", err, re.I)\n    error_class = m.group(1) if m else \"Unknown\"\n    changed = [d[\"field\"] for d in schema_diff_res.get(\"diff\",[])]\n    text = f\"{job}|{error_class}|{','.join(changed)}|{err}\"\n    th = sha256_hex(text)\n\n    print(f\"üìå Signature built for job='{job}', error_class='{error_class}', changed_fields={changed}\")\n\n    return {\n        \"job\": job,\n        \"error_class\": error_class,\n        \"changed_fields\": changed,\n        \"text_hash\": th,\n        \"sample_anomalies\": samples_res.get(\"anomalies\", []),\n        \"created_at\": now_ts()\n    }\n\ndef find_existing_by_text_hash(text_hash):\n    print(f\"üîç Checking for existing incidents with text_hash={text_hash[:10]}...\")\n\n    for inc_id, sess in SESSIONS.items():\n        sig = sess.get(\"signature\") or {}\n        if sig.get(\"text_hash\") == text_hash:\n            print(f\"‚úî Found matching session: {inc_id}\")\n            return (\"session\", inc_id, sess)\n\n    for entry in MEMORY_BANK:\n        if entry.get(\"text_hash\") == text_hash:\n            print(f\"‚úî Found matching historical memory entry: {entry.get('incident_id')}\")\n            return (\"memory\", entry.get(\"incident_id\", None), entry)\n\n    print(\"‚ùå No matching signature found.\")\n    return (None, None, None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.514086Z","iopub.execute_input":"2025-11-26T10:59:50.514314Z","iopub.status.idle":"2025-11-26T10:59:50.539894Z","shell.execute_reply.started":"2025-11-26T10:59:50.514295Z","shell.execute_reply":"2025-11-26T10:59:50.538929Z"}},"outputs":[{"name":"stdout","text":"üîß Loading: Signature builder & existing-entry lookup helpers...\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## üìä Confidence Scoring Layer\n\nPulseTrace combines several signals to compute a final confidence score:\n\n- match strength with historical incidents  \n- schema field drift  \n- anomalies in sampled rows  \n- optional Gemini-based augmentation (hybrid mode)\n\nThe following section computes:\n- the final confidence score  \n- a detailed breakdown  \n- labels (HIGH / MEDIUM / LOW)  \n","metadata":{}},{"cell_type":"code","source":"print(\"üìà Loading: Confidence scoring helpers...\")\n\n# -------------------------\n# Improved confidence (same logic, now in its own cell)\n# -------------------------\ndef confidence_label(score: float) -> str:\n    if score >= 0.80: return \"HIGH\"\n    if score >= 0.50: return \"MEDIUM\"\n    return \"LOW\"\n\ndef compute_confidence(signature: dict, history_matches: list = None, use_gemini: bool = False):\n    history_matches = history_matches or []\n    history_score = 0.0\n\n    if history_matches:\n        try:\n            history_score = max(float(m.get(\"score\", 0.0)) for m in history_matches)\n        except Exception:\n            history_score = 0.0\n\n    field_score = 1.0 if signature.get(\"changed_fields\") else 0.0\n    anomalies = signature.get(\"sample_anomalies\") or []\n    anomaly_count = len(anomalies)\n    anomaly_score = min(1.0, anomaly_count / 3.0)\n    gemini_score = 1.0 if use_gemini else 0.0\n\n    weights = {\"history\": 0.40, \"field\": 0.25, \"anomaly\": 0.25, \"gemini\": 0.10}\n\n    if not use_gemini:\n        s = weights[\"history\"] + weights[\"field\"] + weights[\"anomaly\"]\n        weights[\"history\"] /= s\n        weights[\"field\"]  /= s\n        weights[\"anomaly\"]/= s\n        weights[\"gemini\"] = 0.0\n\n    score = (\n        weights[\"history\"] * history_score +\n        weights[\"field\"] * field_score +\n        weights[\"anomaly\"] * anomaly_score +\n        weights[\"gemini\"] * gemini_score\n    )\n    score = max(0.0, min(1.0, round(score, 2)))\n\n    breakdown = {\n        \"history_score\": round(history_score,2),\n        \"field_score\": round(field_score,2),\n        \"anomaly_score\": round(anomaly_score,2),\n        \"gemini_score\": round(gemini_score,2),\n        \"weights\": {k: round(v,2) for k,v in weights.items()},\n        \"final_score\": score,\n        \"label\": confidence_label(score)\n    }\n\n    print(f\"üìä Confidence score computed: {score} ({breakdown['label']})\")\n\n    return score, breakdown\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.540688Z","iopub.execute_input":"2025-11-26T10:59:50.540970Z","iopub.status.idle":"2025-11-26T10:59:50.565141Z","shell.execute_reply.started":"2025-11-26T10:59:50.540948Z","shell.execute_reply":"2025-11-26T10:59:50.564221Z"}},"outputs":[{"name":"stdout","text":"üìà Loading: Confidence scoring helpers...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## üö® Severity Computation Layer\n\nSeverity is computed using:\n- downstream impact (critical dashboards/models)  \n- schema drift  \n- sample anomalies  \n\nThis produces a normalized severity score and label.\n","metadata":{}},{"cell_type":"code","source":"print(\"üö® Loading: Severity computation helpers...\")\n\n# -------------------------\n# Severity helpers\n# -------------------------\ndef compute_severity(signature, impact):\n    score = 0.0\n\n    ds = (impact.get(\"impact\") if isinstance(impact, dict) else impact) or {}\n    dlist = ds.get(\"downstreams\") if isinstance(ds, dict) else None\n\n    if dlist:\n        if any(d.get(\"critical\") for d in dlist):\n            score += 0.6\n        else:\n            score += 0.2\n\n    if signature.get(\"changed_fields\"):\n        score += 0.2\n\n    if signature.get(\"sample_anomalies\"):\n        score += 0.2\n\n    final = min(1.0, round(score,2))\n    print(f\"üö® Severity computed: {final}\")\n    return final\n\ndef severity_label(score):\n    if score >= 0.75: return \"HIGH\"\n    if score >= 0.4: return \"MEDIUM\"\n    return \"LOW\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.566141Z","iopub.execute_input":"2025-11-26T10:59:50.566430Z","iopub.status.idle":"2025-11-26T10:59:50.588700Z","shell.execute_reply.started":"2025-11-26T10:59:50.566397Z","shell.execute_reply":"2025-11-26T10:59:50.587758Z"}},"outputs":[{"name":"stdout","text":"üö® Loading: Severity computation helpers...\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## üì® Message Bus & Router Core\n\nPulseTrace uses a simple FIFO in-memory message bus (`IN_MEMORY_BUS`)  \nto allow agents to send events to one another.\n\nThis layer contains:\n- `emit()` ‚Üí enqueue messages  \n- `pop_next_message()` ‚Üí get next event  \n- `router_once()` ‚Üí routes one event to the appropriate agent  \n- `router_run_blocking()` ‚Üí drains the queue  \n\nAll agents rely on this for communication.\n","metadata":{}},{"cell_type":"code","source":"print(\"üì® Loading: Message bus & router system...\")\n\n# -------------------------\n# Simple router & agents (deterministic + optional LLM augmentation)\n# -------------------------\ndef emit(frm: str, to: str, payload: dict):\n    msg = {\n        \"id\": uuid.uuid4().hex,\n        \"from\": frm,\n        \"to\": to,\n        \"ts\": now_ts(),\n        \"payload\": payload\n    }\n    TRACE_STORE.append(msg)\n    IN_MEMORY_BUS.append(msg)\n\n    print(f\"[emit] {frm} ‚Üí {to} | type={payload.get('type', payload.get('pattern',''))}\")\n    return msg\n\ndef pop_next_message():\n    return IN_MEMORY_BUS.popleft() if IN_MEMORY_BUS else None\n\ndef router_once():\n    msg = pop_next_message()\n    if not msg:\n        return False\n\n    to = msg[\"to\"]\n\n    print(f\"[router] Dispatching to agent: {to}\")\n\n    if to == \"pulse_detector\":\n        pulse_detector.on_message(msg)\n    elif to == \"root_cause_diagnoser\":\n        diagnoser.on_message(msg)\n    elif to == \"pattern_history_agent\":\n        history_agent.on_message(msg)\n    elif to == \"impact_scope_agent\":\n        impact_agent.on_message(msg)\n    elif to == \"pulse_advisor\":\n        advisor.on_message(msg)\n    elif to == \"save_report\":\n        pass\n    elif to == \"ui\":\n        pass\n\n    return True\n\ndef router_run_blocking():\n    print(\"‚ñ∂Ô∏è Router: starting event loop...\")\n    while IN_MEMORY_BUS:\n        router_once()\n    print(\"‚èπ Router: event queue empty.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.589878Z","iopub.execute_input":"2025-11-26T10:59:50.590446Z","iopub.status.idle":"2025-11-26T10:59:50.610578Z","shell.execute_reply.started":"2025-11-26T10:59:50.590416Z","shell.execute_reply":"2025-11-26T10:59:50.609576Z"}},"outputs":[{"name":"stdout","text":"üì® Loading: Message bus & router system...\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## üîé Pulse Detector Agent (`pulse_detector`)\n\nThis is the first agent in the multi-agent pipeline.\n\nIt:\n- receives uploaded logs or demo logs  \n- detects scenario (`schema_drift`, `invalid_values`, etc.)  \n- extracts job name and error snippet  \n- builds the initial incident object  \n- emits it to the Diagnoser agent  \n\nThis makes PulseTrace‚Äôs architecture fully aligned with the write-up.\n","metadata":{}},{"cell_type":"code","source":"# -------------------------\n# NEW: Detector Agent (pulse_detector)\n# -------------------------\nprint(\"üì® Loading: Detector Agent (pulse_detector)...\")\n\nclass PulseDetector:\n    \"\"\"\n    Lightweight failure detector.\n    Reads raw log lines and identifies scenario + job + error snippet.\n    Emits a detection payload to root_cause_diagnoser.\n    \"\"\"\n\n    def detect(self, lines):\n        text = \"\\n\".join(lines).lower()\n\n        # infer scenario\n        if \"cannot cast\" in text or \"cannot convert\" in text:\n            scenario = \"schema_drift\"\n        elif \"no files found\" in text:\n            scenario = \"missing_partition\"\n        elif \"negative value\" in text:\n            scenario = \"invalid_values\"\n        else:\n            scenario = \"unknown\"\n\n        # infer job\n        m = re.findall(r\"job=([A-Za-z0-9_\\-\\.]+)\", text)\n        job = Counter(m).most_common(1)[0][0] if m else \"unknown\"\n\n        return scenario, job\n\n    def on_message(self, msg):\n        payload = msg[\"payload\"]\n        lines = payload.get(\"lines\", [])\n        incident_id = payload.get(\"incident_id\", make_id(\"inc\"))\n\n        if not lines:\n            print(\"[pulse_detector] No log lines provided.\")\n            return\n\n        scenario, job = self.detect(lines)\n        error_snip = lines[0] if lines else \"\"\n\n        print(f\"[pulse_detector] Scenario={scenario}, job={job}\")\n\n        emit(\n            \"pulse_detector\",\n            \"root_cause_diagnoser\",\n            {\n                \"type\": \"detection\",\n                \"incident_id\": incident_id,\n                \"scenario\": scenario,\n                \"job\": job,\n                \"error_snippet\": error_snip,\n                \"source_lines_count\": len(lines),\n                \"pattern\": \"detected_by_detector\"\n            }\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.614367Z","iopub.execute_input":"2025-11-26T10:59:50.614625Z","iopub.status.idle":"2025-11-26T10:59:50.633145Z","shell.execute_reply.started":"2025-11-26T10:59:50.614603Z","shell.execute_reply":"2025-11-26T10:59:50.632107Z"}},"outputs":[{"name":"stdout","text":"üì® Loading: Detector Agent (pulse_detector)...\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## üß† Diagnoser Agent (`root_cause_diagnoser`)\n\nThis is the core reasoning agent.\n\nIt:\n- fetches logs  \n- detects schema drift  \n- collects sample rows  \n- builds the incident signature  \n- optionally enriches logs using Gemini (hybrid mode)  \n- sends signature to History + Impact agents  \n\nThis is the heart of the RCA workflow.\n","metadata":{}},{"cell_type":"code","source":"print(\"üß† Loading: Diagnoser agent...\")\n\nclass Diagnoser:\n    def on_message(self, msg):\n        inc = msg[\"payload\"]\n        incident_id = inc[\"incident_id\"]\n        scenario = inc.get(\"scenario\")\n\n        print(f\"üîç Diagnoser triggered for incident: {incident_id}, scenario={scenario}\")\n\n        # Fetch evidence\n        logs_res    = TOOLS[\"log_fetch\"].run({\"scenario\": scenario, \"job\": None})\n        schema_res  = TOOLS[\"schema_diff\"].run({\"scenario\": scenario, \"job\": inc.get(\"job\")})\n        samples_res = TOOLS[\"sample_data\"].run({\"scenario\": scenario, \"job\": inc.get(\"job\")})\n\n        # Build signature\n        signature = build_signature(inc, schema_res, logs_res, samples_res)\n        signature[\"incident_id\"] = incident_id\n\n        # Optional Gemini log summarization\n        try:\n            if USE_GEMINI and _GEMINI_CONFIGURED:\n                print(\"‚ú® Gemini hybrid mode ON ‚Äî summarizing logs...\")\n                prompt = (\n                    \"Summarize these log lines and highlight likely root causes:\\n\\n\" +\n                    \"\\n\".join(logs_res.get(\"lines\",[])[:200])\n                )\n                llm_res = call_gemini_api(prompt, timeout_s=5)\n                signature[\"llm\"] = {\n                    \"used\": bool(llm_res.get(\"text\") and not llm_res.get(\"error\")),\n                    \"summary\": (llm_res.get(\"text\") or \"\")[:3000],\n                    \"confidence\": llm_res.get(\"confidence\") or 0.0,\n                    \"error\": llm_res.get(\"error\"),\n                    \"prompt_snippet\": prompt[:800],\n                    \"ts\": now_ts()\n                }\n            else:\n                signature[\"llm\"] = {\"used\": False}\n        except Exception as e:\n            signature[\"llm\"] = {\"used\": False, \"error\": str(e)}\n\n        # Store draft session\n        DRAFT_MEMORY.append(dict(signature))\n\n        SESSIONS[incident_id] = {\n            \"signature\": signature,\n            \"logs\": logs_res.get(\"lines\", [])[:50],\n            \"schema_diff\": schema_res.get(\"diff\", []),\n            \"samples\": samples_res.get(\"rows\", [])[:20],\n            \"sample_anomalies\": samples_res.get(\"anomalies\", []),\n            \"history\": None,\n            \"impact\": None,\n            \"report\": None,\n            \"stage\": \"draft\"\n        }\n\n        print(f\"üì¶ Diagnoser built signature & stored session for {incident_id}\")\n\n        emit(\"root_cause_diagnoser\", \"pattern_history_agent\",\n             {\"type\":\"signature\", \"signature\": signature})\n        emit(\"root_cause_diagnoser\", \"impact_scope_agent\",\n             {\"type\":\"failure_point\", \"signature\": signature})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.634208Z","iopub.execute_input":"2025-11-26T10:59:50.635079Z","iopub.status.idle":"2025-11-26T10:59:50.656024Z","shell.execute_reply.started":"2025-11-26T10:59:50.635052Z","shell.execute_reply":"2025-11-26T10:59:50.655265Z"}},"outputs":[{"name":"stdout","text":"üß† Loading: Diagnoser agent...\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## üß¨ History Analyzer & Impact Analyzer\n\n### **History Analyzer (`pattern_history_agent`)**\nSurfaces recurring incidents via `history_query`.\n\n### **Impact Analyzer (`impact_scope_agent`)**\nDetermines downstream blast radius using `lineage_query`.\n\nBoth agents enrich the session and forward results to the Advisor.\n","metadata":{}},{"cell_type":"code","source":"print(\"üß¨ Loading: History & Impact analyzer agents...\")\n\nclass PatternHistoryAgent:\n    def on_message(self, msg):\n        sig = msg[\"payload\"][\"signature\"]\n        print(f\"üìö History agent running for signature: {sig['incident_id']}\")\n\n        res = TOOLS[\"history_query\"].run({\"signature\": sig})\n        emit(\"pattern_history_agent\", \"pulse_advisor\",\n             {\"type\":\"history_matches\", \"matches\": res[\"matches\"], \"signature\": sig})\n\nclass ImpactScopeAgent:\n    def on_message(self, msg):\n        sig = msg[\"payload\"][\"signature\"]\n        print(f\"üåê Impact agent running for job={sig.get('job')}\")\n\n        job = sig.get(\"job\")\n        res = TOOLS[\"lineage_query\"].run({\"job\": job})\n        emit(\"impact_scope_agent\", \"pulse_advisor\",\n             {\"type\":\"impact\", \"impact\": res, \"signature\": sig})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.656966Z","iopub.execute_input":"2025-11-26T10:59:50.657307Z","iopub.status.idle":"2025-11-26T10:59:50.680682Z","shell.execute_reply.started":"2025-11-26T10:59:50.657281Z","shell.execute_reply":"2025-11-26T10:59:50.679908Z"}},"outputs":[{"name":"stdout","text":"üß¨ Loading: History & Impact analyzer agents...\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## üßæ Advisor Agent (`pulse_advisor`)\n\nThe Advisor is the final reasoning layer.  \nIt waits until *both* history + impact results arrive, then:\n\n- computes final severity  \n- computes confidence (with optional Gemini contribution)  \n- merges logs, schema diff, anomalies  \n- generates:\n  - Markdown RCA report  \n  - machine-readable JSON report\n\nFinally, it emits:\n- `ui` ‚Üí to display the draft report  \n- `save_report` ‚Üí to persist it  \n","metadata":{}},{"cell_type":"code","source":"print(\"üß≠ Loading: Advisor agent...\")\n\nclass Advisor:\n    def __init__(self, use_gemini=False):\n        self.use_gemini = use_gemini\n\n    def _extract_downstreams(self, impact):\n        if isinstance(impact, dict):\n            if \"downstreams\" in impact:\n                return impact.get(\"downstreams\")\n            if \"impact\" in impact and isinstance(impact[\"impact\"], dict) and \"downstreams\" in impact[\"impact\"]:\n                return impact[\"impact\"].get(\"downstreams\")\n        return []\n\n    def synthesize_report(self, session):\n        signature = session.get(\"signature\", {})\n        history = session.get(\"history\") or []\n        impact = session.get(\"impact\") or {}\n        sev_score = compute_severity(signature, impact)\n        sev_label = severity_label(sev_score)\n        confidence_score, confidence_breakdown = compute_confidence(signature, history, use_gemini=self.use_gemini)\n\n        # determine llm usage from session.signature if present\n        llm_info = session.get(\"signature\", {}).get(\"llm\", {\"used\": False})\n\n        # Metadata block (ensure seeded/source) -- set mode deterministically: deterministic | gemini | hybrid\n        meta = {\n            \"incident_id\": signature.get(\"incident_id\"),\n            \"job\": signature.get(\"job\"),\n            \"scenario\": signature.get(\"scenario\") or signature.get(\"error_class\") or \"unknown\",\n            \"text_hash\": signature.get(\"text_hash\"),\n            \"mode\": (\"hybrid\" if (self.use_gemini and llm_info.get(\"used\")) else (\"gemini\" if self.use_gemini else \"deterministic\")),\n            \"generated_at\": now_ts()\n        }\n        # ensure seeded/source presence\n        meta[\"seeded\"] = any(e.get(\"text_hash\")==signature.get(\"text_hash\") and e.get(\"seeded\") for e in MEMORY_BANK)\n        meta[\"source\"] = \"demo_seed\" if meta[\"seeded\"] else \"runtime_draft\"\n\n        # Build markdown (report text preserved as before)\n        lines = []\n        lines.append(\"---\")\n        for k,v in meta.items():\n            lines.append(f\"{k}: {v}\")\n        lines.append(\"---\"); lines.append(\"\")\n        lines.append(f\"# RCA Report - {signature.get('incident_id')}\")\n        lines.append(\"\")\n        lines.append(f\"**Root Cause (hypothesis):** {signature.get('error_class')} on job `{signature.get('job')}`\")\n        lines.append(\"\")\n        lines.append(f\"**Severity:** {sev_label} ({sev_score})\")\n        lines.append(f\"**Confidence:** {confidence_breakdown['label']} ({confidence_breakdown['final_score']})\")\n        lines.append(\"\")\n        # Confidence breakdown in MD (top)\n        cb = confidence_breakdown\n        lines.append(\"**Confidence breakdown:**\")\n        lines.append(f\"- final_score: {cb['final_score']} ({cb['label']})\")\n        lines.append(f\"- history_score: {cb['history_score']}, field_score: {cb['field_score']}, anomaly_score: {cb['anomaly_score']}\")\n        lines.append(\"\")\n        # Evidence\n        lines.append(\"## Evidence\")\n        log_lines = session.get(\"logs\", [])[:10]\n        if log_lines:\n            lines.append(\"### Log excerpt (`log_fetch`)\")\n            lines.append(\"```\")\n            for i, ln in enumerate(log_lines, start=1):\n                lines.append(f\"{i:3d}: {ln}\")\n            lines.append(\"```\")\n        schema_diff = session.get(\"schema_diff\", [])\n        if schema_diff:\n            lines.append(\"### Schema diff (`schema_diff`)\")\n            for d in schema_diff:\n                lines.append(f\"- field: `{d.get('field')}` ‚Äî {d.get('old')} ‚Üí {d.get('new')}\")\n        sample_anoms = session.get(\"sample_anomalies\", [])\n        if sample_anoms:\n            lines.append(\"### Sample anomalies (`sample_data`)\")\n            lines.append(\"|row|field|value|\")\n            lines.append(\"|--:|:--|:--|\")\n            for a in sample_anoms:\n                lines.append(f\"|{a.get('row')}|{a.get('field')}|`{a.get('value')}`|\")\n        if (not log_lines) and (not schema_diff) and (not sample_anoms):\n            lines.append(\"- none\")\n        lines.append(\"\")\n\n        # Historical matches\n        lines.append(\"## Historical Matches (`history_query`)\")\n        history = session.get(\"history\") or []\n        if history:\n            for m in history:\n                comp = m.get(\"components\",{})\n                lines.append(f\"- matched past incident {m['past_incident'].get('created_at','n/a')} ‚Äî score {m['score']} (job:{comp.get('job',0)}, fields:{round(comp.get('fields',0),2)}, error_class:{comp.get('error_class',0)})\")\n        else:\n            lines.append(\"- none\")\n        lines.append(\"\")\n\n        # Impact (lineage)\n        lines.append(\"## Impacted Downstream (`lineage_query`)\")\n        dlist = self._extract_downstreams(impact)\n        if dlist:\n            for d in dlist:\n                lines.append(f\"- `{d['asset']}` ({d['type']}) critical={d.get('critical', False)}\")\n        else:\n            lines.append(\"- none\")\n        lines.append(\"\")\n\n        # Recommendations (scenario-driven, generic templates with placeholders)\n        lines.append(\"## Recommended Next Steps\")\n\n        # Default scenario templates (merge-safe)\n        _default_scenario_recs = {\n            \"missing_partition\": {\n                \"title\": \"Missing partition detected\",\n                \"steps\": [\n                    \"Verify storage prefix exists and list objects: `aws s3 ls {prefix}` or `gsutil ls {prefix}` and confirm objects under {partition}.\",\n                    \"If upstream should have produced this partition: re-run the upstream producer job for the {partition} window (job: {job}).\",\n                    \"If backup data exists: run a targeted backfill to restore {prefix}/{partition}.\",\n                    \"If upstream intentionally skipped: mark the partition as 'no-data-expected' and configure downstream pipelines to soft-skip.\"\n                ],\n                \"confidence_hint\": \"High ‚Äî missing partition pattern found in logs.\"\n            },\n            \"schema_drift\": {\n                \"title\": \"Schema change / drift detected\",\n                \"steps\": [\n                    \"Identify the changed field(s) and impacted sinks with a quick schema diff.\",\n                    \"If breaking: coordinate a schema contract update with upstream and deploy a compatible parser or migration.\",\n                    \"Run a selective backfill for affected partitions if data loss or type coercion happened.\"\n                ],\n                \"confidence_hint\": \"Medium ‚Äî schema mismatch detected; confirm with schema registry or sample rows.\"\n            },\n            \"invalid_values\": {\n                \"title\": \"Invalid values / data quality issues\",\n                \"steps\": [\n                    \"Quantify affected rows (e.g., `SELECT COUNT(*) FROM upstream WHERE <predicate>`).\",\n                    \"Add validation rules at ingest or transformation to reject/flag invalid values.\",\n                    \"Create an automated remediation/backfill run for the affected windows.\"\n                ],\n                \"confidence_hint\": \"Medium ‚Äî sample anomalies indicate bad values.\"\n            },\n            \"unknown\": {\n                \"title\": \"Unknown / requires further investigation\",\n                \"steps\": [\n                    \"Collect more contextual logs and sample rows for the incident window.\",\n                    \"Run the diagnosis again with enriched context (longer log excerpt, schema diff, sample data).\",\n                    \"If possible, consult upstream job logs or owners for additional clues.\"\n                ],\n                \"confidence_hint\": \"Low ‚Äî insufficient evidence to recommend automated remediation.\"\n            }\n        }\n\n        # Merge into any global mapping without overwriting existing keys\n        _SCENARIO_RECOMMENDATIONS = globals().get(\"SCENARIO_RECOMMENDATIONS\", {})\n        for k, v in _default_scenario_recs.items():\n            if k not in _SCENARIO_RECOMMENDATIONS:\n                _SCENARIO_RECOMMENDATIONS[k] = v\n        globals()[\"SCENARIO_RECOMMENDATIONS\"] = _SCENARIO_RECOMMENDATIONS\n\n        # Build recs list using template if available, else fall back to legacy logic\n        recs = []\n\n        # -----------------------------\n        # Robust scenario detection:\n        # try error_class, then signature.scenario, then session-level scenario,\n        # then look for keywords in error_snippet or first log lines.\n        # -----------------------------\n        sig_scenario = (\n            signature.get(\"error_class\")\n            or signature.get(\"scenario\")\n            or session.get(\"scenario\")\n            or signature.get(\"detected_scenario\")\n            or \"unknown\"\n        )\n        \n        # --- FIX IMPLEMENTATION: Force 'schema_drift' if strong evidence exists ---\n        log_text = \"\\n\".join(session.get(\"logs\", [])).lower()\n        if signature.get(\"changed_fields\") or schema_diff or \"cannot cast\" in log_text or \"mismatch\" in log_text:\n            sig_scenario = \"schema_drift\"\n            signature[\"scenario\"] = sig_scenario # Persist fix for report consistency\n        # --- END FIX ---\n\n\n        # fallback heuristic: inspect error_snippet or first log line for missing-partition cues\n        if sig_scenario == \"unknown\":\n            snippet = (signature.get(\"error_snippet\") or \"\")\n            if not snippet and log_lines:\n                snippet = log_lines[0]\n            s_low = snippet.lower() if isinstance(snippet, str) else \"\"\n            if (\"no files\" in s_low or \"no objects\" in s_low or \"dt=\" in s_low or \"partition\" in s_low):\n                sig_scenario = \"missing_partition\"\n\n        # Context for formatting placeholders\n        context = {\n            \"prefix\": signature.get(\"prefix\", \"<prefix>\"),\n            \"partition\": signature.get(\"partition\", \"<partition>\"),\n            \"job\": signature.get(\"job\", \"<job>\"),\n            \"owner\": signature.get(\"owner\", \"<owner>\"),\n            \"error_snippet\": signature.get(\"error_snippet\", signature.get(\"error_class\", \"<error>\"))\n        }\n        # --- Normalize freeform error_class/scenario -> canonical scenario keys (minimal, in-place) ---\n        sig_low = str(sig_scenario).lower() if sig_scenario is not None else \"\"\n        if \"no files\" in sig_low or \"no objects\" in sig_low or \"missing partition\" in sig_low or \"dt=\" in sig_low:\n            sig_scenario = \"missing_partition\"\n        elif \"schema\" in sig_low or \"mismatch\" in sig_low or \"cannot cast\" in sig_low or \"cannot convert\" in sig_low:\n            sig_scenario = \"schema_drift\"\n        elif \"jsondecodeerror\" in sig_low or \"malformed\" in sig_low or \"parse error\" in sig_low:\n            sig_scenario = \"invalid_values\"\n        # persist normalized scenario back to signature so downstream code sees canonical key\n        if sig_scenario in [\"missing_partition\", \"schema_drift\", \"invalid_values\"]:\n             signature[\"scenario\"] = sig_scenario\n\n        template = _SCENARIO_RECOMMENDATIONS.get(sig_scenario)\n\n        if template:\n            # Add a header-like first line (keeps readability consistent)\n            recs.append(template.get(\"title\", \"Recommended actions\"))\n            for step in template.get(\"steps\", []):\n                try:\n                    recs.append(step.format(**context))\n                except Exception:\n                    recs.append(step)  # fallback raw\n            if template.get(\"confidence_hint\"):\n                recs.append(f\"Confidence: {template.get('confidence_hint')}\")\n        else:\n            # Preserve existing special-case logic if no template mapped\n            if signature.get(\"changed_fields\"):\n                recs.append(\"1) Run a selective query to quantify affected rows, e.g.: `SELECT COUNT(*) FROM upstream_table WHERE TRY_CAST(price AS INT) IS NULL;`\")\n                recs.append(\"2) Add safe CAST/COALESCE in transformation or coordinate upstream backfill.\")\n                recs.append(\"3) If critical downstreams exist, run a prioritized backfill for affected partitions.\")\n            for a in signature.get(\"sample_anomalies\", []):\n                v = a.get(\"value\")\n                if isinstance(v, (int, float)) and v < 0:\n                    recs.append(\"Add validation rule at ingest: reject or flag negative prices; consider alerting on new validation failures.\")\n            # Missing partition scenario override\n            if signature.get(\"error_class\") == \"missing_partition\" or signature.get(\"scenario\") == \"missing_partition\":\n                recs = [f\"Verify that the expected partition exists in storage: run `aws s3 ls {signature.get('prefix','<prefix>')}` and confirm objects under {signature.get('partition','<partition>')}`.\",\n                        f\"If upstream should have produced this partition: re-run the upstream job for `{signature.get('partition','<partition>')}` (job: {signature.get('job')}).\",\n                        f\"If backup data exists: run a targeted backfill to restore `{signature.get('partition','<partition>')}`.\",\"If upstream intentionally skipped this partition, mark it as 'no-data-expected' and configure downstream pipelines to soft-skip instead of erroring.\"]\n            elif not recs:\n                recs.append(\"Inspect logs, collect additional sample rows, and rerun diagnosis with more context.\")\n\n        for r in recs: lines.append(f\"- {r}\")\n        lines.append(\"\")\n        lines.append(f\"\\nGenerated at: {now_ts()}\")\n        md = \"\\n\".join(lines)\n\n        # Build machine-readable JSON (ensure impact.downstreams always present)\n        dlist_json = dlist or []\n        rep_json = {\n            \"meta\": meta,\n            \"root_cause\": {\"hypothesis\": signature.get(\"error_class\"), \"job\": signature.get(\"job\")},\n            \"severity\": {\"label\": sev_label, \"score\": sev_score},\n            \"confidence_breakdown\": confidence_breakdown,\n            \"evidence\": {\n                \"logs\": {\"tool\":\"log_fetch\",\"lines\": log_lines},\n                \"schema_diff\": {\"tool\":\"schema_diff\",\"diff\": schema_diff},\n                \"sample_anomalies\": {\"tool\":\"sample_data\",\"anomalies\": sample_anoms}\n            },\n            \"history_matches\": history,\n            \"impact\": {\"downstreams\": dlist_json},\n            \"recommended_next_steps\": recs,\n            \"generated_at\": now_ts(),\n            # add non-intrusive LLM provenance info (machine-readable only)\n            \"llm\": llm_info if isinstance(llm_info, dict) else {\"used\": False}\n        }\n        return md, rep_json\n\n    def on_message(self, msg):\n        p = msg[\"payload\"]\n        sig = p.get(\"signature\") or {}\n        inc_id = sig.get(\"incident_id\") or make_id(\"inc\")\n        session = SESSIONS.setdefault(inc_id, {\"signature\":sig,\"history\":None,\"impact\":None,\"report\":None,\"stage\":\"draft\"})\n        if p.get(\"type\") == \"history_matches\":\n            session[\"history\"] = p.get(\"matches\")\n        if p.get(\"type\") == \"impact\":\n            session[\"impact\"] = p.get(\"impact\")\n\n        if session.get(\"history\") is not None and session.get(\"impact\") is not None and session.get(\"report\") is None:\n            md, rep_json = self.synthesize_report(session)\n            session[\"report\"] = {\"md\": md, \"json\": rep_json}\n            session[\"stage\"] = \"awaiting_approval\"\n            SESSIONS[inc_id] = session\n            emit(\"pulse_advisor\", \"ui\", {\"type\":\"report_ready\", \"incident_id\": inc_id, \"report\": md})\n            emit(\"pulse_advisor\", \"save_report\", {\"type\":\"save_pending\", \"incident_id\": inc_id})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.681889Z","iopub.execute_input":"2025-11-26T10:59:50.682173Z","iopub.status.idle":"2025-11-26T10:59:50.718504Z","shell.execute_reply.started":"2025-11-26T10:59:50.682147Z","shell.execute_reply":"2025-11-26T10:59:50.717485Z"}},"outputs":[{"name":"stdout","text":"üß≠ Loading: Advisor agent...\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## ‚öôÔ∏è Agent Instantiation & Router Wiring\n\nHere we create all agents:\n- Diagnoser  \n- History Analyzer  \n- Impact Analyzer  \n- Advisor (hybrid mode if Gemini available)\n\nThen they are ready to receive events through the router system.\n","metadata":{}},{"cell_type":"code","source":"print(\"‚öôÔ∏è Instantiating agents...\")\n\n# instantiate agents\npulse_detector = PulseDetector()     # NEW\ndiagnoser = Diagnoser()\nhistory_agent = PatternHistoryAgent()\nimpact_agent = ImpactScopeAgent()\nadvisor = Advisor(use_gemini=USE_GEMINI)\n\nprint(\"ü§ñ Agents ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.719560Z","iopub.execute_input":"2025-11-26T10:59:50.719895Z","iopub.status.idle":"2025-11-26T10:59:50.737268Z","shell.execute_reply.started":"2025-11-26T10:59:50.719862Z","shell.execute_reply":"2025-11-26T10:59:50.736420Z"}},"outputs":[{"name":"stdout","text":"‚öôÔ∏è Instantiating agents...\nü§ñ Agents ready.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## üìÑ Robust File Parsing Helper\n\nThis helper turns uploaded files (log/txt/json/csv/ndjson)\ninto clean line lists for the RCA pipeline.\n\nSupports:\n- JSON object\n- JSONL/NDJSON\n- CSV with \"message\" column\n- Plain text log files\n\nUsed by:\n- offline runner  \n- UI handling module  \n","metadata":{}},{"cell_type":"code","source":"print(\"üìÑ Loading: Robust file parsing helper...\")\n\ndef parse_uploaded_file_bytes(b: bytes):\n    s = b.decode(\"utf-8\", errors=\"replace\")\n\n    # Try to parse as a full JSON document\n    try:\n        obj = json.loads(s)\n        if isinstance(obj, dict):\n            if \"message\" in obj:\n                print(\"üìÑ Parsed JSON dict with message.\")\n                return [obj[\"message\"]]\n            return [json.dumps(obj)]\n        if isinstance(obj, list):\n            print(\"üìÑ Parsed JSON list.\")\n            return [\n                json.dumps(i) if not isinstance(i,str) else i\n                for i in obj\n            ]\n    except:\n        pass\n\n    # Try NDJSON / JSONL\n    lines = s.splitlines()\n    nd=[]; nd_ok=True\n    for ln in lines:\n        ln_strip = ln.strip()\n        if not ln_strip:\n            continue\n        try:\n            j = json.loads(ln_strip)\n            if isinstance(j, dict) and \"message\" in j:\n                nd.append(j[\"message\"])\n            else:\n                nd.append(json.dumps(j))\n        except:\n            nd_ok=False\n            break\n\n    if nd_ok and nd:\n        print(\"üìÑ Parsed NDJSON / JSONL format.\")\n        return nd\n\n    # CSV format\n    try:\n        import io, csv\n        reader = csv.DictReader(io.StringIO(s))\n        if reader.fieldnames:\n            print(\"üìÑ Parsed CSV file.\")\n            msgs=[]\n            for row in reader:\n                if \"message\" in row and row[\"message\"]:\n                    msgs.append(row[\"message\"])\n                else:\n                    msgs.append(\", \".join(f\"{k}={v}\" for k,v in row.items()))\n            return msgs\n    except:\n        pass\n\n    # fallback: plain text log lines\n    print(\"üìÑ Parsed plain text log file.\")\n    return [ln for ln in s.splitlines() if ln.strip()]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.738251Z","iopub.execute_input":"2025-11-26T10:59:50.738529Z","iopub.status.idle":"2025-11-26T10:59:50.755668Z","shell.execute_reply.started":"2025-11-26T10:59:50.738507Z","shell.execute_reply":"2025-11-26T10:59:50.754667Z"}},"outputs":[{"name":"stdout","text":"üìÑ Loading: Robust file parsing helper...\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## üõ°Ô∏è Validator helper\n\nThis cell defines `validate_report()` which sanity-checks the generated RCA JSON.  \nIt prints a short summary when called to make debugging easier.\n","metadata":{}},{"cell_type":"code","source":"print(\"üîß Loading: Validator helper...\")\n\ndef validate_report(rep_json):\n    errors = []\n    if not rep_json:\n        return {\"ok\": False, \"errors\": [\"report is None\"]}\n\n    meta = rep_json.get(\"meta\")\n    if not meta:\n        errors.append(\"missing meta\")\n    else:\n        for k in (\"incident_id\", \"job\", \"text_hash\", \"generated_at\"):\n            if k not in meta:\n                errors.append(f\"meta.{k} missing\")\n\n    if \"root_cause\" not in rep_json:\n        errors.append(\"missing root_cause\")\n\n    # severity should be present and include a score key\n    sev = rep_json.get(\"severity\")\n    if not sev or \"score\" not in sev:\n        errors.append(\"missing severity.score\")\n\n    cb = rep_json.get(\"confidence_breakdown\")\n    if not cb or \"final_score\" not in cb:\n        errors.append(\"confidence breakdown missing final_score\")\n\n    ev = rep_json.get(\"evidence\") or {}\n    if \"logs\" not in ev:\n        errors.append(\"evidence.logs missing\")\n    if \"schema_diff\" not in ev:\n        errors.append(\"evidence.schema_diff missing\")\n    # proper presence check for sample_anomalies (allow empty list)\n    if \"sample_anomalies\" not in ev:\n        errors.append(\"evidence.sample_anomalies missing\")\n\n    hist = rep_json.get(\"history_matches\", [])\n    if not isinstance(hist, list):\n        errors.append(\"history_matches not a list\")\n\n    impact = rep_json.get(\"impact\") or {}\n    if not (\"downstreams\" in impact):\n        errors.append(\"impact.downstreams missing\")\n\n    if \"recommended_next_steps\" not in rep_json:\n        errors.append(\"no recommended_next_steps\")\n\n    result = {\n        \"ok\": len(errors) == 0,\n        \"errors\": errors,\n        \"summary\": {\n            \"meta\": meta,\n            \"severity\": rep_json.get(\"severity\"),\n            \"confidence\": cb.get(\"final_score\") if cb else None,\n            \"history_count\": len(hist)\n        }\n    }\n\n    print(f\"üîç validate_report -> ok={result['ok']}, errors_count={len(errors)}\")\n    if errors:\n        for e in errors:\n            print(f\"  - {e}\")\n    return result\n\nprint(\"üîß Validator helper loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.756767Z","iopub.execute_input":"2025-11-26T10:59:50.757462Z","iopub.status.idle":"2025-11-26T10:59:50.779694Z","shell.execute_reply.started":"2025-11-26T10:59:50.757428Z","shell.execute_reply":"2025-11-26T10:59:50.778967Z"}},"outputs":[{"name":"stdout","text":"üîß Loading: Validator helper...\nüîß Validator helper loaded.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## ‚ñ∂Ô∏è Offline runner: `run_offline_samples`\n\nThis cell contains the main offline runner that:\n- parses input samples\n- emits detection events (now routed through `pulse_detector`)\n- waits for the agent chain to complete\n- optionally saves Markdown + JSON reports to `submission_reports/`\n\nExtra prints are included to trace progress.\n","metadata":{}},{"cell_type":"code","source":"print(\"‚ñ∂Ô∏è Loading: Offline runner (run_offline_samples)...\")\n\ndef run_offline_samples(samples=None, show=True, save_reports=True, out_dir=\"submission_reports\"):\n    if samples is None:\n        samples = {k:(\"\\n\".join(v)).encode(\"utf-8\") for k,v in SYNTHETIC_LOGS.items()}\n    results=[]\n    if save_reports: os.makedirs(out_dir, exist_ok=True)\n\n    for name, b in samples.items():\n        try:\n            print(f\"\\n--- Processing sample: {name} ---\")\n            IN_MEMORY_BUS.clear(); TRACE_STORE.clear(); SESSIONS.clear(); DRAFT_MEMORY.clear()\n            lines = parse_uploaded_file_bytes(b)\n\n            # canonical logs first\n            logs_res = TOOLS[\"log_fetch\"].run({\"scenario\": name, \"job\": None})\n            log_lines = logs_res.get(\"lines\", []) or lines\n\n            # prefer log_fetch.job, else most frequent job= token, else scenario\n            parsed_job = logs_res.get(\"job\")\n            if not parsed_job:\n                jobs = re.findall(r\"\\bjob=([A-Za-z0-9_\\-\\.]+)\", \"\\n\".join(log_lines))\n                if jobs:\n                    parsed_job = Counter(jobs).most_common(1)[0][0]\n            job = parsed_job or name\n\n            print(f\"Detected job='{job}' for sample '{name}' (parsed_job={parsed_job})\")\n\n            # call tools using canonical job\n            schema_res = TOOLS[\"schema_diff\"].run({\"scenario\": name, \"job\": job})\n            samples_res = TOOLS[\"sample_data\"].run({\"scenario\": name, \"job\": job})\n\n            provisional_incident_id = make_id(\"inc\")\n            provisional_inc = {\n                \"incident_id\": provisional_incident_id,\n                \"scenario\": name,\n                \"job\": job,\n                \"error_snippet\": lines[0] if lines else \"\"\n            }\n            signature = build_signature(provisional_inc, schema_res, logs_res, samples_res)\n            signature[\"incident_id\"] = provisional_incident_id\n\n            kind, existing_id, entry = find_existing_by_text_hash(signature[\"text_hash\"])\n            if kind == \"session\":\n                existing_sess = entry\n                report_obj = existing_sess.get(\"report\")\n                incident_id = existing_id\n                md = report_obj.get(\"md\") if report_obj else None\n                rep_json = report_obj.get(\"json\") if report_obj else None\n                print(f\"[idempotency] Reusing existing session report for sample '{name}' -> incident {incident_id}\")\n            elif kind == \"memory\":\n                incident_id = existing_id or make_id(\"inc\")\n                md_lines = [f\"# RCA Report - {incident_id}\", \"\", f\"*Reused historical incident matching text_hash {signature['text_hash']}*\"]\n                md = \"\\n\".join(md_lines)\n                rep_json = {\"meta\":{\"incident_id\": incident_id, \"text_hash\": signature[\"text_hash\"]}, \"note\":\"reused_from_memory\"}\n                print(f\"[idempotency] Reusing confirmed memory for sample '{name}' -> incident {incident_id}\")\n            else:\n                payload = {\n                    \"incident_id\": provisional_incident_id,\n                    \"scenario\": name,\n                    \"job\": job,\n                    \"type\": \"detection\",\n                    \"pattern\": \"detected_offline\",\n                    \"category\": \"demo\",\n                    \"error_snippet\": lines[0] if lines else \"\",\n                    \"source_lines_count\": len(lines),\n                    \"text_hash\": signature[\"text_hash\"],\n                    # Provide raw lines so PulseDetector can operate\n                    \"lines\": log_lines\n                }\n                print(f\"[emit] offline_runner -> pulse_detector | incident={provisional_incident_id}\")\n                emit(\"offline_runner\", \"pulse_detector\", payload)\n                router_run_blocking()\n                sess = SESSIONS.get(provisional_incident_id)\n                if not sess:\n                    print(f\"[error] No session created for {provisional_incident_id}\")\n                    md = None; rep_json = None; incident_id = provisional_incident_id\n                else:\n                    # ensure downstream agents run to completion\n                    router_run_blocking()\n                    report_obj = sess.get(\"report\")\n                    md = report_obj.get(\"md\") if report_obj else None\n                    rep_json = report_obj.get(\"json\") if report_obj else None\n                    incident_id = provisional_incident_id\n\n            print(f\"\\n=== Draft report for sample '{name}' (incident: {incident_id}) ===\\n\")\n            if md and show:\n                display(Markdown(md))\n            else:\n                print(\"No report produced. Check TRACE_STORE and SESSIONS.\")\n\n            saved_info = None\n            if save_reports and md and rep_json:\n                out = TOOLS[\"save_report\"].run({\"report_md\": md, \"report_json\": rep_json, \"incident_id\": incident_id, \"out_dir\": out_dir})\n                saved_info = out\n                print(f\"Saved artifacts: MD -> {out['md_path']}, JSON -> {out['json_path']}\")\n\n            results.append((incident_id, md, rep_json, saved_info))\n        except Exception as e:\n            print(\"Error processing sample\", name, e)\n            traceback.print_exc()\n            results.append((None, None, None, None))\n    return results\n\nprint(\"‚ñ∂Ô∏è Offline runner loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.781069Z","iopub.execute_input":"2025-11-26T10:59:50.781346Z","iopub.status.idle":"2025-11-26T10:59:50.807378Z","shell.execute_reply.started":"2025-11-26T10:59:50.781324Z","shell.execute_reply":"2025-11-26T10:59:50.806594Z"}},"outputs":[{"name":"stdout","text":"‚ñ∂Ô∏è Loading: Offline runner (run_offline_samples)...\n‚ñ∂Ô∏è Offline runner loaded.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## ‚ñ∂ Run demo & quick validation\n\nSeed demo memory (idempotent), run the offline demo, and validate saved reports.  \nThis cell prints concise validation results.\n","metadata":{}},{"cell_type":"code","source":"print(\"üöÄ Running demo: seed memory and run offline samples...\")\n\nseed_demo_memory_once()\nprint(\"‚úÖ Demo memory seeded.\")\n\ndemo_results = run_offline_samples(save_reports=True)\nprint(\"\\n‚úÖ Demo finished. 'demo_results' contains tuples (incident_id, md, json, saved_info).\")\n\n# Quick validation output:\nfor inc_id, md, rep_json, saved in demo_results:\n    print(f\"\\nüîé Validation for {inc_id}: {validate_report(rep_json)}\")\n\nprint(\"üèÅ Demo & validation complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.808221Z","iopub.execute_input":"2025-11-26T10:59:50.808441Z","iopub.status.idle":"2025-11-26T10:59:50.842631Z","shell.execute_reply.started":"2025-11-26T10:59:50.808421Z","shell.execute_reply":"2025-11-26T10:59:50.841967Z"}},"outputs":[{"name":"stdout","text":"üöÄ Running demo: seed memory and run offline samples...\n  ‚Ü™ MEMORY_BANK already seeded.\n‚úÖ Demo memory seeded.\n\n--- Processing sample: schema_drift ---\nüìÑ Parsed CSV file.\nDetected job='orders' for sample 'schema_drift' (parsed_job=orders)\nüìå Signature built for job='orders', error_class='TypeError', changed_fields=['price']\nüîç Checking for existing incidents with text_hash=95ac10f46a...\n‚ùå No matching signature found.\n[emit] offline_runner -> pulse_detector | incident=inc-9e7e460c\n[emit] offline_runner ‚Üí pulse_detector | type=detection\n‚ñ∂Ô∏è Router: starting event loop...\n[router] Dispatching to agent: pulse_detector\n[pulse_detector] Scenario=schema_drift, job=orders\n[emit] pulse_detector ‚Üí root_cause_diagnoser | type=detection\n[router] Dispatching to agent: root_cause_diagnoser\nüîç Diagnoser triggered for incident: inc-9e7e460c, scenario=schema_drift\nüìå Signature built for job='orders', error_class='Unknown', changed_fields=['price']\nüì¶ Diagnoser built signature & stored session for inc-9e7e460c\n[emit] root_cause_diagnoser ‚Üí pattern_history_agent | type=signature\n[emit] root_cause_diagnoser ‚Üí impact_scope_agent | type=failure_point\n[router] Dispatching to agent: pattern_history_agent\nüìö History agent running for signature: inc-9e7e460c\n[emit] pattern_history_agent ‚Üí pulse_advisor | type=history_matches\n[router] Dispatching to agent: impact_scope_agent\nüåê Impact agent running for job=orders\n[emit] impact_scope_agent ‚Üí pulse_advisor | type=impact\n[router] Dispatching to agent: pulse_advisor\n[router] Dispatching to agent: pulse_advisor\nüö® Severity computed: 0.4\nüìä Confidence score computed: 0.73 (MEDIUM)\n[emit] pulse_advisor ‚Üí ui | type=report_ready\n[emit] pulse_advisor ‚Üí save_report | type=save_pending\n[router] Dispatching to agent: ui\n[router] Dispatching to agent: save_report\n‚èπ Router: event queue empty.\n‚ñ∂Ô∏è Router: starting event loop...\n‚èπ Router: event queue empty.\n\n=== Draft report for sample 'schema_drift' (incident: inc-9e7e460c) ===\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"---\nincident_id: inc-9e7e460c\njob: orders\nscenario: Unknown\ntext_hash: 68f9abb188909a082050690f0dcb9eaef7d072fa3f92a76e14313e61b79baea5\nmode: deterministic\ngenerated_at: 2025-11-26 10:59:50\nseeded: False\nsource: runtime_draft\n---\n\n# RCA Report - inc-9e7e460c\n\n**Root Cause (hypothesis):** Unknown on job `orders`\n\n**Severity:** MEDIUM (0.4)\n**Confidence:** MEDIUM (0.73)\n\n**Confidence breakdown:**\n- final_score: 0.73 (MEDIUM)\n- history_score: 0.8, field_score: 1.0, anomaly_score: 0.33\n\n## Evidence\n### Log excerpt (`log_fetch`)\n```\n  1: 2025-11-24 10:00:01 INFO job=orders ETL step=ingest files=3\n  2: 2025-11-24 10:03:02 ERROR job=orders transform TypeError: cannot cast '123.45' to INT on column price\n```\n### Schema diff (`schema_diff`)\n- field: `price` ‚Äî INT ‚Üí FLOAT\n### Sample anomalies (`sample_data`)\n|row|field|value|\n|--:|:--|:--|\n|1|price|`123.45`|\n\n## Historical Matches (`history_query`)\n- matched past incident 2025-11-01 09:00:00 ‚Äî score 0.8 (job:0, fields:0, error_class:0)\n\n## Impacted Downstream (`lineage_query`)\n- `dashboard.sales_over_time` (dashboard) critical=True\n- `ml.revenue_forecast` (model) critical=True\n\n## Recommended Next Steps\n- Schema change / drift detected\n- Identify the changed field(s) and impacted sinks with a quick schema diff.\n- If breaking: coordinate a schema contract update with upstream and deploy a compatible parser or migration.\n- Run a selective backfill for affected partitions if data loss or type coercion happened.\n- Confidence: Medium ‚Äî schema mismatch detected; confirm with schema registry or sample rows.\n\n\nGenerated at: 2025-11-26 10:59:50"},"metadata":{}},{"name":"stdout","text":"Saved artifacts: MD -> submission_reports/pulsetrace_report_inc-9e7e460c.md, JSON -> submission_reports/pulsetrace_report_inc-9e7e460c.json\n\n--- Processing sample: missing_partition ---\nüìÑ Parsed CSV file.\nDetected job='reports' for sample 'missing_partition' (parsed_job=reports)\nüìå Signature built for job='reports', error_class='Unknown', changed_fields=[]\nüîç Checking for existing incidents with text_hash=5d54bc2d7a...\n‚ùå No matching signature found.\n[emit] offline_runner -> pulse_detector | incident=inc-d0f171ce\n[emit] offline_runner ‚Üí pulse_detector | type=detection\n‚ñ∂Ô∏è Router: starting event loop...\n[router] Dispatching to agent: pulse_detector\n[pulse_detector] Scenario=missing_partition, job=reports\n[emit] pulse_detector ‚Üí root_cause_diagnoser | type=detection\n[router] Dispatching to agent: root_cause_diagnoser\nüîç Diagnoser triggered for incident: inc-d0f171ce, scenario=missing_partition\nüìå Signature built for job='reports', error_class='No files found', changed_fields=[]\nüì¶ Diagnoser built signature & stored session for inc-d0f171ce\n[emit] root_cause_diagnoser ‚Üí pattern_history_agent | type=signature\n[emit] root_cause_diagnoser ‚Üí impact_scope_agent | type=failure_point\n[router] Dispatching to agent: pattern_history_agent\nüìö History agent running for signature: inc-d0f171ce\n[emit] pattern_history_agent ‚Üí pulse_advisor | type=history_matches\n[router] Dispatching to agent: impact_scope_agent\nüåê Impact agent running for job=reports\n[emit] impact_scope_agent ‚Üí pulse_advisor | type=impact\n[router] Dispatching to agent: pulse_advisor\n[router] Dispatching to agent: pulse_advisor\nüö® Severity computed: 0.0\nüìä Confidence score computed: 0.0 (LOW)\n[emit] pulse_advisor ‚Üí ui | type=report_ready\n[emit] pulse_advisor ‚Üí save_report | type=save_pending\n[router] Dispatching to agent: ui\n[router] Dispatching to agent: save_report\n‚èπ Router: event queue empty.\n‚ñ∂Ô∏è Router: starting event loop...\n‚èπ Router: event queue empty.\n\n=== Draft report for sample 'missing_partition' (incident: inc-d0f171ce) ===\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"---\nincident_id: inc-d0f171ce\njob: reports\nscenario: No files found\ntext_hash: 5ee4b5a44c3e75c1d2494d5c8dca490c2d9be52bd03a5251356879b79129dffe\nmode: deterministic\ngenerated_at: 2025-11-26 10:59:50\nseeded: False\nsource: runtime_draft\n---\n\n# RCA Report - inc-d0f171ce\n\n**Root Cause (hypothesis):** No files found on job `reports`\n\n**Severity:** LOW (0.0)\n**Confidence:** LOW (0.0)\n\n**Confidence breakdown:**\n- final_score: 0.0 (LOW)\n- history_score: 0.0, field_score: 0.0, anomaly_score: 0.0\n\n## Evidence\n### Log excerpt (`log_fetch`)\n```\n  1: 2025-11-24 11:00:05 ERROR job=reports No files found for partition dt=2025-11-24\n```\n\n## Historical Matches (`history_query`)\n- none\n\n## Impacted Downstream (`lineage_query`)\n- `dashboard.reports` (dashboard) critical=False\n\n## Recommended Next Steps\n- Missing partition detected\n- Verify storage prefix exists and list objects: `aws s3 ls <prefix>` or `gsutil ls <prefix>` and confirm objects under <partition>.\n- If upstream should have produced this partition: re-run the upstream producer job for the <partition> window (job: reports).\n- If backup data exists: run a targeted backfill to restore <prefix>/<partition>.\n- If upstream intentionally skipped: mark the partition as 'no-data-expected' and configure downstream pipelines to soft-skip.\n- Confidence: High ‚Äî missing partition pattern found in logs.\n\n\nGenerated at: 2025-11-26 10:59:50"},"metadata":{}},{"name":"stdout","text":"Saved artifacts: MD -> submission_reports/pulsetrace_report_inc-d0f171ce.md, JSON -> submission_reports/pulsetrace_report_inc-d0f171ce.json\n\n--- Processing sample: invalid_values ---\nüìÑ Parsed CSV file.\nDetected job='pricing' for sample 'invalid_values' (parsed_job=pricing)\nüìå Signature built for job='pricing', error_class='Unknown', changed_fields=[]\nüîç Checking for existing incidents with text_hash=8e7f75aa1f...\n‚ùå No matching signature found.\n[emit] offline_runner -> pulse_detector | incident=inc-49eb7b90\n[emit] offline_runner ‚Üí pulse_detector | type=detection\n‚ñ∂Ô∏è Router: starting event loop...\n[router] Dispatching to agent: pulse_detector\n[pulse_detector] Scenario=invalid_values, job=pricing\n[emit] pulse_detector ‚Üí root_cause_diagnoser | type=detection\n[router] Dispatching to agent: root_cause_diagnoser\nüîç Diagnoser triggered for incident: inc-49eb7b90, scenario=invalid_values\nüìå Signature built for job='pricing', error_class='ValueError', changed_fields=[]\nüì¶ Diagnoser built signature & stored session for inc-49eb7b90\n[emit] root_cause_diagnoser ‚Üí pattern_history_agent | type=signature\n[emit] root_cause_diagnoser ‚Üí impact_scope_agent | type=failure_point\n[router] Dispatching to agent: pattern_history_agent\nüìö History agent running for signature: inc-49eb7b90\n[emit] pattern_history_agent ‚Üí pulse_advisor | type=history_matches\n[router] Dispatching to agent: impact_scope_agent\nüåê Impact agent running for job=pricing\n[emit] impact_scope_agent ‚Üí pulse_advisor | type=impact\n[router] Dispatching to agent: pulse_advisor\n[router] Dispatching to agent: pulse_advisor\nüö® Severity computed: 0.2\nüìä Confidence score computed: 0.36 (LOW)\n[emit] pulse_advisor ‚Üí ui | type=report_ready\n[emit] pulse_advisor ‚Üí save_report | type=save_pending\n[router] Dispatching to agent: ui\n[router] Dispatching to agent: save_report\n‚èπ Router: event queue empty.\n‚ñ∂Ô∏è Router: starting event loop...\n‚èπ Router: event queue empty.\n\n=== Draft report for sample 'invalid_values' (incident: inc-49eb7b90) ===\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"---\nincident_id: inc-49eb7b90\njob: pricing\nscenario: ValueError\ntext_hash: bd9555b803cb819c6072036eb090d993df1c0523a5fc9438e0760870bbda0d3b\nmode: deterministic\ngenerated_at: 2025-11-26 10:59:50\nseeded: False\nsource: runtime_draft\n---\n\n# RCA Report - inc-49eb7b90\n\n**Root Cause (hypothesis):** ValueError on job `pricing`\n\n**Severity:** LOW (0.2)\n**Confidence:** LOW (0.36)\n\n**Confidence breakdown:**\n- final_score: 0.36 (LOW)\n- history_score: 0.6, field_score: 0.0, anomaly_score: 0.33\n\n## Evidence\n### Log excerpt (`log_fetch`)\n```\n  1: 2025-11-24 12:05:02 ERROR job=pricing transform ValueError: negative value found in price at row 524\n```\n### Sample anomalies (`sample_data`)\n|row|field|value|\n|--:|:--|:--|\n|524|price|`-12.5`|\n\n## Historical Matches (`history_query`)\n- matched past incident 2025-10-20 14:30:00 ‚Äî score 0.6 (job:0, fields:0, error_class:0)\n\n## Impacted Downstream (`lineage_query`)\n- `dashboard.sales_over_time` (dashboard) critical=True\n- `ml.revenue_forecast` (model) critical=True\n\n## Recommended Next Steps\n- Add validation rule at ingest: reject or flag negative prices; consider alerting on new validation failures.\n\n\nGenerated at: 2025-11-26 10:59:50"},"metadata":{}},{"name":"stdout","text":"Saved artifacts: MD -> submission_reports/pulsetrace_report_inc-49eb7b90.md, JSON -> submission_reports/pulsetrace_report_inc-49eb7b90.json\n\n‚úÖ Demo finished. 'demo_results' contains tuples (incident_id, md, json, saved_info).\nüîç validate_report -> ok=True, errors_count=0\n\nüîé Validation for inc-9e7e460c: {'ok': True, 'errors': [], 'summary': {'meta': {'incident_id': 'inc-9e7e460c', 'job': 'orders', 'scenario': 'Unknown', 'text_hash': '68f9abb188909a082050690f0dcb9eaef7d072fa3f92a76e14313e61b79baea5', 'mode': 'deterministic', 'generated_at': '2025-11-26 10:59:50', 'seeded': False, 'source': 'runtime_draft'}, 'severity': {'label': 'MEDIUM', 'score': 0.4}, 'confidence': 0.73, 'history_count': 1}}\nüîç validate_report -> ok=True, errors_count=0\n\nüîé Validation for inc-d0f171ce: {'ok': True, 'errors': [], 'summary': {'meta': {'incident_id': 'inc-d0f171ce', 'job': 'reports', 'scenario': 'No files found', 'text_hash': '5ee4b5a44c3e75c1d2494d5c8dca490c2d9be52bd03a5251356879b79129dffe', 'mode': 'deterministic', 'generated_at': '2025-11-26 10:59:50', 'seeded': False, 'source': 'runtime_draft'}, 'severity': {'label': 'LOW', 'score': 0.0}, 'confidence': 0.0, 'history_count': 0}}\nüîç validate_report -> ok=True, errors_count=0\n\nüîé Validation for inc-49eb7b90: {'ok': True, 'errors': [], 'summary': {'meta': {'incident_id': 'inc-49eb7b90', 'job': 'pricing', 'scenario': 'ValueError', 'text_hash': 'bd9555b803cb819c6072036eb090d993df1c0523a5fc9438e0760870bbda0d3b', 'mode': 'deterministic', 'generated_at': '2025-11-26 10:59:50', 'seeded': False, 'source': 'runtime_draft'}, 'severity': {'label': 'LOW', 'score': 0.2}, 'confidence': 0.36, 'history_count': 1}}\nüèÅ Demo & validation complete.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### üõ∞Ô∏è Observability: Traces, Sessions & Memory\n\nPulseTrace exposes lightweight observability tools that help you inspect how the system behaves during an RCA run:\n\n- **`render_trace_store()`** ‚Äî displays recent agent-to-agent (A2A) events flowing through the message bus  \n- **`render_sessions()`** ‚Äî shows active incident sessions, including signatures, history usage, and impact metadata  \n- **Memory Bank Summary** ‚Äî lists long-term stored incident signatures so you can see which patterns and root causes were retained across runs(implemented later in code)\n\nThese views make it easy to understand agent communication, trace execution flow, and verify that multi-agent coordination is happening correctly.\n","metadata":{}},{"cell_type":"code","source":"# Cell: define render_trace_store and render_sessions\nprint(\"üîß defining render_trace_store and render_sessions...\")\n\nfrom IPython.display import display, HTML, Markdown, clear_output\nimport json\n\ndef render_trace_store(limit=200):\n    \"\"\"Display TRACE_STORE (most recent entries). Safe to call repeatedly.\"\"\"\n    try:\n        ts = globals().get(\"TRACE_STORE\", None)\n        if ts is None:\n            display(HTML(\"<i>TRACE_STORE is not defined in globals()</i>\"))\n            print(\"render_trace_store: TRACE_STORE not defined.\")\n            return\n        if not ts:\n            display(HTML(\"<i>No trace messages recorded.</i>\"))\n            print(\"render_trace_store: TRACE_STORE is empty.\")\n            return\n        rows = []\n        for m in ts[-limit:]:\n            payload_str = json.dumps(m.get('payload', {}), default=str, indent=2)\n            rows.append(\n                f\"<tr>\"\n                f\"<td style='vertical-align:top;padding:4px'>{m.get('ts')}</td>\"\n                f\"<td style='vertical-align:top;padding:4px'><b>{m.get('from')}</b> ‚Üí <b>{m.get('to')}</b></td>\"\n                f\"<td style='vertical-align:top;padding:4px'><pre style='white-space:pre-wrap;margin:0'>{payload_str}</pre></td>\"\n                f\"</tr>\"\n            )\n        html = (\n            \"<table style='width:100%;border-collapse:collapse' border=1>\"\n            \"<tr style='background:#f6f6f6'><th>ts</th><th>route</th><th>payload</th></tr>\"\n            + \"\".join(rows) + \"</table>\"\n        )\n        display(HTML(html))\n        print(f\"render_trace_store: displayed {min(len(ts), limit)} trace entries (total stored: {len(ts)}).\")\n    except Exception as e:\n        print(\"render_trace_store: error:\", e)\n        traceback.print_exc()\n\ndef render_sessions():\n    \"\"\"Display SESSIONS dict content (compact).\"\"\"\n    try:\n        sess = globals().get(\"SESSIONS\", None)\n        if sess is None:\n            display(HTML(\"<i>SESSIONS is not defined in globals()</i>\"))\n            print(\"render_sessions: SESSIONS not defined.\")\n            return\n        if not sess:\n            display(HTML(\"<i>No active sessions.</i>\"))\n            print(\"render_sessions: SESSIONS is empty.\")\n            return\n        for sid, s in sess.items():\n            hdr = f\"<h4>Session: {sid} ‚Äî stage: {s.get('stage')}</h4>\"\n            meta = {\n                \"signature\": s.get(\"signature\"),\n                \"history_len\": len(s.get(\"history\") or []),\n                \"impact\": s.get(\"impact\")\n            }\n            display(HTML(hdr))\n            display(HTML(f\"<pre>{json.dumps(meta, indent=2, default=str)}</pre>\"))\n        print(f\"render_sessions: displayed {len(sess)} sessions.\")\n    except Exception as e:\n        print(\"render_sessions: error:\", e)\n        traceback.print_exc()\n\n# expose to globals (redundant but explicit)\nglobals()['render_trace_store'] = render_trace_store\nglobals()['render_sessions'] = render_sessions\n\nprint(\"‚úÖ render_trace_store and render_sessions registered.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.843616Z","iopub.execute_input":"2025-11-26T10:59:50.844187Z","iopub.status.idle":"2025-11-26T10:59:50.859399Z","shell.execute_reply.started":"2025-11-26T10:59:50.844162Z","shell.execute_reply":"2025-11-26T10:59:50.858558Z"}},"outputs":[{"name":"stdout","text":"üîß defining render_trace_store and render_sessions...\n‚úÖ render_trace_store and render_sessions registered.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### UI dependencies & environment checks\n\nDetect & (only if missing) install `ipywidgets` and bring in display helpers.\nThis cell will not reinstall if `ipywidgets` is already available.\n","metadata":{}},{"cell_type":"code","source":"# Cell: UI dependencies & environment checks\nprint(\"üîé UI deps check: verifying ipywidgets and display utilities...\")\n\nimport importlib, sys, subprocess\n\ndef ensure_package(pkg_name, import_name=None, version_spec=None):\n    \"\"\"\n    Ensure a package is importable. If not installed, try pip installing it.\n    Returns True if import succeeded, False otherwise.\n    \"\"\"\n    import_name = import_name or pkg_name\n    try:\n        importlib.import_module(import_name)\n        print(f\"‚úÖ {import_name} already available.\")\n        return True\n    except Exception as e:\n        print(f\"‚ö†Ô∏è {import_name} not found ({e}). Attempting to install...\")\n        try:\n            cmd = [sys.executable, \"-m\", \"pip\", \"install\", pkg_name] + ([version_spec] if version_spec else [])\n            subprocess.check_call(cmd)\n            importlib.invalidate_caches()\n            importlib.import_module(import_name)\n            print(f\"‚úÖ Successfully installed and imported {import_name}.\")\n            return True\n        except Exception as ie:\n            print(f\"‚ùå Failed to install {pkg_name}: {ie}\")\n            return False\n\n# Only install ipywidgets if missing\n_ok_widgets = ensure_package(\"ipywidgets\", \"ipywidgets\", None)\n\n# Bring common display helpers into scope (safe repeated import)\ntry:\n    from IPython.display import display, Markdown, HTML, clear_output\n    from pathlib import Path\n    print(\"‚úÖ IPython.display and Path available.\")\nexcept Exception as e:\n    print(\"‚ùå Could not import IPython.display or Path:\", e)\n\n# Expose UI_AVAILABLE flag for downstream cells\nUI_AVAILABLE = _ok_widgets\nprint(f\"UI_AVAILABLE = {UI_AVAILABLE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.860444Z","iopub.execute_input":"2025-11-26T10:59:50.860745Z","iopub.status.idle":"2025-11-26T10:59:50.883511Z","shell.execute_reply.started":"2025-11-26T10:59:50.860695Z","shell.execute_reply":"2025-11-26T10:59:50.882614Z"}},"outputs":[{"name":"stdout","text":"üîé UI deps check: verifying ipywidgets and display utilities...\n‚úÖ ipywidgets already available.\n‚úÖ IPython.display and Path available.\nUI_AVAILABLE = True\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Optional: Previously uploaded file tracking\n","metadata":{}},{"cell_type":"markdown","source":"### UI Controls & Helpers\n\nCreate widgets (dropdown, file uploader, run/inspect buttons) and small helpers:\n- make_save_path_for_uploaded()\n- approve_and_save_local()\nThis cell uses the same behavior as your original single-cell UI; it only defines helpers and widgets.\n","metadata":{}},{"cell_type":"code","source":"# Cell: UI Controls & Helpers (Clean Version)\nprint(\"üß© UI: creating controls & helpers (no duplicate imports)...\")\n\n# widgets are available only if ipywidgets import succeeded earlier\nif not UI_AVAILABLE:\n    print(\"‚ö†Ô∏è ipywidgets unavailable ‚Äî UI widgets will not be created.\")\nelse:\n    import ipywidgets as widgets  # safe even if already imported\n    from pathlib import Path\n\n    # UI controls (matching your original names)\n    sample_dropdown = widgets.Dropdown(\n        options=[\"-- select demo sample --\"] + list(SYNTHETIC_LOGS.keys()),\n        description=\"Demo:\"\n    )\n    file_uploader = widgets.FileUpload(accept=\".log,.txt,.json,.ndjson,.csv\", multiple=False)\n    run_button = widgets.Button(description=\"Run Diagnosis\", button_style=\"success\")\n    inspect_button = widgets.Button(description=\"Inspect\", button_style=\"\")\n    out_area = widgets.Output()\n\n    print(\"üîß Ensuring make_save_path_for_uploaded is available...\")\n    if 'make_save_path_for_uploaded' not in globals():\n        def make_save_path_for_uploaded(filename, out_dir=\"submission_reports\"):\n            safe = filename.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n            suffix = int(time.time())\n            os.makedirs(out_dir, exist_ok=True)\n            stem = Path(safe).stem\n            ext = Path(safe).suffix or \".log\"\n            fname = f\"{stem}_{suffix}{ext}\"\n            path = os.path.join(out_dir, fname)\n            return path\n\n    print(\"üîß Ensuring approve_and_save_local is available...\")\n    if 'approve_and_save_local' not in globals():\n        def approve_and_save_local(incident_id, uploaded_bytes=None, uploaded_filename=None, out_dir=\"submission_reports\"):\n            # Removed debug print: \"attempting to save...\"\n            sess = SESSIONS.get(incident_id)\n            if not sess or not sess.get(\"report\"):\n                print(\"Error: No session or report found to save.\")\n                return None\n\n            md = sess[\"report\"][\"md\"]\n            rep_json = sess[\"report\"][\"json\"]\n\n            out = TOOLS[\"save_report\"].run(\n                {\"report_md\": md, \"report_json\": rep_json, \"incident_id\": incident_id, \"out_dir\": out_dir}\n            )\n            sess[\"saved_info\"] = out\n            sess[\"stage\"] = \"approved\"\n            SESSIONS[incident_id] = sess\n            \n            # Removed debug print: \"report saved -> {dict}\"\n\n            uploaded_saved_path = None\n            if uploaded_bytes is not None:\n                try:\n                    uploaded_saved_path = make_save_path_for_uploaded(\n                        uploaded_filename or f\"uploaded_{incident_id}\",\n                        out_dir=out_dir\n                    )\n                    with open(uploaded_saved_path, \"wb\") as fh:\n                        fh.write(uploaded_bytes)\n                except Exception as e:\n                    uploaded_saved_path = f\"FAILED_TO_SAVE: {e}\"\n                    print(f\"Error saving uploaded file: {e}\")\n\n            return out, uploaded_saved_path\n\n    print(\"üîß Ensuring wire_save_controls is available...\")\n    def wire_save_controls(incident_id, uploaded_filename=None, uploaded_bytes=None):\n        try:\n            if not globals().get(\"UI_AVAILABLE\", False):\n                return\n\n            approval_checkbox = widgets.Checkbox(\n                description=\"I confirm and approve saving the report\",\n                indent=False,\n                value=False\n            )\n            save_btn = widgets.Button(description=\"Save Report\", button_style=\"primary\", disabled=True)\n\n            info_html = widgets.HTML(value=f\"<small>Incident id: <b>{incident_id}</b></small>\")\n            ctrl = widgets.HBox([approval_checkbox, save_btn])\n            display(info_html)\n            display(ctrl)\n\n            def on_check(change):\n                save_btn.disabled = not approval_checkbox.value\n\n            approval_checkbox.observe(on_check, names=\"value\")\n\n            def on_save(b):\n                with out_area:\n                    # Removed debug prints (\"handler triggered\", \"checkbox value\")\n                    \n                    # Enforce approval UI contract\n                    try:\n                        if not approval_checkbox.value:\n                            display(HTML(\"<b style='color:red'>Please check the approval box before saving.</b>\"))\n                            return\n                    except Exception as e:\n                        display(HTML(f\"<b>Save failed (internal error):</b> {e}\"))\n                        return\n\n                    # Attempt to save\n                    try:\n                        result = approve_and_save_local(\n                            incident_id,\n                            uploaded_bytes=uploaded_bytes,\n                            uploaded_filename=uploaded_filename,\n                            out_dir=\"submission_reports\"\n                        )\n                    except Exception as e:\n                        display(HTML(f\"<b>Save failed (exception):</b> {e}\"))\n                        return\n\n                    if not result:\n                        display(HTML(\"<b>Save failed:</b> no session or report found.\"))\n                        return\n\n                    out, uploaded_saved_path = result\n                    \n                    # ONLY display the clean success message\n                    success_msg = (\n                        f\"<div style='background-color:#e6fffa; padding:10px; border-radius:5px; border:1px solid #b2f5ea;'>\"\n                        f\"<b style='color:#285e61;'>‚úÖ Report saved successfully!</b><br/>\"\n                        f\"üìÑ MD: <code>{out.get('md_path')}</code><br/>\"\n                        f\"üìä JSON: <code>{out.get('json_path')}</code>\"\n                    )\n                    \n                    if uploaded_saved_path:\n                        success_msg += f\"<br/>üìÇ Uploaded File: <code>{uploaded_saved_path}</code>\"\n                    \n                    success_msg += \"</div>\"\n                    \n                    display(HTML(success_msg))\n\n                    save_btn.disabled = True\n                    approval_checkbox.disabled = True\n\n            save_btn.on_click(on_save)\n\n        except Exception as e:\n            print(\"‚ùå Failed to initialize wire_save_controls:\", e)\n\nprint(\"üß© UI Controls & Helpers ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.884445Z","iopub.execute_input":"2025-11-26T10:59:50.884662Z","iopub.status.idle":"2025-11-26T10:59:50.919788Z","shell.execute_reply.started":"2025-11-26T10:59:50.884636Z","shell.execute_reply":"2025-11-26T10:59:50.918744Z"}},"outputs":[{"name":"stdout","text":"üß© UI: creating controls & helpers (no duplicate imports)...\nüîß Ensuring make_save_path_for_uploaded is available...\nüîß Ensuring approve_and_save_local is available...\nüîß Ensuring wire_save_controls is available...\nüß© UI Controls & Helpers ready.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### Run handler, inspector & UI display\n\nDefines run_from_ui(), inspect_ui(), wires callbacks (idempotent), and renders the UI row.\nThis cell avoids re-importing modules, and prints debug lines as actions occur.\n","metadata":{}},{"cell_type":"code","source":"# Cell: Run handler, inspector & UI display (fixed upload extraction)\nprint(\"‚ñ∂Ô∏è UI: setting up run handler, inspector, hooking callbacks, and rendering UI...\")\n\ndef _extract_uploaded_bytes_fallback(u):\n    \"\"\"\n    Robust, self-contained extractor for ipywidgets.FileUpload-like values.\n    Handles:\n      - dict: {filename: { 'content': bytes, 'metadata': ... } }\n      - list/tuple: [(filename, { ... }), ...] or [(filename, content_bytes), ...]\n      - bytes / bytearray / memoryview\n      - fallback: str() encoded as utf-8\n    \"\"\"\n    if u is None:\n        return None\n    try:\n        # direct bytes-like\n        if isinstance(u, (bytes, bytearray)):\n            return bytes(u)\n        if isinstance(u, memoryview):\n            return u.tobytes()\n\n        # dict mapping filename -> info (common FileUpload.value)\n        if isinstance(u, dict):\n            # pick first file\n            for fname, info in u.items():\n                # ipywidgets FileUpload uses 'content' (bytes) or 'metadata' with 'content' sometimes\n                if isinstance(info, dict):\n                    cont = info.get(\"content\") or info.get(\"data\") or info.get(\"content_bytes\")\n                    if isinstance(cont, (bytes, bytearray)):\n                        return bytes(cont)\n                    if isinstance(cont, memoryview):\n                        return cont.tobytes()\n                    # some wrappers store bytes directly at top-level\n                    for k in (\"content\",\"data\",\"body\"):\n                        val = info.get(k)\n                        if isinstance(val, (bytes, bytearray)):\n                            return bytes(val)\n                    # sometimes the dict contains a 'metadata' or base64 string - try best-effort\n                    if \"metadata\" in info and isinstance(info[\"metadata\"], dict):\n                        # nothing reliable to extract - continue to other keys or fallback\n                        pass\n                    # if info itself is bytes\n                if isinstance(info, (bytes, bytearray)):\n                    return bytes(info)\n            return None\n\n        # list/tuple possibilities\n        if isinstance(u, (list, tuple)) and len(u) > 0:\n            first = u[0]\n            # common pattern: [(filename, { 'content': bytes, ...})]\n            if isinstance(first, (list, tuple)) and len(first) >= 2:\n                info = first[1]\n                if isinstance(info, dict):\n                    cont = info.get(\"content\") or info.get(\"data\")\n                    if isinstance(cont, (bytes, bytearray)):\n                        return bytes(cont)\n                if isinstance(info, (bytes, bytearray)):\n                    return bytes(info)\n            # maybe it's a list of dicts with 'content'\n            if isinstance(first, dict):\n                cont = first.get(\"content\") or first.get(\"data\")\n                if isinstance(cont, (bytes, bytearray)):\n                    return bytes(cont)\n            # otherwise, if first element is bytes\n            if isinstance(first, (bytes, bytearray)):\n                return bytes(first)\n\n        # fallback: attempt to stringify and encode\n        return str(u).encode(\"utf-8\")\n    except Exception as e:\n        print(\"‚ùó _extract_uploaded_bytes_fallback error:\", e)\n        try:\n            return str(u).encode(\"utf-8\")\n        except:\n            return None\n\n# Primary run handler (same behavior as original but robust extraction)\ndef run_from_ui(_):\n    out_area.clear_output()\n    with out_area:\n        chosen = sample_dropdown.value if 'sample_dropdown' in globals() else None\n        uploaded = file_uploader.value if 'file_uploader' in globals() else None\n\n        # Use local robust extractor first; fall back to previously-defined one if present\n        uploaded_bytes = None\n        try:\n            if uploaded:\n                # Try user's extractor if available, but guard it in try/except\n                if 'extract_bytes_from_upload' in globals() and callable(globals().get('extract_bytes_from_upload')):\n                    try:\n                        uploaded_bytes = extract_bytes_from_upload(uploaded)\n                        print(\"run_from_ui: used existing extract_bytes_from_upload ->\", \"present\" if uploaded_bytes else \"none\")\n                    except Exception as e:\n                        print(\"run_from_ui: existing extract_bytes_from_upload raised error, falling back:\", e)\n                        uploaded_bytes = _extract_uploaded_bytes_fallback(uploaded)\n                        print(\"run_from_ui: fallback extractor ->\", \"present\" if uploaded_bytes else \"none\")\n                else:\n                    uploaded_bytes = _extract_uploaded_bytes_fallback(uploaded)\n                    print(\"run_from_ui: fallback extractor ->\", \"present\" if uploaded_bytes else \"none\")\n            else:\n                print(\"run_from_ui: no uploaded value detected (file_uploader.value is empty)\")\n                uploaded_bytes = None\n        except Exception as e:\n            print(\"run_from_ui: extraction failed with unexpected error:\", e)\n            uploaded_bytes = None\n\n        # Resolve uploaded filename (best-effort)\n        uploaded_fname = None\n        try:\n            if isinstance(uploaded, dict):\n                uploaded_fname = next(iter(uploaded.keys()), None)\n            elif isinstance(uploaded, (list,tuple)) and len(uploaded) > 0:\n                first = uploaded[0]\n                if isinstance(first, (list,tuple)) and len(first) > 0:\n                    uploaded_fname = first[0]\n                elif isinstance(first, dict):\n                    # maybe dict contains 'name' or 'filename'\n                    uploaded_fname = first.get(\"name\") or first.get(\"filename\") or None\n            print(\"run_from_ui: uploaded filename resolved ->\", uploaded_fname)\n        except Exception:\n            uploaded_fname = None\n\n        # Main run logic\n        results = None\n        if uploaded_bytes:\n            try:\n                parsed_lines = parse_uploaded_file_bytes(uploaded_bytes) if 'parse_uploaded_file_bytes' in globals() else uploaded_bytes.decode(\"utf-8\", errors=\"replace\").splitlines()\n            except Exception as e:\n                print(\"run_from_ui: parse_uploaded_file_bytes failed, falling back to naive decode:\", e)\n                parsed_lines = uploaded_bytes.decode(\"utf-8\", errors=\"replace\").splitlines()\n\n            detected = detect_scenario_from_lines(parsed_lines) if 'detect_scenario_from_lines' in globals() else None\n\n            if detected:\n                scenario_key = detected\n            elif chosen and chosen in SYNTHETIC_LOGS and chosen != \"-- select demo sample --\":\n                scenario_key = chosen\n                detected = None\n            else:\n                scenario_key = next(iter(SYNTHETIC_LOGS.keys()))\n\n            print(f\"Running diagnosis on uploaded file '{uploaded_fname or 'uploaded'}' ‚Äî detected scenario: {detected or 'none'} (using scenario key: {scenario_key})\")\n            results = run_offline_samples(samples={scenario_key: uploaded_bytes}, show=False, save_reports=False, out_dir=\"submission_reports\")\n\n        elif chosen and chosen in SYNTHETIC_LOGS and chosen != \"-- select demo sample --\":\n            print(f\"Running demo sample: {chosen}\")\n            results = run_offline_samples(samples={chosen: (\"\\n\".join(SYNTHETIC_LOGS[chosen])).encode(\"utf-8\")}, show=False, save_reports=False, out_dir=\"submission_reports\")\n\n        else:\n            print(\"Select a demo sample or upload a log file.\")\n            prev_path = globals().get(\"UPLOADED_FILE_PATH\")\n            prev_url  = globals().get(\"UPLOADED_FILE_URL\")\n            if prev_path and prev_url and Path(prev_path).exists():\n                display(HTML(f\"<small>Previously uploaded file (for reference only): <a href='{prev_url}' target='_blank'>{prev_path}</a></small>\"))\n            else:\n                print(\"No previously uploaded file available.\")\n            return\n\n        if not results:\n            print(\"No results returned.\")\n            return\n\n        incident_id, md, rep_json, saved_info = results[0]\n\n        if not md:\n            print(\"No draft report produced.\")\n            return\n\n        print(\"\\nDraft produced. Click 'Save Report' below to persist the report (and uploaded file if present).\")\n        display(Markdown(md)) # Displays the Markdown content\n\n        # --- FIX: EXPLICITLY RENDER PDF DOWNLOAD BUTTON ---\n        if 'render_pdf_download_button' in globals():\n            # Generate the button HTML using the raw Markdown (md)\n            pdf_button_html = render_pdf_download_button(incident_id, md)\n            \n            # Display the HTML button directly (the notebook renders this immediately)\n            display(HTML(pdf_button_html))\n            \n            # Update the session's report MD (which is what gets saved) to include the button HTML\n            # This ensures that if the agent's internal logic is bypassed, the session is consistent.\n            # We must use SESSIONS[incident_id] as 'md' is just the returned string\n            if incident_id in SESSIONS and SESSIONS[incident_id].get(\"report\"):\n                SESSIONS[incident_id][\"report\"][\"md\"] += \"\\n\\n\" + pdf_button_html\n        # --- END FIX ---\n        \n        # Wire save controls gracefully\n        if 'wire_save_controls' in globals() and callable(globals().get('wire_save_controls')):\n            try:\n                wire_save_controls(incident_id, uploaded_filename=uploaded_fname, uploaded_bytes=uploaded_bytes)\n            except Exception as e:\n                print(\"run_from_ui: wire_save_controls failed:\", e)\n                if 'approve_and_save_local' in globals() and callable(globals().get('approve_and_save_local')):\n                    print(\"run_from_ui: falling back to approve_and_save_local.\")\n                    out = approve_and_save_local(incident_id, uploaded_bytes=uploaded_bytes, uploaded_filename=uploaded_fname, out_dir=\"submission_reports\")\n                    print(\"approve_and_save_local result:\", out)\n                else:\n                    print(\"No fallback save function available. Please call approve_and_save_local(...) manually to save.\")\n        elif 'approve_and_save_local' in globals() and callable(globals().get('approve_and_save_local')):\n            print(\"run_from_ui: using approve_and_save_local directly (wire_save_controls not present).\")\n            out = approve_and_save_local(incident_id, uploaded_bytes=uploaded_bytes, uploaded_filename=uploaded_fname, out_dir=\"submission_reports\")\n            print(\"approve_and_save_local result:\", out)\n        else:\n            print(\"run_from_ui: neither wire_save_controls nor approve_and_save_local are available. Report cannot be saved via UI automatically.\")\n\n# Inspector UI (unchanged)\ndef inspect_ui(_):\n    out_area.clear_output()\n    with out_area:\n        print(\"=== TRACE STORE (most recent) ===\")\n        if 'render_trace_store' in globals():\n            render_trace_store()\n        else:\n            print(\"render_trace_store not available.\")\n        print(\"\\n=== SESSIONS ===\")\n        if 'render_sessions' in globals():\n            render_sessions()\n        else:\n            print(\"render_sessions not available.\")\n        print(\"\\n=== MEMORY_BANK SUMMARY ===\")\n        if not globals().get(\"MEMORY_BANK\"):\n            display(HTML(\"<i>Memory bank empty.</i>\"))\n        else:\n            for m in globals().get(\"MEMORY_BANK\",[])[-10:]:\n                display(HTML(f\"<pre>{json.dumps({'incident_id':m.get('incident_id'),'job':m.get('job'),'text_hash':m.get('text_hash'),'created_at':m.get('created_at')}, indent=2)}</pre>\"))\n\n# Hook up callbacks safely (idempotent)\ntry:\n    if 'run_button' in globals() and hasattr(run_button, \"on_click\"):\n        run_button.on_click(run_from_ui)\n    if 'inspect_button' in globals() and hasattr(inspect_button, \"on_click\"):\n        inspect_button.on_click(inspect_ui)\n    print(\"‚úÖ UI: callbacks hooked for run_button and inspect_button (if available).\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è UI: hooking callbacks failed (maybe already hooked):\", e)\n\n# Display UI row if widgets created\nif 'sample_dropdown' in globals() and globals().get(\"UI_AVAILABLE\"):\n    ui_row = widgets.VBox([\n        widgets.HTML(\"<h3>PulseTrace ‚Äî demo UI (updated)</h3>\"),\n        widgets.HBox([sample_dropdown, file_uploader, run_button, inspect_button]),\n        out_area,\n        (widgets.HTML(f\"<small>Previously uploaded file (for reference only): <a href='{globals().get('UPLOADED_FILE_URL')}' target='_blank'>{globals().get('UPLOADED_FILE_PATH')}</a></small>\")\n           if globals().get('UPLOADED_FILE_PATH') and globals().get('UPLOADED_FILE_URL') else widgets.HTML(\"<small>No previously uploaded file.</small>\"))\n    ])\n    display(ui_row)\n    print(\"‚úÖ UI: displayed.\")\nelse:\n    print(\"‚ÑπÔ∏è UI: widgets not available; UI row not rendered.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T10:59:50.921034Z","iopub.execute_input":"2025-11-26T10:59:50.921326Z","iopub.status.idle":"2025-11-26T10:59:51.035297Z","shell.execute_reply.started":"2025-11-26T10:59:50.921294Z","shell.execute_reply":"2025-11-26T10:59:51.034222Z"}},"outputs":[{"name":"stdout","text":"‚ñ∂Ô∏è UI: setting up run handler, inspector, hooking callbacks, and rendering UI...\n‚úÖ UI: callbacks hooked for run_button and inspect_button (if available).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<h3>PulseTrace ‚Äî demo UI (updated)</h3>'), HBox(children=(Dropdown(description='Dem‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfce7846d3ec4787aab87b19c6316694"}},"metadata":{}},{"name":"stdout","text":"‚úÖ UI: displayed.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### üõ†Ô∏è Troubleshooting\n\nThis section helps diagnose issues that may appear while running the PulseTrace demo.  \nUse the points below to understand and resolve common problems.\n\n\n### ‚öôÔ∏è UI Panel Not Showing\nIf the UI dropdown, file uploader, or buttons do not appear:\n\n- `ipywidgets` may not be available in the environment.  \n- Ensure the `UI_AVAILABLE` flag printed during setup shows **True**.\n- If widgets cannot be installed, you can still run PulseTrace manually using:\n  ```python\n  run_offline_samples(...)\n  ```\n\n\n### üßæ No Draft Report Generated\nIf the UI prints **‚ÄúNo draft report produced.‚Äù**:\n\n- The Diagnoser may not have produced a signature (often due to log parsing failures).\n- Ensure your uploaded file contains readable text lines.\n- Use the Inspector UI to verify:\n  - **Trace Store** ‚Üí agent flow reached Advisor  \n  - **Sessions** ‚Üí signature, history, and impact exist\n\n\n### üì° Traces / Sessions Not Visible\nIf the Inspector UI shows:\n\n- `render_trace_store not available`  \n- `render_sessions not available`\n\nThen:\n\n- Confirm that the Trace & Session Rendering Helpers cell was executed.\n- Verify that `TRACE_STORE` and `SESSIONS` were initialized earlier.\n- You may also inspect manually:\n  ```python\n  TRACE_STORE[-5:]\n  SESSIONS.keys()\n  ```\n\n\n### ‚òÅÔ∏è Gemini / Hybrid Mode Errors\nIf Gemini calls fail or hybrid mode doesn‚Äôt activate:\n\n- Ensure `GOOGLE_API_KEY` is added to Kaggle Secrets.\n- Verify the environment setup printed **‚ÄúGemini Mode: ON‚Äù**.\n- In restricted environments, Gemini is automatically disabled.\n- You can also force offline mode by setting:\n  ```python\n  USE_GEMINI = False\n  ```\n\n\n### üìÅ Uploaded File Not Detected\nIf you see:\n\n- **‚Äúno uploaded value detected‚Äù**  \n- or  \n  **‚ÄúSelect a demo sample or upload a log file.‚Äù**\n\nThen:\n\n- Make sure you selected a file *and* clicked **Run Diagnosis** after uploading.\n- Some notebook environments return unusual `FileUpload.value` structures ‚Äî the fallback extractor handles most, but not all malformed objects.\n- Try uploading a simple `.log` or `.txt` file.\n\n\n### ‚õìÔ∏è General Diagnostics\nIf PulseTrace behaves unexpectedly:\n\n- Ensure all previous cells executed without errors.\n- Confirm router behavior by checking Trace Store.\n- Test a synthetic scenario manually:\n  ```python\n  run_offline_samples(show=True)\n  ```\n- If you modified agent code, ensure every agent still defines a valid `on_message()` method.\n\n\n### ‚úÖ Quick Troubleshooting Checklist\n\n- [ ] UI visible (`UI_AVAILABLE == True`)  \n- [ ] Uploaded file detected or sample selected  \n- [ ] Diagnoser produced a signature (`SESSIONS[...][\"signature\"]` exists)  \n- [ ] A2A traces appear in Trace Store  \n- [ ] Advisor produced Markdown draft (`SESSIONS[...][\"report\"][\"md\"]`)  \n- [ ] Gemini optional ‚Äî offline mode fully functional\n","metadata":{}},{"cell_type":"markdown","source":"### ‚ö†Ô∏è Current Limitations\n\nPulseTrace is a functional multi-agent RCA demo, but it operates within a few intentional constraints:\n\n- **Offline-first design**  \n  The system runs fully offline by default using synthetic logs, schemas, and lineage metadata. Real system integrations are not included.\n\n- **Simplified failure patterns**  \n  Scenario detection is optimized for structured log formats and may not generalize to noisy, multi-stage, or unstructured logs.\n\n- **No persistent storage layer**  \n  Reports are displayed in the UI but not permanently written to disk unless the save function is explicitly triggered.\n\n- **Basic agent memory**  \n  Memory stores lightweight incident fingerprints but does not include embeddings, similarity search, or long-term vector memory.\n\n- **Notebook UI dependence**  \n  The interactive UI relies on `ipywidgets`. Environments without widget support must use manual execution via code.\n\n- **Hybrid Gemini mode is optional**  \n  PulseTrace supports live online reasoning via Gemini when an API key is present, but only specific steps (like log summarization) use it currently.\n\nThese limitations allow PulseTrace to stay lightweight and responsive while still demonstrating a complete multi-agent RCA workflow.\n","metadata":{}},{"cell_type":"markdown","source":"### üöÄ What's Next for PulseTrace\n\nPulseTrace already demonstrates a complete multi-agent RCA workflow, but there are several exciting directions to expand the system:\n\n- **Add real integrations**  \n  Connect to actual log stores, schema registries, lineage tools, and monitoring systems instead of offline simulation.\n\n- **Strengthen Gemini hybrid mode**  \n  Route more agent reasoning through Gemini when available and add richer summaries or deeper log insights.\n\n- **Extend the Diagnoser**  \n  Implement anomaly detection, data quality checks, and graph-based propagation logic for more complex failures.\n\n- **Improve the UI**  \n  Add collapsible panels, richer report previews, and a timeline view of agent-to-agent messages.\n\n- **Model memory enhancements**  \n  Store richer historical fingerprints and use them to surface smarter, pattern-based suggestions.\n\nThese additions will help PulseTrace evolve from a demo into a robust, production-grade RCA assistant for data engineering workflows.\n","metadata":{}},{"cell_type":"markdown","source":"### üìù Conclusion\n\nPulseTrace demonstrates how a coordinated multi-agent system can streamline root cause analysis for data pipeline failures. By combining deterministic tools, message-based agent orchestration, hybrid Gemini reasoning, and optional UI interaction, it provides a clear blueprint for building intelligent, modular RCA systems.\n\nThe workflow‚ÄîDetector ‚Üí Diagnoser ‚Üí History Analyzer ‚Üí Impact Analyzer ‚Üí Advisor‚Äîshows how specialized agents can collaborate, exchange context, and synthesize a final explanation that is both actionable and transparent.\n\nAlthough this notebook runs on simulated data, the architecture is designed to extend naturally to real monitoring systems, log stores, lineage platforms, and large-scale data ecosystems. With further enhancements in memory, anomaly detection, and integrations, PulseTrace can evolve into a fully capable, production-ready RCA assistant for modern data engineering teams.\n\n**Thank you for exploring PulseTrace!**\n","metadata":{}}]}