{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13882489,"sourceType":"datasetVersion","datasetId":8844839}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üîç PulseTrace ‚Äî Multi-Agent Root Cause Analysis for Data Pipelines\n\nPulseTrace automates how data engineers investigate failing pipelines using a **coordinated multi-agent system**.  \nInstead of manually scanning logs, schemas, lineage, and past incidents, PulseTrace orchestrates:\n\n- **Detector Agent** ‚Äî detects failures and triggers analysis  \n- **Diagnoser Agent** ‚Äî fetches logs, schema diffs, sample data, and builds incident signatures  \n- **History Analyzer Agent** ‚Äî matches incidents against the memory bank  \n- **Impact Analyzer Agent** ‚Äî determines downstream blast radius using lineage  \n- **Advisor Agent** ‚Äî synthesizes the final RCA report and fix recommendations  \n\n\n## üîß Technology Implemented Behind PulseTrace\n\nPulseTrace demonstrates:\n- a **multi-agent workflow**  \n- **asynchronous agent-to-agent (A2A) messaging** using an in-memory bus  \n- custom diagnostic tools  \n- observability via `TRACE_STORE`  \n- per-incident session state (`SESSIONS`)  \n- historical memory bank (`MEMORY_BANK`)  \n- hybrid deterministic + LLM reasoning  \n- human confirmation steps  \n","metadata":{}},{"cell_type":"markdown","source":"## üìò Notebook Roadmap\n\n- üîß Setup & environment checks (offline mode + optional Gemini)\n- ‚ñ∂Ô∏è How to Run This Notebook\n- üß≠ Architecture overview & how the agents interact  \n- ü§ñ Agent implementations (Diagnoser, History Analyzer, Impact Analyzer, Advisor)  \n- ‚ñ∂Ô∏è Orchestrator: run an end-to-end RCA demo using offline samples\n- üì° Observability: traces, sessions, memory bank \n- üé® Interactive UI (ipywidgets) ‚Äî upload logs, run RCA, approve & save reports\n- üöÄ Agent Deployment (Vertex AI Agent Engine)\n- üõ† Troubleshooting & diagnostics\n- ‚ö†Ô∏è Current Limitations\n- üöÄ What's Next steps for PulseTrace\n- üìù Conclusion  \n","metadata":{}},{"cell_type":"markdown","source":"## üöÄ How to Run PulseTrace  \n\nRunning PulseTrace is simple and intentionally structured to feel smooth end-to-end.  \nFollow these steps, and you‚Äôll have the full RCA pipeline running in minutes.\n\n### **1. Run the Setup Cells**  \nStart at the top of the notebook and execute each setup cell in order.  \nThese cells:  \n- Prepare the environment  \n- Register all agents  \n- Load synthetic log samples  \n- Initialize helper utilities  \n\nYou‚Äôll see clear confirmation messages as components are loaded.\n\n### **2. (Optional) Execute the Demo Flow**  \nFind the cell titled **‚ÄúRun demo & quick validation.‚Äù**  \nRunning it gives you a quick sanity check:  \n- A complete RCA workflow runs automatically  \n- Traces and sessions are generated  \n- A draft report is produced and validated  \n\nThis helps confirm everything is wired correctly.\n\n### **3. Use the Interactive UI**  \nScroll to the section titled **‚ÄúInteractive UI (ipywidgets)‚Äù**.  \nHere you can:  \n- Upload your own log file  \n- Or choose a demo sample  \n- Click **Run Diagnosis** to execute the full RCA pipeline  \n- Inspect **Traces**, **Sessions**, and **Memory Bank** live  \n- Approve and save reports as needed  \n\nThe UI is designed to be fast, clear, and beginner-friendly.\n\n\n### **4. Inspect Internal Activity (Optional)**  \nUse the **Observability** section to view:  \n- Agent-to-Agent event traces  \n- Active sessions  \n- Memory bank summaries  \n\nThese tools help verify‚Äîand showcase‚Äîhow the pipeline behaves under the hood.\n\n\n### **5. Explore Further Sections**  \nThe notebook ends with:  \n- Troubleshooting  \n- Current limitations  \n- Next steps  \n- A clean conclusion  \n\n**You're ready to run PulseTrace.  \nFollow the notebook from top to bottom, and the workflow will run smoothly end-to-end.**\n","metadata":{}},{"cell_type":"markdown","source":"## üîß Environment Setup & Mode Detection\n\nThis cell initializes the core environment for PulseTrace:\n- loads helper libraries  \n- prepares global state (sessions, trace store, memory bank)  \n- checks whether Gemini is available  \n- configures hybrid mode automatically (fallback-safe)  \n- detects UI support (ipywidgets)\n","metadata":{}},{"cell_type":"markdown","source":"## (1) Optional: Install ipywidgets (only if missing)\nThis cell checks whether `ipywidgets` is installed and installs it only if required.\n","metadata":{}},{"cell_type":"code","source":"# install ipywidgets if missing (with readable debug logs)\nprint(\"üîß Checking for ipywidgets...\")\n\ntry:\n    import ipywidgets\n    print(\"‚úî ipywidgets is already installed.\")\nexcept ImportError:\n    print(\"‚ö† ipywidgets not installed. Installing ipywidgets==7.7.1 ...\")\n    import sys\n    !{sys.executable} -m pip install ipywidgets==7.7.1\n\n    print(\"‚è≥ Installation attempted. A kernel restart may be required depending on the environment.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:57:59.505744Z","iopub.execute_input":"2025-11-28T16:57:59.506146Z","iopub.status.idle":"2025-11-28T16:57:59.640807Z","shell.execute_reply.started":"2025-11-28T16:57:59.506112Z","shell.execute_reply":"2025-11-28T16:57:59.639778Z"}},"outputs":[{"name":"stdout","text":"üîß Checking for ipywidgets...\n‚úî ipywidgets is already installed.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## (2) Optional: Google API key setup (Kaggle Secrets)\nTries to load `GOOGLE_API_KEY`. If found, saves it to the environment for Gemini.\n","metadata":{}},{"cell_type":"code","source":"# Kaggle secret loader (with readable status messages)\nprint(\"üîë Checking for GOOGLE_API_KEY in Kaggle Secrets...\")\n\nimport os\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n\n    key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n\n    if key:\n        os.environ[\"GOOGLE_API_KEY\"] = key\n        print(\"‚úî GOOGLE_API_KEY successfully loaded into environment.\")\n    else:\n        print(\"‚ö† GOOGLE_API_KEY not found in Kaggle Secrets. Continuing in offline mode.\")\n\nexcept Exception as e:\n    print(\"‚ö† Could not load GOOGLE_API_KEY from Kaggle Secrets.\")\n    print(\"   Reason:\", e)\n    print(\"   Proceeding in offline mode.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:57:59.641809Z","iopub.execute_input":"2025-11-28T16:57:59.642098Z","iopub.status.idle":"2025-11-28T16:57:59.801659Z","shell.execute_reply.started":"2025-11-28T16:57:59.642075Z","shell.execute_reply":"2025-11-28T16:57:59.800465Z"}},"outputs":[{"name":"stdout","text":"üîë Checking for GOOGLE_API_KEY in Kaggle Secrets...\n‚úî GOOGLE_API_KEY successfully loaded into environment.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## (3) Environment Summary & Mode Detection\nDetects:\n- Gemini availability  \n- ipywidgets UI availability  \n- Prints a clean summary block\n","metadata":{}},{"cell_type":"code","source":"# Environment summary & detection with detailed print messages\nprint(\"üåê Initializing environment summary...\")\n\n# Detect Gemini availability\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\nUSE_GEMINI = bool(GOOGLE_API_KEY)\n\n# Detect UI availability\ntry:\n    import ipywidgets as widgets\n    from IPython.display import display, clear_output\n    UI_AVAILABLE = True\nexcept Exception:\n    UI_AVAILABLE = False\n\n# Print summary (with fallbacks if rich is not installed)\nsummary_lines = [\n    \"\\n=== PulseTrace Environment Summary ===\",\n    f\"Gemini Mode: {'ON (API key detected)' if USE_GEMINI else 'OFF (no API key found)'}\",\n    f\"UI Mode: {'ENABLED (ipywidgets available)' if UI_AVAILABLE else 'UNAVAILABLE'}\",\n]\n\ntry:\n    from rich import print as rprint\n    for line in summary_lines:\n        rprint(line)\nexcept Exception:\n    for line in summary_lines:\n        print(line)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:57:59.806291Z","iopub.execute_input":"2025-11-28T16:57:59.806834Z","iopub.status.idle":"2025-11-28T16:57:59.893875Z","shell.execute_reply.started":"2025-11-28T16:57:59.806797Z","shell.execute_reply":"2025-11-28T16:57:59.892529Z"}},"outputs":[{"name":"stdout","text":"üåê Initializing environment summary...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\n=== PulseTrace Environment Summary ===\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n=== PulseTrace Environment Summary ===\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Gemini Mode: ON \u001b[1m(\u001b[0mAPI key detected\u001b[1m)\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Gemini Mode: ON <span style=\"font-weight: bold\">(</span>API key detected<span style=\"font-weight: bold\">)</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"UI Mode: ENABLED \u001b[1m(\u001b[0mipywidgets available\u001b[1m)\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">UI Mode: ENABLED <span style=\"font-weight: bold\">(</span>ipywidgets available<span style=\"font-weight: bold\">)</span>\n</pre>\n"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## (4) Configure Gemini Client (defensive)\nAttempts to import and configure `google-generativeai`.  \nFalls back safely if unavailable or misconfigured.\n","metadata":{}},{"cell_type":"code","source":"# Gemini client initialization with clear debugging output\nprint(\"ü§ñ Checking if Gemini client can be initialized...\")\n\nUSE_GEMINI = bool(os.getenv(\"GOOGLE_API_KEY\"))\n\nif USE_GEMINI:\n    print(\"üîç GOOGLE_API_KEY detected. Attempting to configure Gemini client...\")\n    try:\n        import google.generativeai as genai\n        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n        print(\"‚úî Gemini client configured successfully.\")\n    except Exception as e:\n        print(\"‚ùå Failed to initialize Gemini client.\")\n        print(\"   Falling back to deterministic offline mode.\")\n        print(\"   Reason:\", e)\n        USE_GEMINI = False\nelse:\n    print(\"‚ö† Gemini disabled ‚Äî no GOOGLE_API_KEY found. Running in offline deterministic mode.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:57:59.895113Z","iopub.execute_input":"2025-11-28T16:57:59.895414Z","iopub.status.idle":"2025-11-28T16:58:02.128984Z","shell.execute_reply.started":"2025-11-28T16:57:59.895371Z","shell.execute_reply":"2025-11-28T16:58:02.127989Z"}},"outputs":[{"name":"stdout","text":"ü§ñ Checking if Gemini client can be initialized...\nüîç GOOGLE_API_KEY detected. Attempting to configure Gemini client...\n‚úî Gemini client configured successfully.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## üß© Core Runtime Structures  \nThis cell initializes all global stores, time helpers, hashing helpers, and idempotent guards used across the PulseTrace engine.  \nThese are required before tools, agents, or the router can function.\n","metadata":{}},{"cell_type":"code","source":"print(\"üîß Initializing core runtime structures...\")\n\nimport os, time, uuid, json, re, hashlib, traceback, pathlib, threading\nfrom collections import Counter, deque\nfrom IPython.display import display, Markdown\n\nif \"_PULSETRACE_FINAL\" not in globals():\n    _PULSETRACE_FINAL = True\n    DEMO_SEED = True\n\n    def now_ts(): \n        return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\n    def make_id(prefix=\"inc\"): \n        return f\"{prefix}-{uuid.uuid4().hex[:8]}\"\n\n    def sha256_hex(s: str): \n        return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n\n    TRACE_STORE = []\n    IN_MEMORY_BUS = deque()\n    SESSIONS = {}\n    MEMORY_BANK = []\n    DRAFT_MEMORY = []\n\nprint(\"‚úÖ Core runtime structures ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.130033Z","iopub.execute_input":"2025-11-28T16:58:02.130456Z","iopub.status.idle":"2025-11-28T16:58:02.139485Z","shell.execute_reply.started":"2025-11-28T16:58:02.130415Z","shell.execute_reply":"2025-11-28T16:58:02.138130Z"}},"outputs":[{"name":"stdout","text":"üîß Initializing core runtime structures...\n‚úÖ Core runtime structures ready.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## ‚¨áÔ∏è Adding PDF Export and Report Utilities\n\nThis section introduces helper functions necessary to provide a **\"Download as PDF\"** option for the final report. \n\nSince the Jupyter UI is rendering a mix of Markdown and HTML, we implement a file-download mechanism that is common in web applications:\n\n- It uses the pure-Python **`reportlab`** library to convert the Markdown report into a raw PDF file format in memory.\n- The PDF bytes are then encoded using **Base64**.\n- The Base64 string is embedded into an **HTML download link** (`<a download>`) that triggers the file save when clicked.\n\nThis ensures the **integrity and professional formatting** of the final RCA report artifact.","metadata":{}},{"cell_type":"code","source":"# --- INSTALLATION ---\ntry:\n    import reportlab\nexcept ImportError:\n    !pip install reportlab\n\n# --- IMPORTS ---\nimport base64\nimport json\nfrom io import BytesIO\nfrom reportlab.lib import colors\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib.units import inch\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak, Image\nfrom reportlab.lib.enums import TA_CENTER, TA_LEFT\n\nprint(\"‚úÖ 'reportlab' loaded. Defining professional PDF generator with Blue Button...\")\n\n# --- PROFESSIONAL PDF GENERATOR (Same layout as before) ---\ndef create_professional_pdf(report_json):\n    buffer = BytesIO()\n    doc = SimpleDocTemplate(buffer, pagesize=letter, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=72)\n    styles = getSampleStyleSheet()\n    \n    # Custom Styles\n    style_title = ParagraphStyle('PulseTitle', parent=styles['Title'], fontSize=24, spaceAfter=20, textColor=colors.HexColor(\"#1f4e79\"))\n    style_heading = ParagraphStyle('PulseHeading', parent=styles['Heading2'], fontSize=16, spaceBefore=15, spaceAfter=10, textColor=colors.HexColor(\"#2e75b6\"), borderPadding=5)\n    style_subheading = ParagraphStyle('PulseSubHeading', parent=styles['Heading3'], fontSize=12, spaceBefore=10, textColor=colors.HexColor(\"#333333\"))\n    style_body = ParagraphStyle('PulseBody', parent=styles['BodyText'], fontSize=10, leading=14, spaceAfter=6)\n    style_code = ParagraphStyle('PulseCode', parent=styles['Code'], fontSize=8, leading=10, backColor=colors.whitesmoke, borderPadding=5)\n    \n    meta = report_json.get(\"meta\", {})\n    rca = report_json.get(\"root_cause\", {})\n    sev = report_json.get(\"severity\", {})\n    conf = report_json.get(\"confidence_breakdown\", {})\n    evidence = report_json.get(\"evidence\", {})\n    impact = report_json.get(\"impact\", {})\n    recs = report_json.get(\"recommended_next_steps\", [])\n    \n    Story = []\n    \n    # PAGE 1: COVER\n    Story.append(Spacer(1, 2 * inch))\n    Story.append(Paragraph(\"Incident Report\", style_title))\n    Story.append(Spacer(1, 0.2 * inch))\n    Story.append(Paragraph(f\"<b>Incident ID:</b> {meta.get('incident_id', 'N/A')}\", style_body))\n    Story.append(Paragraph(f\"<b>Job:</b> {meta.get('job', 'N/A')}\", style_body))\n    Story.append(Paragraph(f\"<b>Scenario:</b> {meta.get('scenario', 'N/A')}\", style_body))\n    Story.append(Paragraph(f\"<b>Generated:</b> {meta.get('generated_at', 'N/A')}\", style_body))\n    Story.append(Spacer(1, 1 * inch))\n    Story.append(Paragraph(\"Executive Summary\", style_heading))\n    summary_text = (f\"A <b>{sev.get('label', 'UNKNOWN')}</b> severity issue was detected in job <b>{meta.get('job')}</b>. \"\n                    f\"The root cause is hypothesized to be <b>{rca.get('hypothesis')}</b>. \"\n                    f\"{len(impact.get('downstreams', []))} downstream assets are potentially impacted.\")\n    Story.append(Paragraph(summary_text, style_body))\n    Story.append(Spacer(1, 2 * inch))\n    Story.append(Paragraph(\"Generated by PulseTrace RCA\", ParagraphStyle('Footer', parent=style_body, alignment=TA_CENTER, textColor=colors.grey)))\n    Story.append(PageBreak())\n\n    # PAGE 2: DETAILS\n    Story.append(Paragraph(\"2. Root Cause Analysis\", style_heading))\n    Story.append(Paragraph(f\"Hypothesis: {rca.get('hypothesis')}\", style_body))\n    \n    Story.append(Paragraph(\"3. Confidence Breakdown\", style_heading))\n    conf_data = [[\"Component\", \"Score\"], [\"Final Score\", f\"{conf.get('final_score')} ({conf.get('label')})\"],\n                 [\"History Score\", str(conf.get('history_score'))], [\"Field Drift\", str(conf.get('field_score'))], [\"Anomaly Score\", str(conf.get('anomaly_score'))]]\n    t_conf = Table(conf_data, colWidths=[200, 100], hAlign='LEFT')\n    t_conf.setStyle(TableStyle([('BACKGROUND', (0, 0), (1, 0), colors.HexColor(\"#e1e1e1\")), ('TEXTCOLOR', (0, 0), (1, 0), colors.black), ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'), ('GRID', (0, 0), (-1, -1), 0.5, colors.grey), ('PADDING', (0, 0), (-1, -1), 6)]))\n    Story.append(t_conf)\n\n    Story.append(Paragraph(\"4. Evidence\", style_heading))\n    Story.append(Paragraph(\"4.1 Log Excerpt\", style_subheading))\n    logs = evidence.get('logs', {}).get('lines', [])\n    if logs: Story.append(Paragraph(\"<br/>\".join([line[:120] for line in logs[:10]]), style_code))\n    \n    anomalies = evidence.get('sample_anomalies', {}).get('anomalies', [])\n    if anomalies:\n        Story.append(Paragraph(\"4.2 Sample Anomalies\", style_subheading))\n        anom_data = [[\"Row\", \"Field\", \"Value\"]] + [[str(a.get('row')), str(a.get('field')), str(a.get('value'))] for a in anomalies]\n        t_anom = Table(anom_data, colWidths=[60, 150, 200], hAlign='LEFT')\n        t_anom.setStyle(TableStyle([('BACKGROUND', (0, 0), (-1, 0), colors.HexColor(\"#fce4d6\")), ('GRID', (0, 0), (-1, -1), 0.5, colors.grey), ('PADDING', (0, 0), (-1, -1), 5)]))\n        Story.append(t_anom)\n\n    Story.append(Paragraph(\"5. Downstream Impact\", style_heading))\n    downstreams = impact.get('downstreams', [])\n    if downstreams:\n        imp_data = [[\"Asset\", \"Type\", \"Critical\"]] + [[d.get('asset'), d.get('type'), \"YES\" if d.get('critical') else \"No\"] for d in downstreams]\n        t_imp = Table(imp_data, colWidths=[250, 100, 80], hAlign='LEFT')\n        t_imp.setStyle(TableStyle([('BACKGROUND', (0, 0), (-1, 0), colors.HexColor(\"#d9e1f2\")), ('GRID', (0, 0), (-1, -1), 0.5, colors.grey), ('PADDING', (0, 0), (-1, -1), 5)]))\n        Story.append(t_imp)\n    \n    Story.append(Paragraph(\"6. Recommendations\", style_heading))\n    for i, rec in enumerate(recs, 1): Story.append(Paragraph(f\"{i}. {rec}\", style_body))\n\n    doc.build(Story)\n    buffer.seek(0)\n    return buffer.read()\n\ndef render_pdf_download_button(incident_id, report_json):\n    \"\"\"\n    Generates a Blue 'Download as PDF' button that delivers the Professional PDF.\n    \"\"\"\n    try:\n        pdf_bytes = create_professional_pdf(report_json)\n        pdf_base64 = base64.b64encode(pdf_bytes).decode('utf-8')\n        filename = f\"PulseTrace_Report_{incident_id}.pdf\"\n\n        html_button = f\"\"\"\n        <a download=\"{filename}\" href=\"data:application/pdf;base64,{pdf_base64}\">\n            <button style=\"background-color: #2196F3; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; font-size: 16px; margin-top: 20px;\">\n                ‚¨áÔ∏è Download as PDF\n            </button>\n        </a>\n        \"\"\"\n        return html_button\n    except Exception as e:\n        return f\"<b>Error generating PDF:</b> {e}\"\n\nprint(\"‚ú® Professional PDF utility loaded (Blue Button style restored).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.141054Z","iopub.execute_input":"2025-11-28T16:58:02.141423Z","iopub.status.idle":"2025-11-28T16:58:02.287117Z","shell.execute_reply.started":"2025-11-28T16:58:02.141389Z","shell.execute_reply":"2025-11-28T16:58:02.285983Z"}},"outputs":[{"name":"stdout","text":"‚úÖ 'reportlab' loaded. Defining professional PDF generator with Blue Button...\n‚ú® Professional PDF utility loaded (Blue Button style restored).\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## üß™ Demo Memory Seed + Synthetic Log Inputs  \nThis cell seeds the demo MEMORY_BANK (idempotent) and defines the synthetic log inputs used in offline RCA mode.\n","metadata":{}},{"cell_type":"code","source":"print(\"üß™ Seeding demo memory & loading synthetic logs...\")\n\ndef seed_demo_memory_once():\n    global MEMORY_BANK\n    if not DEMO_SEED: \n        print(\"  ‚Ü™ DEMO_SEED disabled.\")\n        return\n    if any(e.get(\"seeded\") for e in MEMORY_BANK): \n        print(\"  ‚Ü™ MEMORY_BANK already seeded.\")\n        return\n\n    MEMORY_BANK.clear()\n    seeded = [\n        {\n            \"incident_id\": \"past-inc-orders-2025-11-01\",\n            \"job\": \"orders\",\n            \"error_class\": \"TypeError\",\n            \"changed_fields\": [\"price\"],\n            \"sample_anomalies\": [{\"row\":10,\"field\":\"price\",\"value\":\"999.99\"}],\n            \"created_at\": \"2025-11-01 09:00:00\",\n            \"hint_snippet\": \"cannot cast '123.45' to INT\",\n        },\n        {\n            \"incident_id\": \"past-inc-pricing-2025-10-20\",\n            \"job\": \"pricing\",\n            \"error_class\": \"ValueError\",\n            \"changed_fields\": [],\n            \"sample_anomalies\": [{\"row\":400,\"field\":\"price\",\"value\":-5.0}],\n            \"created_at\": \"2025-10-20 14:30:00\",\n            \"hint_snippet\": \"negative value\",\n        }\n    ]\n\n    for e in seeded:\n        e[\"text_hash\"] = sha256_hex(\n            f\"{e['job']}|{e['error_class']}|{','.join(e.get('changed_fields',[]))}|{e.get('hint_snippet','')}\"\n        )\n        e[\"seeded\"] = True\n        e[\"source\"] = \"demo_seed\"\n        MEMORY_BANK.append(e)\n\n    print(\"  ‚úî MEMORY_BANK seeded:\", [m[\"incident_id\"] for m in MEMORY_BANK])\n\nseed_demo_memory_once()\n\nSYNTHETIC_LOGS = {\n    \"schema_drift\": [\n        \"2025-11-24 10:00:01 INFO job=orders ETL step=ingest files=3\",\n        \"2025-11-24 10:03:02 ERROR job=orders transform TypeError: cannot cast '123.45' to INT on column price\",\n    ],\n    \"missing_partition\": [\n        \"2025-11-24 11:00:05 ERROR job=reports No files found for partition dt=2025-11-24\",\n    ],\n    \"invalid_values\": [\n        \"2025-11-24 12:05:02 ERROR job=pricing transform ValueError: negative value found in price at row 524\",\n    ],\n}\n\nprint(\"‚úÖ Demo memory + synthetic logs loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.288325Z","iopub.execute_input":"2025-11-28T16:58:02.288746Z","iopub.status.idle":"2025-11-28T16:58:02.300002Z","shell.execute_reply.started":"2025-11-28T16:58:02.288719Z","shell.execute_reply":"2025-11-28T16:58:02.298845Z"}},"outputs":[{"name":"stdout","text":"üß™ Seeding demo memory & loading synthetic logs...\n  ‚úî MEMORY_BANK seeded: ['past-inc-orders-2025-11-01', 'past-inc-pricing-2025-10-20']\n‚úÖ Demo memory + synthetic logs loaded.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## üõ† Deterministic Diagnostic Tools  \nThis cell defines the lightweight deterministic tools used by agents:  \n- log_fetch  \n- schema_diff  \n- sample_data  \n- lineage_query  \n- history_query  \n- save_report  \nThese simulate a real pipeline environment for offline RCA.\n","metadata":{}},{"cell_type":"code","source":"print(\"üõ† Initializing deterministic diagnostic tools...\")\n\nclass LogFetch:\n    def run(self, params):\n        return {\"lines\": SYNTHETIC_LOGS.get(params.get(\"scenario\"), []), \"job\": None}\n\nclass SchemaDiff:\n    def run(self, params):\n        if params.get(\"scenario\") == \"schema_drift\":\n            return {\"diff\":[{\"field\":\"price\",\"old\":\"INT\",\"new\":\"FLOAT\"}]}\n        return {\"diff\":[]}\n\nclass SampleData:\n    def run(self, params):\n        scenario = params.get(\"scenario\")\n        if scenario == \"schema_drift\":\n            return {\n                \"rows\":[{\"id\":1,\"price\":\"123.45\"},{\"id\":2,\"price\":\"200.00\"}],\n                \"anomalies\":[{\"row\":1,\"field\":\"price\",\"value\":\"123.45\"}]\n            }\n        if scenario == \"invalid_values\":\n            return {\n                \"rows\":[{\"id\":524,\"price\":-12.5},{\"id\":100,\"price\":10.0}],\n                \"anomalies\":[{\"row\":524,\"field\":\"price\",\"value\":-12.5}]\n            }\n        return {\"rows\":[], \"anomalies\":[]}\n\nclass LineageQuery:\n    def run(self, params):\n        job = params.get(\"job\")\n        if job in (\"orders\",\"pricing\"):\n            return {\"downstreams\":[\n                {\"asset\":\"dashboard.sales_over_time\",\"type\":\"dashboard\",\"critical\":True},\n                {\"asset\":\"ml.revenue_forecast\",\"type\":\"model\",\"critical\":True}\n            ]}\n        if job == \"reports\":\n            return {\"downstreams\":[{\"asset\":\"dashboard.reports\",\"type\":\"dashboard\",\"critical\":False}]}\n        return {\"downstreams\":[]}\n\nclass HistoryQuery:\n    def run(self, params):\n        signature = params.get(\"signature\", {})\n        matches=[]\n        for past in MEMORY_BANK:\n            score = 0.0\n            if signature.get(\"job\") == past.get(\"job\"):\n                score += 0.4\n            cf=set(signature.get(\"changed_fields\",[])); pf=set(past.get(\"changed_fields\",[]))\n            if cf and pf:\n                overlap = len(cf & pf)/max(1,len(cf|pf))\n                score += 0.4*overlap\n            if signature.get(\"error_class\") == past.get(\"error_class\"):\n                score += 0.2\n            if score>0: \n                matches.append({\"past_incident\":past,\"score\":round(score,2)})\n        matches.sort(key=lambda x: x[\"score\"], reverse=True)\n        return {\"matches\": matches[:5]}\n\nclass SaveReport:\n    def run(self, params):\n        report_md = params.get(\"report_md\")\n        report_json = params.get(\"report_json\")\n        incident_id = params.get(\"incident_id\", make_id(\"rpt\"))\n        out_dir = params.get(\"out_dir\",\"submission_reports\")\n        os.makedirs(out_dir, exist_ok=True)\n        base = f\"{out_dir}/pulsetrace_report_{incident_id}\"\n        md_path = f\"{base}.md\"; json_path = f\"{base}.json\"\n        with open(md_path, \"w\") as f: f.write(report_md)\n        with open(json_path, \"w\") as f: json.dump(report_json, f, indent=2)\n        return {\"saved\": True, \"md_path\": md_path, \"json_path\": json_path}\n\nTOOLS = {\n    \"log_fetch\": LogFetch(),\n    \"schema_diff\": SchemaDiff(),\n    \"sample_data\": SampleData(),\n    \"lineage_query\": LineageQuery(),\n    \"history_query\": HistoryQuery(),\n    \"save_report\": SaveReport()\n}\n\nprint(\"‚úÖ Tools initialized.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.301183Z","iopub.execute_input":"2025-11-28T16:58:02.301458Z","iopub.status.idle":"2025-11-28T16:58:02.324589Z","shell.execute_reply.started":"2025-11-28T16:58:02.301435Z","shell.execute_reply":"2025-11-28T16:58:02.323206Z"}},"outputs":[{"name":"stdout","text":"üõ† Initializing deterministic diagnostic tools...\n‚úÖ Tools initialized.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## üîç Signature Builder & Existing Incident Lookup\n\nThese helpers generate the canonical incident signature and support\nidempotent behavior by checking whether the same incident signature\nalready exists in the current session or memory bank.\n\nThis ensures:\n- recurring incidents reuse past reports  \n- no duplicate work  \n- deterministic RCA behavior across runs  \n","metadata":{}},{"cell_type":"code","source":"print(\"üîß Loading: Signature builder & existing-entry lookup helpers...\")\n\n# -------------------------\n# Helpers: signature, find existing\n# -------------------------\ndef build_signature(incident, schema_diff_res, logs_res, samples_res):\n    job = incident.get(\"job\")\n    \n    # 1. Try the snippet provided\n    err = incident.get(\"error_snippet\", \"\")\n    \n    # 2. If snippet is weak (Unknown), try scanning the full logs if available\n    full_log_text = \"\\n\".join(logs_res.get(\"lines\", []))\n    \n    # Search for specific error classes in snippet OR full logs\n    regex = r\"(TypeError|ValueError|No files found|timeout|Missing|PermissionError)\"\n    \n    m = re.search(regex, err, re.I)\n    if not m and full_log_text:\n        m = re.search(regex, full_log_text, re.I)\n        \n    error_class = m.group(1) if m else \"Unknown\"\n\n    changed = [d[\"field\"] for d in schema_diff_res.get(\"diff\",[])]\n    text = f\"{job}|{error_class}|{','.join(changed)}|{err}\"\n    th = sha256_hex(text)\n\n    print(f\"üìå Signature built for job='{job}', error_class='{error_class}', changed_fields={changed}\")\n\n    return {\n        \"job\": job,\n        \"error_class\": error_class,\n        \"changed_fields\": changed,\n        \"text_hash\": th,\n        \"sample_anomalies\": samples_res.get(\"anomalies\", []),\n        \"created_at\": now_ts()\n    }\n\ndef find_existing_by_text_hash(text_hash):\n    print(f\"üîç Checking for existing incidents with text_hash={text_hash[:10]}...\")\n\n    for inc_id, sess in SESSIONS.items():\n        sig = sess.get(\"signature\") or {}\n        if sig.get(\"text_hash\") == text_hash:\n            print(f\"‚úî Found matching session: {inc_id}\")\n            return (\"session\", inc_id, sess)\n\n    for entry in MEMORY_BANK:\n        if entry.get(\"text_hash\") == text_hash:\n            print(f\"‚úî Found matching historical memory entry: {entry.get('incident_id')}\")\n            return (\"memory\", entry.get(\"incident_id\", None), entry)\n\n    print(\"‚ùå No matching signature found.\")\n    return (None, None, None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.326037Z","iopub.execute_input":"2025-11-28T16:58:02.326299Z","iopub.status.idle":"2025-11-28T16:58:02.351631Z","shell.execute_reply.started":"2025-11-28T16:58:02.326279Z","shell.execute_reply":"2025-11-28T16:58:02.350469Z"}},"outputs":[{"name":"stdout","text":"üîß Loading: Signature builder & existing-entry lookup helpers...\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## üìä Confidence Scoring Layer\n\nPulseTrace combines several signals to compute a final confidence score:\n\n- match strength with historical incidents  \n- schema field drift  \n- anomalies in sampled rows  \n- optional Gemini-based augmentation (hybrid mode)\n\nThe following section computes:\n- the final confidence score  \n- a detailed breakdown  \n- labels (HIGH / MEDIUM / LOW)  \n","metadata":{}},{"cell_type":"code","source":"print(\"üìà Loading: Confidence scoring helpers...\")\ndef confidence_label(score: float) -> str:\n    if score >= 0.80: return \"HIGH\"\n    if score >= 0.50: return \"MEDIUM\"\n    return \"LOW\"\n\ndef compute_confidence(signature: dict, history_matches: list = None, use_gemini: bool = False):\n    history_matches = history_matches or []\n    history_score = 0.0\n\n    if history_matches:\n        try:\n            history_score = max(float(m.get(\"score\", 0.0)) for m in history_matches)\n        except Exception:\n            history_score = 0.0\n\n    field_score = 1.0 if signature.get(\"changed_fields\") else 0.0\n    anomalies = signature.get(\"sample_anomalies\") or []\n    anomaly_count = len(anomalies)\n    anomaly_score = min(1.0, anomaly_count / 3.0)\n    gemini_score = 1.0 if use_gemini else 0.0\n\n    weights = {\"history\": 0.40, \"field\": 0.25, \"anomaly\": 0.25, \"gemini\": 0.10}\n\n    if not use_gemini:\n        s = weights[\"history\"] + weights[\"field\"] + weights[\"anomaly\"]\n        weights[\"history\"] /= s\n        weights[\"field\"]  /= s\n        weights[\"anomaly\"]/= s\n        weights[\"gemini\"] = 0.0\n\n    score = (\n        weights[\"history\"] * history_score +\n        weights[\"field\"] * field_score +\n        weights[\"anomaly\"] * anomaly_score +\n        weights[\"gemini\"] * gemini_score\n    )\n    score = max(0.0, min(1.0, round(score, 2)))\n\n    breakdown = {\n        \"history_score\": round(history_score,2),\n        \"field_score\": round(field_score,2),\n        \"anomaly_score\": round(anomaly_score,2),\n        \"gemini_score\": round(gemini_score,2),\n        \"weights\": {k: round(v,2) for k,v in weights.items()},\n        \"final_score\": score,\n        \"label\": confidence_label(score)\n    }\n\n    print(f\"üìä Confidence score computed: {score} ({breakdown['label']})\")\n\n    return score, breakdown\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.352835Z","iopub.execute_input":"2025-11-28T16:58:02.353318Z","iopub.status.idle":"2025-11-28T16:58:02.384433Z","shell.execute_reply.started":"2025-11-28T16:58:02.353291Z","shell.execute_reply":"2025-11-28T16:58:02.382771Z"}},"outputs":[{"name":"stdout","text":"üìà Loading: Confidence scoring helpers...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## üö® Severity Computation Layer\n\nSeverity is computed using:\n- downstream impact (critical dashboards/models)  \n- schema drift  \n- sample anomalies  \n\nThis produces a normalized severity score and label.\n","metadata":{}},{"cell_type":"code","source":"print(\"üö® Loading: Severity computation helpers...\")\n\n# -------------------------\n# Severity helpers\n# -------------------------\ndef compute_severity(signature, impact):\n    score = 0.0\n\n    # --- FIX: Robustly find the list of downstream assets ---\n    # This handles both nested impact structures and direct dictionaries\n    dlist = []\n    if isinstance(impact, dict):\n        dlist = impact.get(\"downstreams\") or impact.get(\"impact\", {}).get(\"downstreams\")\n\n    # --- Scoring Logic ---\n    if dlist:\n        # If any downstream asset is critical -> +0.6\n        if any(d.get(\"critical\") for d in dlist):\n            score += 0.6\n        # If downstreams exist but none are critical -> +0.2\n        else:\n            score += 0.2\n\n    # Add points for Schema Drift (+0.2)\n    if signature.get(\"changed_fields\"):\n        score += 0.2\n\n    # Add points for Sample Anomalies (+0.2)\n    if signature.get(\"sample_anomalies\"):\n        score += 0.2\n\n    final = min(1.0, round(score, 2))\n    print(f\"üö® Severity computed: {final}\")\n    \n    return final\n\ndef severity_label(score):\n    if score >= 0.75: return \"HIGH\"\n    if score >= 0.4: return \"MEDIUM\"\n    return \"LOW\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.385956Z","iopub.execute_input":"2025-11-28T16:58:02.386356Z","iopub.status.idle":"2025-11-28T16:58:02.416752Z","shell.execute_reply.started":"2025-11-28T16:58:02.386324Z","shell.execute_reply":"2025-11-28T16:58:02.415563Z"}},"outputs":[{"name":"stdout","text":"üö® Loading: Severity computation helpers...\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## üì® Message Bus & Router Core\n\nPulseTrace uses a simple FIFO in-memory message bus (`IN_MEMORY_BUS`)  \nto allow agents to send events to one another.\n\nThis layer contains:\n- `emit()` ‚Üí enqueue messages  \n- `pop_next_message()` ‚Üí get next event  \n- `router_once()` ‚Üí routes one event to the appropriate agent  \n- `router_run_blocking()` ‚Üí drains the queue  \n\nAll agents rely on this for communication.\n","metadata":{}},{"cell_type":"code","source":"print(\"üì® Loading: Message bus & router system...\")\n\n# -------------------------\n# Simple router & agents (deterministic + optional LLM augmentation)\n# -------------------------\ndef emit(frm: str, to: str, payload: dict):\n    msg = {\n        \"id\": uuid.uuid4().hex,\n        \"from\": frm,\n        \"to\": to,\n        \"ts\": now_ts(),\n        \"payload\": payload\n    }\n    TRACE_STORE.append(msg)\n    IN_MEMORY_BUS.append(msg)\n\n    print(f\"[emit] {frm} ‚Üí {to} | type={payload.get('type', payload.get('pattern',''))}\")\n    return msg\n\ndef pop_next_message():\n    return IN_MEMORY_BUS.popleft() if IN_MEMORY_BUS else None\n\ndef router_once():\n    msg = pop_next_message()\n    if not msg:\n        return False\n\n    to = msg[\"to\"]\n\n    print(f\"[router] Dispatching to agent: {to}\")\n\n    if to == \"pulse_detector\":\n        pulse_detector.on_message(msg)\n    elif to == \"root_cause_diagnoser\":\n        diagnoser.on_message(msg)\n    elif to == \"pattern_history_agent\":\n        history_agent.on_message(msg)\n    elif to == \"impact_scope_agent\":\n        impact_agent.on_message(msg)\n    elif to == \"pulse_advisor\":\n        advisor.on_message(msg)\n    elif to == \"save_report\":\n        pass\n    elif to == \"ui\":\n        pass\n\n    return True\n\ndef router_run_blocking():\n    print(\"‚ñ∂Ô∏è Router: starting event loop...\")\n    while IN_MEMORY_BUS:\n        router_once()\n    print(\"‚èπ Router: event queue empty.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.417812Z","iopub.execute_input":"2025-11-28T16:58:02.418095Z","iopub.status.idle":"2025-11-28T16:58:02.447244Z","shell.execute_reply.started":"2025-11-28T16:58:02.418074Z","shell.execute_reply":"2025-11-28T16:58:02.445845Z"}},"outputs":[{"name":"stdout","text":"üì® Loading: Message bus & router system...\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## üîé Pulse Detector Agent (`pulse_detector`)\n\nThis is the first agent in the multi-agent pipeline.\n\nIt:\n- receives uploaded logs or demo logs  \n- detects scenario (`schema_drift`, `invalid_values`, etc.)  \n- extracts job name and error snippet  \n- builds the initial incident object  \n- emits it to the Diagnoser agent  \n\nThis makes PulseTrace‚Äôs architecture fully aligned with the write-up.\n","metadata":{}},{"cell_type":"code","source":"# -------------------------\n# Detector Agent (pulse_detector)\n# -------------------------\nprint(\"üì® Loading: Detector Agent (pulse_detector)...\")\n\nclass PulseDetector:\n    \"\"\"\n    Lightweight failure detector.\n    Reads raw log lines and identifies scenario + job + error snippet.\n    Emits a detection payload to root_cause_diagnoser.\n    \"\"\"\n\n    def detect(self, lines):\n        text = \"\\n\".join(lines).lower()\n\n        # infer scenario\n        if \"cannot cast\" in text or \"cannot convert\" in text:\n            scenario = \"schema_drift\"\n        elif \"no files found\" in text:\n            scenario = \"missing_partition\"\n        elif \"negative value\" in text:\n            scenario = \"invalid_values\"\n        else:\n            scenario = \"unknown\"\n\n        # infer job\n        m = re.findall(r\"job=([A-Za-z0-9_\\-\\.]+)\", text)\n        job = Counter(m).most_common(1)[0][0] if m else \"unknown\"\n\n        return scenario, job\n\n    def on_message(self, msg):\n        payload = msg[\"payload\"]\n        lines = payload.get(\"lines\", [])\n        incident_id = payload.get(\"incident_id\", make_id(\"inc\"))\n\n        if not lines:\n            print(\"[pulse_detector] No log lines provided.\")\n            return\n\n        scenario, job = self.detect(lines)\n        error_snip = lines[0] if lines else \"\"\n\n        print(f\"[pulse_detector] Scenario={scenario}, job={job}\")\n\n        emit(\n            \"pulse_detector\",\n            \"root_cause_diagnoser\",\n            {\n                \"type\": \"detection\",\n                \"incident_id\": incident_id,\n                \"scenario\": scenario,\n                \"job\": job,\n                \"error_snippet\": error_snip,\n                \"source_lines_count\": len(lines),\n                \"pattern\": \"detected_by_detector\"\n            }\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.448504Z","iopub.execute_input":"2025-11-28T16:58:02.449056Z","iopub.status.idle":"2025-11-28T16:58:02.472275Z","shell.execute_reply.started":"2025-11-28T16:58:02.449013Z","shell.execute_reply":"2025-11-28T16:58:02.471143Z"}},"outputs":[{"name":"stdout","text":"üì® Loading: Detector Agent (pulse_detector)...\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## üß† Diagnoser Agent (`root_cause_diagnoser`)\n\nThis is the core reasoning agent.\n\nIt:\n- fetches logs  \n- detects schema drift  \n- collects sample rows  \n- builds the incident signature  \n- optionally enriches logs using Gemini (hybrid mode)  \n- sends signature to History + Impact agents  \n\nThis is the heart of the RCA workflow.\n","metadata":{}},{"cell_type":"code","source":"print(\"üß† Loading: Diagnoser agent...\")\n\nclass Diagnoser:\n    def on_message(self, msg):\n        inc = msg[\"payload\"]\n        incident_id = inc[\"incident_id\"]\n        scenario = inc.get(\"scenario\")\n\n        print(f\"üîç Diagnoser triggered for incident: {incident_id}, scenario={scenario}\")\n\n        # Fetch evidence\n        logs_res    = TOOLS[\"log_fetch\"].run({\"scenario\": scenario, \"job\": None})\n        schema_res  = TOOLS[\"schema_diff\"].run({\"scenario\": scenario, \"job\": inc.get(\"job\")})\n        samples_res = TOOLS[\"sample_data\"].run({\"scenario\": scenario, \"job\": inc.get(\"job\")})\n\n        # Build signature\n        signature = build_signature(inc, schema_res, logs_res, samples_res)\n        signature[\"incident_id\"] = incident_id\n\n        # Optional Gemini log summarization\n        try:\n            if USE_GEMINI and _GEMINI_CONFIGURED:\n                print(\"‚ú® Gemini hybrid mode ON ‚Äî summarizing logs...\")\n                prompt = (\n                    \"Summarize these log lines and highlight likely root causes:\\n\\n\" +\n                    \"\\n\".join(logs_res.get(\"lines\",[])[:200])\n                )\n                llm_res = call_gemini_api(prompt, timeout_s=5)\n                signature[\"llm\"] = {\n                    \"used\": bool(llm_res.get(\"text\") and not llm_res.get(\"error\")),\n                    \"summary\": (llm_res.get(\"text\") or \"\")[:3000],\n                    \"confidence\": llm_res.get(\"confidence\") or 0.0,\n                    \"error\": llm_res.get(\"error\"),\n                    \"prompt_snippet\": prompt[:800],\n                    \"ts\": now_ts()\n                }\n            else:\n                signature[\"llm\"] = {\"used\": False}\n        except Exception as e:\n            signature[\"llm\"] = {\"used\": False, \"error\": str(e)}\n\n        # Store draft session\n        DRAFT_MEMORY.append(dict(signature))\n\n        SESSIONS[incident_id] = {\n            \"signature\": signature,\n            \"logs\": logs_res.get(\"lines\", [])[:50],\n            \"schema_diff\": schema_res.get(\"diff\", []),\n            \"samples\": samples_res.get(\"rows\", [])[:20],\n            \"sample_anomalies\": samples_res.get(\"anomalies\", []),\n            \"history\": None,\n            \"impact\": None,\n            \"report\": None,\n            \"stage\": \"draft\"\n        }\n\n        print(f\"üì¶ Diagnoser built signature & stored session for {incident_id}\")\n\n        emit(\"root_cause_diagnoser\", \"pattern_history_agent\",\n             {\"type\":\"signature\", \"signature\": signature})\n        emit(\"root_cause_diagnoser\", \"impact_scope_agent\",\n             {\"type\":\"failure_point\", \"signature\": signature})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.473664Z","iopub.execute_input":"2025-11-28T16:58:02.474061Z","iopub.status.idle":"2025-11-28T16:58:02.501818Z","shell.execute_reply.started":"2025-11-28T16:58:02.474029Z","shell.execute_reply":"2025-11-28T16:58:02.500703Z"}},"outputs":[{"name":"stdout","text":"üß† Loading: Diagnoser agent...\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## üß¨ History Analyzer & Impact Analyzer\n\n### **History Analyzer (`pattern_history_agent`)**\nSurfaces recurring incidents via `history_query`.\n\n### **Impact Analyzer (`impact_scope_agent`)**\nDetermines downstream blast radius using `lineage_query`.\n\nBoth agents enrich the session and forward results to the Advisor.\n","metadata":{}},{"cell_type":"code","source":"print(\"üß¨ Loading: History & Impact analyzer agents...\")\n\nclass PatternHistoryAgent:\n    def on_message(self, msg):\n        sig = msg[\"payload\"][\"signature\"]\n        print(f\"üìö History agent running for signature: {sig['incident_id']}\")\n\n        res = TOOLS[\"history_query\"].run({\"signature\": sig})\n        emit(\"pattern_history_agent\", \"pulse_advisor\",\n             {\"type\":\"history_matches\", \"matches\": res[\"matches\"], \"signature\": sig})\n\nclass ImpactScopeAgent:\n    def on_message(self, msg):\n        sig = msg[\"payload\"][\"signature\"]\n        print(f\"üåê Impact agent running for job={sig.get('job')}\")\n\n        job = sig.get(\"job\")\n        res = TOOLS[\"lineage_query\"].run({\"job\": job})\n        emit(\"impact_scope_agent\", \"pulse_advisor\",\n             {\"type\":\"impact\", \"impact\": res, \"signature\": sig})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.503182Z","iopub.execute_input":"2025-11-28T16:58:02.503489Z","iopub.status.idle":"2025-11-28T16:58:02.533962Z","shell.execute_reply.started":"2025-11-28T16:58:02.503467Z","shell.execute_reply":"2025-11-28T16:58:02.532729Z"}},"outputs":[{"name":"stdout","text":"üß¨ Loading: History & Impact analyzer agents...\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## üßæ Advisor Agent (`pulse_advisor`)\n\nThe Advisor is the final reasoning layer.  \nIt waits until *both* history + impact results arrive, then:\n\n- computes final severity  \n- computes confidence (with optional Gemini contribution)  \n- merges logs, schema diff, anomalies  \n- generates:\n  - Markdown RCA report  \n  - machine-readable JSON report\n\nFinally, it emits:\n- `ui` ‚Üí to display the draft report  \n- `save_report` ‚Üí to persist it  \n","metadata":{}},{"cell_type":"code","source":"print(\"üß≠ Loading: Advisor agent...\")\n\nclass Advisor:\n    def __init__(self, use_gemini=False):\n        self.use_gemini = use_gemini\n\n    def _extract_downstreams(self, impact):\n        if isinstance(impact, dict):\n            if \"downstreams\" in impact:\n                return impact.get(\"downstreams\")\n            if \"impact\" in impact and isinstance(impact[\"impact\"], dict) and \"downstreams\" in impact[\"impact\"]:\n                return impact[\"impact\"].get(\"downstreams\")\n        return []\n\n    def synthesize_report(self, session):\n        signature = session.get(\"signature\", {})\n        history = session.get(\"history\") or []\n        impact = session.get(\"impact\") or {}\n        sev_score = compute_severity(signature, impact)\n        sev_label = severity_label(sev_score)\n        confidence_score, confidence_breakdown = compute_confidence(signature, history, use_gemini=self.use_gemini)\n\n        # determine llm usage from session.signature if present\n        llm_info = session.get(\"signature\", {}).get(\"llm\", {\"used\": False})\n\n        # Metadata block (ensure seeded/source) -- set mode deterministically: deterministic | gemini | hybrid\n        meta = {\n            \"incident_id\": signature.get(\"incident_id\"),\n            \"job\": signature.get(\"job\"),\n            \"scenario\": signature.get(\"scenario\") or signature.get(\"error_class\") or \"unknown\",\n            \"text_hash\": signature.get(\"text_hash\"),\n            \"mode\": (\"hybrid\" if (self.use_gemini and llm_info.get(\"used\")) else (\"gemini\" if self.use_gemini else \"deterministic\")),\n            \"generated_at\": now_ts()\n        }\n        # ensure seeded/source presence\n        meta[\"seeded\"] = any(e.get(\"text_hash\")==signature.get(\"text_hash\") and e.get(\"seeded\") for e in MEMORY_BANK)\n        meta[\"source\"] = \"demo_seed\" if meta[\"seeded\"] else \"runtime_draft\"\n\n        # Build markdown (report text preserved as before)\n        lines = []\n        lines.append(\"---\")\n        for k,v in meta.items():\n            lines.append(f\"{k}: {v}\")\n        lines.append(\"---\"); lines.append(\"\")\n        lines.append(f\"# RCA Report - {signature.get('incident_id')}\")\n        lines.append(\"\")\n        lines.append(f\"**Root Cause (hypothesis):** {signature.get('error_class')} on job `{signature.get('job')}`\")\n        lines.append(\"\")\n        lines.append(f\"**Severity:** {sev_label} ({sev_score})\")\n        lines.append(f\"**Confidence:** {confidence_breakdown['label']} ({confidence_breakdown['final_score']})\")\n        lines.append(\"\")\n        # Confidence breakdown in MD (top)\n        cb = confidence_breakdown\n        lines.append(\"**Confidence breakdown:**\")\n        lines.append(f\"- final_score: {cb['final_score']} ({cb['label']})\")\n        lines.append(f\"- history_score: {cb['history_score']}, field_score: {cb['field_score']}, anomaly_score: {cb['anomaly_score']}\")\n        lines.append(\"\")\n        # Evidence\n        lines.append(\"## Evidence\")\n        log_lines = session.get(\"logs\", [])[:10]\n        if log_lines:\n            lines.append(\"### Log excerpt (`log_fetch`)\")\n            lines.append(\"```\")\n            for i, ln in enumerate(log_lines, start=1):\n                lines.append(f\"{i:3d}: {ln}\")\n            lines.append(\"```\")\n        schema_diff = session.get(\"schema_diff\", [])\n        if schema_diff:\n            lines.append(\"### Schema diff (`schema_diff`)\")\n            for d in schema_diff:\n                lines.append(f\"- field: `{d.get('field')}` ‚Äî {d.get('old')} ‚Üí {d.get('new')}\")\n        sample_anoms = session.get(\"sample_anomalies\", [])\n        if sample_anoms:\n            lines.append(\"### Sample anomalies (`sample_data`)\")\n            lines.append(\"|row|field|value|\")\n            lines.append(\"|--:|:--|:--|\")\n            for a in sample_anoms:\n                lines.append(f\"|{a.get('row')}|{a.get('field')}|`{a.get('value')}`|\")\n        if (not log_lines) and (not schema_diff) and (not sample_anoms):\n            lines.append(\"- none\")\n        lines.append(\"\")\n\n        # Historical matches\n        lines.append(\"## Historical Matches (`history_query`)\")\n        history = session.get(\"history\") or []\n        if history:\n            for m in history:\n                comp = m.get(\"components\",{})\n                lines.append(f\"- matched past incident {m['past_incident'].get('created_at','n/a')} ‚Äî score {m['score']} (job:{comp.get('job',0)}, fields:{round(comp.get('fields',0),2)}, error_class:{comp.get('error_class',0)})\")\n        else:\n            lines.append(\"- none\")\n        lines.append(\"\")\n\n        # Impact (lineage)\n        lines.append(\"## Impacted Downstream (`lineage_query`)\")\n        dlist = self._extract_downstreams(impact)\n        if dlist:\n            for d in dlist:\n                lines.append(f\"- `{d['asset']}` ({d['type']}) critical={d.get('critical', False)}\")\n        else:\n            lines.append(\"- none\")\n        lines.append(\"\")\n\n        # Recommendations (scenario-driven, generic templates with placeholders)\n        lines.append(\"## Recommended Next Steps\")\n\n        # Default scenario templates (merge-safe)\n        _default_scenario_recs = {\n            \"missing_partition\": {\n                \"title\": \"Missing partition detected\",\n                \"steps\": [\n                    \"Verify storage prefix exists and list objects: `aws s3 ls {prefix}` or `gsutil ls {prefix}` and confirm objects under {partition}.\",\n                    \"If upstream should have produced this partition: re-run the upstream producer job for the {partition} window (job: {job}).\",\n                    \"If backup data exists: run a targeted backfill to restore {prefix}/{partition}.\",\n                    \"If upstream intentionally skipped: mark the partition as 'no-data-expected' and configure downstream pipelines to soft-skip.\"\n                ],\n                \"confidence_hint\": \"High ‚Äî missing partition pattern found in logs.\"\n            },\n            \"schema_drift\": {\n                \"title\": \"Schema change / drift detected\",\n                \"steps\": [\n                    \"Identify the changed field(s) and impacted sinks with a quick schema diff.\",\n                    \"If breaking: coordinate a schema contract update with upstream and deploy a compatible parser or migration.\",\n                    \"Run a selective backfill for affected partitions if data loss or type coercion happened.\"\n                ],\n                \"confidence_hint\": \"Medium ‚Äî schema mismatch detected; confirm with schema registry or sample rows.\"\n            },\n            \"invalid_values\": {\n                \"title\": \"Invalid values / data quality issues\",\n                \"steps\": [\n                    \"Quantify affected rows (e.g., `SELECT COUNT(*) FROM upstream WHERE <predicate>`).\",\n                    \"Add validation rules at ingest or transformation to reject/flag invalid values.\",\n                    \"Create an automated remediation/backfill run for the affected windows.\"\n                ],\n                \"confidence_hint\": \"Medium ‚Äî sample anomalies indicate bad values.\"\n            },\n            \"unknown\": {\n                \"title\": \"Unknown / requires further investigation\",\n                \"steps\": [\n                    \"Collect more contextual logs and sample rows for the incident window.\",\n                    \"Run the diagnosis again with enriched context (longer log excerpt, schema diff, sample data).\",\n                    \"If possible, consult upstream job logs or owners for additional clues.\"\n                ],\n                \"confidence_hint\": \"Low ‚Äî insufficient evidence to recommend automated remediation.\"\n            }\n        }\n\n        # Merge into any global mapping without overwriting existing keys\n        _SCENARIO_RECOMMENDATIONS = globals().get(\"SCENARIO_RECOMMENDATIONS\", {})\n        for k, v in _default_scenario_recs.items():\n            if k not in _SCENARIO_RECOMMENDATIONS:\n                _SCENARIO_RECOMMENDATIONS[k] = v\n        globals()[\"SCENARIO_RECOMMENDATIONS\"] = _SCENARIO_RECOMMENDATIONS\n\n        # Build recs list using template if available, else fall back to legacy logic\n        recs = []\n\n        # -----------------------------\n        # Robust scenario detection:\n        # try error_class, then signature.scenario, then session-level scenario,\n        # then look for keywords in error_snippet or first log lines.\n        # -----------------------------\n        sig_scenario = (\n            signature.get(\"error_class\")\n            or signature.get(\"scenario\")\n            or session.get(\"scenario\")\n            or signature.get(\"detected_scenario\")\n            or \"unknown\"\n        )\n        \n        # --- Force 'schema_drift' if strong evidence exists ---\n        log_text = \"\\n\".join(session.get(\"logs\", [])).lower()\n        if signature.get(\"changed_fields\") or schema_diff or \"cannot cast\" in log_text or \"mismatch\" in log_text:\n            sig_scenario = \"schema_drift\"\n            signature[\"scenario\"] = sig_scenario \n        # --- END ---\n\n\n        # fallback heuristic: inspect error_snippet or first log line for missing-partition cues\n        if sig_scenario == \"unknown\":\n            snippet = (signature.get(\"error_snippet\") or \"\")\n            if not snippet and log_lines:\n                snippet = log_lines[0]\n            s_low = snippet.lower() if isinstance(snippet, str) else \"\"\n            if (\"no files\" in s_low or \"no objects\" in s_low or \"dt=\" in s_low or \"partition\" in s_low):\n                sig_scenario = \"missing_partition\"\n\n        # Context for formatting placeholders\n        context = {\n            \"prefix\": signature.get(\"prefix\", \"<prefix>\"),\n            \"partition\": signature.get(\"partition\", \"<partition>\"),\n            \"job\": signature.get(\"job\", \"<job>\"),\n            \"owner\": signature.get(\"owner\", \"<owner>\"),\n            \"error_snippet\": signature.get(\"error_snippet\", signature.get(\"error_class\", \"<error>\"))\n        }\n        # --- Normalize freeform error_class/scenario -> canonical scenario keys (minimal, in-place) ---\n        sig_low = str(sig_scenario).lower() if sig_scenario is not None else \"\"\n        if \"no files\" in sig_low or \"no objects\" in sig_low or \"missing partition\" in sig_low or \"dt=\" in sig_low:\n            sig_scenario = \"missing_partition\"\n        elif \"schema\" in sig_low or \"mismatch\" in sig_low or \"cannot cast\" in sig_low or \"cannot convert\" in sig_low:\n            sig_scenario = \"schema_drift\"\n        elif \"jsondecodeerror\" in sig_low or \"malformed\" in sig_low or \"parse error\" in sig_low:\n            sig_scenario = \"invalid_values\"\n        # persist normalized scenario back to signature so downstream code sees canonical key\n        if sig_scenario in [\"missing_partition\", \"schema_drift\", \"invalid_values\"]:\n             signature[\"scenario\"] = sig_scenario\n\n        template = _SCENARIO_RECOMMENDATIONS.get(sig_scenario)\n\n        if template:\n            # Add a header-like first line (keeps readability consistent)\n            recs.append(template.get(\"title\", \"Recommended actions\"))\n            for step in template.get(\"steps\", []):\n                try:\n                    recs.append(step.format(**context))\n                except Exception:\n                    recs.append(step)  # fallback raw\n            if template.get(\"confidence_hint\"):\n                recs.append(f\"Confidence: {template.get('confidence_hint')}\")\n        else:\n            # Preserve existing special-case logic if no template mapped\n            if signature.get(\"changed_fields\"):\n                recs.append(\"1) Run a selective query to quantify affected rows, e.g.: `SELECT COUNT(*) FROM upstream_table WHERE TRY_CAST(price AS INT) IS NULL;`\")\n                recs.append(\"2) Add safe CAST/COALESCE in transformation or coordinate upstream backfill.\")\n                recs.append(\"3) If critical downstreams exist, run a prioritized backfill for affected partitions.\")\n            for a in signature.get(\"sample_anomalies\", []):\n                v = a.get(\"value\")\n                if isinstance(v, (int, float)) and v < 0:\n                    recs.append(\"Add validation rule at ingest: reject or flag negative prices; consider alerting on new validation failures.\")\n            # Missing partition scenario override\n            if signature.get(\"error_class\") == \"missing_partition\" or signature.get(\"scenario\") == \"missing_partition\":\n                recs = [f\"Verify that the expected partition exists in storage: run `aws s3 ls {signature.get('prefix','<prefix>')}` and confirm objects under {signature.get('partition','<partition>')}`.\",\n                        f\"If upstream should have produced this partition: re-run the upstream job for `{signature.get('partition','<partition>')}` (job: {signature.get('job')}).\",\n                        f\"If backup data exists: run a targeted backfill to restore `{signature.get('partition','<partition>')}`.\",\"If upstream intentionally skipped this partition, mark it as 'no-data-expected' and configure downstream pipelines to soft-skip instead of erroring.\"]\n            elif not recs:\n                recs.append(\"Inspect logs, collect additional sample rows, and rerun diagnosis with more context.\")\n\n        for r in recs: lines.append(f\"- {r}\")\n        lines.append(\"\")\n        lines.append(f\"\\nGenerated at: {now_ts()}\")\n        md = \"\\n\".join(lines)\n\n        # Build machine-readable JSON (ensure impact.downstreams always present)\n        dlist_json = dlist or []\n        rep_json = {\n            \"meta\": meta,\n            \"root_cause\": {\"hypothesis\": signature.get(\"error_class\"), \"job\": signature.get(\"job\")},\n            \"severity\": {\"label\": sev_label, \"score\": sev_score},\n            \"confidence_breakdown\": confidence_breakdown,\n            \"evidence\": {\n                \"logs\": {\"tool\":\"log_fetch\",\"lines\": log_lines},\n                \"schema_diff\": {\"tool\":\"schema_diff\",\"diff\": schema_diff},\n                \"sample_anomalies\": {\"tool\":\"sample_data\",\"anomalies\": sample_anoms}\n            },\n            \"history_matches\": history,\n            \"impact\": {\"downstreams\": dlist_json},\n            \"recommended_next_steps\": recs,\n            \"generated_at\": now_ts(),\n            # add non-intrusive LLM provenance info (machine-readable only)\n            \"llm\": llm_info if isinstance(llm_info, dict) else {\"used\": False}\n        }\n        return md, rep_json\n\n    def on_message(self, msg):\n        p = msg[\"payload\"]\n        sig = p.get(\"signature\") or {}\n        inc_id = sig.get(\"incident_id\") or make_id(\"inc\")\n        session = SESSIONS.setdefault(inc_id, {\"signature\":sig,\"history\":None,\"impact\":None,\"report\":None,\"stage\":\"draft\"})\n        if p.get(\"type\") == \"history_matches\":\n            session[\"history\"] = p.get(\"matches\")\n        if p.get(\"type\") == \"impact\":\n            session[\"impact\"] = p.get(\"impact\")\n\n        if session.get(\"history\") is not None and session.get(\"impact\") is not None and session.get(\"report\") is None:\n            md, rep_json = self.synthesize_report(session)\n            session[\"report\"] = {\"md\": md, \"json\": rep_json}\n            session[\"stage\"] = \"awaiting_approval\"\n            SESSIONS[inc_id] = session\n            emit(\"pulse_advisor\", \"ui\", {\"type\":\"report_ready\", \"incident_id\": inc_id, \"report\": md})\n            emit(\"pulse_advisor\", \"save_report\", {\"type\":\"save_pending\", \"incident_id\": inc_id})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.535539Z","iopub.execute_input":"2025-11-28T16:58:02.535888Z","iopub.status.idle":"2025-11-28T16:58:02.579185Z","shell.execute_reply.started":"2025-11-28T16:58:02.535858Z","shell.execute_reply":"2025-11-28T16:58:02.577836Z"}},"outputs":[{"name":"stdout","text":"üß≠ Loading: Advisor agent...\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## ‚öôÔ∏è Agent Instantiation & Router Wiring\n\nHere we create all agents:\n- Diagnoser  \n- History Analyzer  \n- Impact Analyzer  \n- Advisor (hybrid mode if Gemini available)\n\nThen they are ready to receive events through the router system.\n","metadata":{}},{"cell_type":"code","source":"print(\"‚öôÔ∏è Instantiating agents...\")\n\n# instantiate agents\npulse_detector = PulseDetector()    \ndiagnoser = Diagnoser()\nhistory_agent = PatternHistoryAgent()\nimpact_agent = ImpactScopeAgent()\nadvisor = Advisor(use_gemini=USE_GEMINI)\n\nprint(\"ü§ñ Agents ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.584056Z","iopub.execute_input":"2025-11-28T16:58:02.584505Z","iopub.status.idle":"2025-11-28T16:58:02.607472Z","shell.execute_reply.started":"2025-11-28T16:58:02.584469Z","shell.execute_reply":"2025-11-28T16:58:02.606364Z"}},"outputs":[{"name":"stdout","text":"‚öôÔ∏è Instantiating agents...\nü§ñ Agents ready.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## üìÑ Robust File Parsing Helper\n\nThis helper turns uploaded files (log/txt/json/csv/ndjson)\ninto clean line lists for the RCA pipeline.\n\nSupports:\n- JSON object\n- JSONL/NDJSON\n- CSV with \"message\" column\n- Plain text log files\n\nUsed by:\n- offline runner  \n- UI handling module  \n","metadata":{}},{"cell_type":"code","source":"print(\"üìÑ Loading: Robust file parsing helper...\")\n\ndef parse_uploaded_file_bytes(b: bytes):\n    s = b.decode(\"utf-8\", errors=\"replace\")\n\n    # Try to parse as a full JSON document\n    try:\n        obj = json.loads(s)\n        if isinstance(obj, dict):\n            if \"message\" in obj:\n                print(\"üìÑ Parsed JSON dict with message.\")\n                return [obj[\"message\"]]\n            return [json.dumps(obj)]\n        if isinstance(obj, list):\n            print(\"üìÑ Parsed JSON list.\")\n            return [\n                json.dumps(i) if not isinstance(i,str) else i\n                for i in obj\n            ]\n    except:\n        pass\n\n    # Try NDJSON / JSONL\n    lines = s.splitlines()\n    nd=[]; nd_ok=True\n    for ln in lines:\n        ln_strip = ln.strip()\n        if not ln_strip:\n            continue\n        try:\n            j = json.loads(ln_strip)\n            if isinstance(j, dict) and \"message\" in j:\n                nd.append(j[\"message\"])\n            else:\n                nd.append(json.dumps(j))\n        except:\n            nd_ok=False\n            break\n\n    if nd_ok and nd:\n        print(\"üìÑ Parsed NDJSON / JSONL format.\")\n        return nd\n\n    # CSV format\n    try:\n        import io, csv\n        reader = csv.DictReader(io.StringIO(s))\n        if reader.fieldnames:\n            print(\"üìÑ Parsed CSV file.\")\n            msgs=[]\n            for row in reader:\n                if \"message\" in row and row[\"message\"]:\n                    msgs.append(row[\"message\"])\n                else:\n                    msgs.append(\", \".join(f\"{k}={v}\" for k,v in row.items()))\n            return msgs\n    except:\n        pass\n\n    # fallback: plain text log lines\n    print(\"üìÑ Parsed plain text log file.\")\n    return [ln for ln in s.splitlines() if ln.strip()]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.608736Z","iopub.execute_input":"2025-11-28T16:58:02.609271Z","iopub.status.idle":"2025-11-28T16:58:02.633119Z","shell.execute_reply.started":"2025-11-28T16:58:02.609239Z","shell.execute_reply":"2025-11-28T16:58:02.632020Z"}},"outputs":[{"name":"stdout","text":"üìÑ Loading: Robust file parsing helper...\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## üõ°Ô∏è Validator helper\n\nThis cell defines `validate_report()` which sanity-checks the generated RCA JSON.  \nIt prints a short summary when called to make debugging easier.\n","metadata":{}},{"cell_type":"code","source":"print(\"üîß Loading: Validator helper...\")\n\ndef validate_report(rep_json):\n    errors = []\n    if not rep_json:\n        return {\"ok\": False, \"errors\": [\"report is None\"]}\n\n    meta = rep_json.get(\"meta\")\n    if not meta:\n        errors.append(\"missing meta\")\n    else:\n        for k in (\"incident_id\", \"job\", \"text_hash\", \"generated_at\"):\n            if k not in meta:\n                errors.append(f\"meta.{k} missing\")\n\n    if \"root_cause\" not in rep_json:\n        errors.append(\"missing root_cause\")\n\n    # severity should be present and include a score key\n    sev = rep_json.get(\"severity\")\n    if not sev or \"score\" not in sev:\n        errors.append(\"missing severity.score\")\n\n    cb = rep_json.get(\"confidence_breakdown\")\n    if not cb or \"final_score\" not in cb:\n        errors.append(\"confidence breakdown missing final_score\")\n\n    ev = rep_json.get(\"evidence\") or {}\n    if \"logs\" not in ev:\n        errors.append(\"evidence.logs missing\")\n    if \"schema_diff\" not in ev:\n        errors.append(\"evidence.schema_diff missing\")\n    # proper presence check for sample_anomalies (allow empty list)\n    if \"sample_anomalies\" not in ev:\n        errors.append(\"evidence.sample_anomalies missing\")\n\n    hist = rep_json.get(\"history_matches\", [])\n    if not isinstance(hist, list):\n        errors.append(\"history_matches not a list\")\n\n    impact = rep_json.get(\"impact\") or {}\n    if not (\"downstreams\" in impact):\n        errors.append(\"impact.downstreams missing\")\n\n    if \"recommended_next_steps\" not in rep_json:\n        errors.append(\"no recommended_next_steps\")\n\n    result = {\n        \"ok\": len(errors) == 0,\n        \"errors\": errors,\n        \"summary\": {\n            \"meta\": meta,\n            \"severity\": rep_json.get(\"severity\"),\n            \"confidence\": cb.get(\"final_score\") if cb else None,\n            \"history_count\": len(hist)\n        }\n    }\n\n    print(f\"üîç validate_report -> ok={result['ok']}, errors_count={len(errors)}\")\n    if errors:\n        for e in errors:\n            print(f\"  - {e}\")\n    return result\n\nprint(\"üîß Validator helper loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.634350Z","iopub.execute_input":"2025-11-28T16:58:02.634694Z","iopub.status.idle":"2025-11-28T16:58:02.666345Z","shell.execute_reply.started":"2025-11-28T16:58:02.634664Z","shell.execute_reply":"2025-11-28T16:58:02.665177Z"}},"outputs":[{"name":"stdout","text":"üîß Loading: Validator helper...\nüîß Validator helper loaded.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## ‚ñ∂Ô∏è Offline runner: `run_offline_samples`\n\nThis cell contains the main offline runner that:\n- parses input samples\n- emits detection events (now routed through `pulse_detector`)\n- waits for the agent chain to complete\n- optionally saves Markdown + JSON reports to `submission_reports/`\n\nExtra prints are included to trace progress.\n","metadata":{}},{"cell_type":"code","source":"print(\"‚ñ∂Ô∏è Loading: Offline runner (run_offline_samples)...\")\n\ndef run_offline_samples(samples=None, show=True, save_reports=True, out_dir=\"submission_reports\"):\n    if samples is None:\n        samples = {k:(\"\\n\".join(v)).encode(\"utf-8\") for k,v in SYNTHETIC_LOGS.items()}\n    results=[]\n    if save_reports: os.makedirs(out_dir, exist_ok=True)\n\n    for name, b in samples.items():\n        try:\n            print(f\"\\n--- Processing sample: {name} ---\")\n            IN_MEMORY_BUS.clear(); TRACE_STORE.clear(); SESSIONS.clear(); DRAFT_MEMORY.clear()\n            lines = parse_uploaded_file_bytes(b)\n\n            # canonical logs first\n            logs_res = TOOLS[\"log_fetch\"].run({\"scenario\": name, \"job\": None})\n            log_lines = logs_res.get(\"lines\", []) or lines\n\n            # prefer log_fetch.job, else most frequent job= token, else scenario\n            parsed_job = logs_res.get(\"job\")\n            if not parsed_job:\n                jobs = re.findall(r\"\\bjob=([A-Za-z0-9_\\-\\.]+)\", \"\\n\".join(log_lines))\n                if jobs:\n                    parsed_job = Counter(jobs).most_common(1)[0][0]\n            job = parsed_job or name\n\n            print(f\"Detected job='{job}' for sample '{name}' (parsed_job={parsed_job})\")\n\n            # call tools using canonical job\n            schema_res = TOOLS[\"schema_diff\"].run({\"scenario\": name, \"job\": job})\n            samples_res = TOOLS[\"sample_data\"].run({\"scenario\": name, \"job\": job})\n\n            provisional_incident_id = make_id(\"inc\")\n            provisional_inc = {\n                \"incident_id\": provisional_incident_id,\n                \"scenario\": name,\n                \"job\": job,\n                \"error_snippet\": lines[0] if lines else \"\"\n            }\n            signature = build_signature(provisional_inc, schema_res, logs_res, samples_res)\n            signature[\"incident_id\"] = provisional_incident_id\n\n            kind, existing_id, entry = find_existing_by_text_hash(signature[\"text_hash\"])\n            if kind == \"session\":\n                existing_sess = entry\n                report_obj = existing_sess.get(\"report\")\n                incident_id = existing_id\n                md = report_obj.get(\"md\") if report_obj else None\n                rep_json = report_obj.get(\"json\") if report_obj else None\n                print(f\"[idempotency] Reusing existing session report for sample '{name}' -> incident {incident_id}\")\n            elif kind == \"memory\":\n                incident_id = existing_id or make_id(\"inc\")\n                md_lines = [f\"# RCA Report - {incident_id}\", \"\", f\"*Reused historical incident matching text_hash {signature['text_hash']}*\"]\n                md = \"\\n\".join(md_lines)\n                rep_json = {\"meta\":{\"incident_id\": incident_id, \"text_hash\": signature[\"text_hash\"]}, \"note\":\"reused_from_memory\"}\n                print(f\"[idempotency] Reusing confirmed memory for sample '{name}' -> incident {incident_id}\")\n            else:\n                payload = {\n                    \"incident_id\": provisional_incident_id,\n                    \"scenario\": name,\n                    \"job\": job,\n                    \"type\": \"detection\",\n                    \"pattern\": \"detected_offline\",\n                    \"category\": \"demo\",\n                    \"error_snippet\": lines[0] if lines else \"\",\n                    \"source_lines_count\": len(lines),\n                    \"text_hash\": signature[\"text_hash\"],\n                    # Provide raw lines so PulseDetector can operate\n                    \"lines\": log_lines\n                }\n                print(f\"[emit] offline_runner -> pulse_detector | incident={provisional_incident_id}\")\n                emit(\"offline_runner\", \"pulse_detector\", payload)\n                router_run_blocking()\n                sess = SESSIONS.get(provisional_incident_id)\n                if not sess:\n                    print(f\"[error] No session created for {provisional_incident_id}\")\n                    md = None; rep_json = None; incident_id = provisional_incident_id\n                else:\n                    # ensure downstream agents run to completion\n                    router_run_blocking()\n                    report_obj = sess.get(\"report\")\n                    md = report_obj.get(\"md\") if report_obj else None\n                    rep_json = report_obj.get(\"json\") if report_obj else None\n                    incident_id = provisional_incident_id\n\n            print(f\"\\n=== Draft report for sample '{name}' (incident: {incident_id}) ===\\n\")\n            if md and show:\n                display(Markdown(md))\n            else:\n                print(\"No report produced. Check TRACE_STORE and SESSIONS.\")\n\n            saved_info = None\n            if save_reports and md and rep_json:\n                out = TOOLS[\"save_report\"].run({\"report_md\": md, \"report_json\": rep_json, \"incident_id\": incident_id, \"out_dir\": out_dir})\n                saved_info = out\n                print(f\"Saved artifacts: MD -> {out['md_path']}, JSON -> {out['json_path']}\")\n\n            results.append((incident_id, md, rep_json, saved_info))\n        except Exception as e:\n            print(\"Error processing sample\", name, e)\n            traceback.print_exc()\n            results.append((None, None, None, None))\n    return results\n\nprint(\"‚ñ∂Ô∏è Offline runner loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.667395Z","iopub.execute_input":"2025-11-28T16:58:02.667751Z","iopub.status.idle":"2025-11-28T16:58:02.691537Z","shell.execute_reply.started":"2025-11-28T16:58:02.667722Z","shell.execute_reply":"2025-11-28T16:58:02.690442Z"}},"outputs":[{"name":"stdout","text":"‚ñ∂Ô∏è Loading: Offline runner (run_offline_samples)...\n‚ñ∂Ô∏è Offline runner loaded.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## ‚ñ∂ Run demo & quick validation\n\nSeed demo memory (idempotent), run the offline demo, and validate saved reports.  \nThis cell prints concise validation results.\n","metadata":{}},{"cell_type":"code","source":"print(\"üöÄ Running demo: seed memory and run offline samples...\")\n\nseed_demo_memory_once()\nprint(\"‚úÖ Demo memory seeded.\")\n\ndemo_results = run_offline_samples(save_reports=True)\nprint(\"\\n‚úÖ Demo finished. 'demo_results' contains tuples (incident_id, md, json, saved_info).\")\n\n# Quick validation output:\nfor inc_id, md, rep_json, saved in demo_results:\n    print(f\"\\nüîé Validation for {inc_id}: {validate_report(rep_json)}\")\n\nprint(\"üèÅ Demo & validation complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.692868Z","iopub.execute_input":"2025-11-28T16:58:02.693231Z","iopub.status.idle":"2025-11-28T16:58:02.736579Z","shell.execute_reply.started":"2025-11-28T16:58:02.693200Z","shell.execute_reply":"2025-11-28T16:58:02.735513Z"}},"outputs":[{"name":"stdout","text":"üöÄ Running demo: seed memory and run offline samples...\n  ‚Ü™ MEMORY_BANK already seeded.\n‚úÖ Demo memory seeded.\n\n--- Processing sample: schema_drift ---\nüìÑ Parsed CSV file.\nDetected job='orders' for sample 'schema_drift' (parsed_job=orders)\nüìå Signature built for job='orders', error_class='TypeError', changed_fields=['price']\nüîç Checking for existing incidents with text_hash=95ac10f46a...\n‚ùå No matching signature found.\n[emit] offline_runner -> pulse_detector | incident=inc-2f7a9d23\n[emit] offline_runner ‚Üí pulse_detector | type=detection\n‚ñ∂Ô∏è Router: starting event loop...\n[router] Dispatching to agent: pulse_detector\n[pulse_detector] Scenario=schema_drift, job=orders\n[emit] pulse_detector ‚Üí root_cause_diagnoser | type=detection\n[router] Dispatching to agent: root_cause_diagnoser\nüîç Diagnoser triggered for incident: inc-2f7a9d23, scenario=schema_drift\nüìå Signature built for job='orders', error_class='TypeError', changed_fields=['price']\nüì¶ Diagnoser built signature & stored session for inc-2f7a9d23\n[emit] root_cause_diagnoser ‚Üí pattern_history_agent | type=signature\n[emit] root_cause_diagnoser ‚Üí impact_scope_agent | type=failure_point\n[router] Dispatching to agent: pattern_history_agent\nüìö History agent running for signature: inc-2f7a9d23\n[emit] pattern_history_agent ‚Üí pulse_advisor | type=history_matches\n[router] Dispatching to agent: impact_scope_agent\nüåê Impact agent running for job=orders\n[emit] impact_scope_agent ‚Üí pulse_advisor | type=impact\n[router] Dispatching to agent: pulse_advisor\n[router] Dispatching to agent: pulse_advisor\nüö® Severity computed: 1.0\nüìä Confidence score computed: 0.83 (HIGH)\n[emit] pulse_advisor ‚Üí ui | type=report_ready\n[emit] pulse_advisor ‚Üí save_report | type=save_pending\n[router] Dispatching to agent: ui\n[router] Dispatching to agent: save_report\n‚èπ Router: event queue empty.\n‚ñ∂Ô∏è Router: starting event loop...\n‚èπ Router: event queue empty.\n\n=== Draft report for sample 'schema_drift' (incident: inc-2f7a9d23) ===\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"---\nincident_id: inc-2f7a9d23\njob: orders\nscenario: TypeError\ntext_hash: 78eb9cde7a72074a5aa3dac445160041e34c72d864d5a1c54f9b7a3b2af21d1a\nmode: gemini\ngenerated_at: 2025-11-28 16:58:02\nseeded: False\nsource: runtime_draft\n---\n\n# RCA Report - inc-2f7a9d23\n\n**Root Cause (hypothesis):** TypeError on job `orders`\n\n**Severity:** HIGH (1.0)\n**Confidence:** HIGH (0.83)\n\n**Confidence breakdown:**\n- final_score: 0.83 (HIGH)\n- history_score: 1.0, field_score: 1.0, anomaly_score: 0.33\n\n## Evidence\n### Log excerpt (`log_fetch`)\n```\n  1: 2025-11-24 10:00:01 INFO job=orders ETL step=ingest files=3\n  2: 2025-11-24 10:03:02 ERROR job=orders transform TypeError: cannot cast '123.45' to INT on column price\n```\n### Schema diff (`schema_diff`)\n- field: `price` ‚Äî INT ‚Üí FLOAT\n### Sample anomalies (`sample_data`)\n|row|field|value|\n|--:|:--|:--|\n|1|price|`123.45`|\n\n## Historical Matches (`history_query`)\n- matched past incident 2025-11-01 09:00:00 ‚Äî score 1.0 (job:0, fields:0, error_class:0)\n\n## Impacted Downstream (`lineage_query`)\n- `dashboard.sales_over_time` (dashboard) critical=True\n- `ml.revenue_forecast` (model) critical=True\n\n## Recommended Next Steps\n- Schema change / drift detected\n- Identify the changed field(s) and impacted sinks with a quick schema diff.\n- If breaking: coordinate a schema contract update with upstream and deploy a compatible parser or migration.\n- Run a selective backfill for affected partitions if data loss or type coercion happened.\n- Confidence: Medium ‚Äî schema mismatch detected; confirm with schema registry or sample rows.\n\n\nGenerated at: 2025-11-28 16:58:02"},"metadata":{}},{"name":"stdout","text":"Saved artifacts: MD -> submission_reports/pulsetrace_report_inc-2f7a9d23.md, JSON -> submission_reports/pulsetrace_report_inc-2f7a9d23.json\n\n--- Processing sample: missing_partition ---\nüìÑ Parsed CSV file.\nDetected job='reports' for sample 'missing_partition' (parsed_job=reports)\nüìå Signature built for job='reports', error_class='No files found', changed_fields=[]\nüîç Checking for existing incidents with text_hash=01cb6ba658...\n‚ùå No matching signature found.\n[emit] offline_runner -> pulse_detector | incident=inc-57531b2a\n[emit] offline_runner ‚Üí pulse_detector | type=detection\n‚ñ∂Ô∏è Router: starting event loop...\n[router] Dispatching to agent: pulse_detector\n[pulse_detector] Scenario=missing_partition, job=reports\n[emit] pulse_detector ‚Üí root_cause_diagnoser | type=detection\n[router] Dispatching to agent: root_cause_diagnoser\nüîç Diagnoser triggered for incident: inc-57531b2a, scenario=missing_partition\nüìå Signature built for job='reports', error_class='No files found', changed_fields=[]\nüì¶ Diagnoser built signature & stored session for inc-57531b2a\n[emit] root_cause_diagnoser ‚Üí pattern_history_agent | type=signature\n[emit] root_cause_diagnoser ‚Üí impact_scope_agent | type=failure_point\n[router] Dispatching to agent: pattern_history_agent\nüìö History agent running for signature: inc-57531b2a\n[emit] pattern_history_agent ‚Üí pulse_advisor | type=history_matches\n[router] Dispatching to agent: impact_scope_agent\nüåê Impact agent running for job=reports\n[emit] impact_scope_agent ‚Üí pulse_advisor | type=impact\n[router] Dispatching to agent: pulse_advisor\n[router] Dispatching to agent: pulse_advisor\nüö® Severity computed: 0.2\nüìä Confidence score computed: 0.1 (LOW)\n[emit] pulse_advisor ‚Üí ui | type=report_ready\n[emit] pulse_advisor ‚Üí save_report | type=save_pending\n[router] Dispatching to agent: ui\n[router] Dispatching to agent: save_report\n‚èπ Router: event queue empty.\n‚ñ∂Ô∏è Router: starting event loop...\n‚èπ Router: event queue empty.\n\n=== Draft report for sample 'missing_partition' (incident: inc-57531b2a) ===\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"---\nincident_id: inc-57531b2a\njob: reports\nscenario: No files found\ntext_hash: 5ee4b5a44c3e75c1d2494d5c8dca490c2d9be52bd03a5251356879b79129dffe\nmode: gemini\ngenerated_at: 2025-11-28 16:58:02\nseeded: False\nsource: runtime_draft\n---\n\n# RCA Report - inc-57531b2a\n\n**Root Cause (hypothesis):** No files found on job `reports`\n\n**Severity:** LOW (0.2)\n**Confidence:** LOW (0.1)\n\n**Confidence breakdown:**\n- final_score: 0.1 (LOW)\n- history_score: 0.0, field_score: 0.0, anomaly_score: 0.0\n\n## Evidence\n### Log excerpt (`log_fetch`)\n```\n  1: 2025-11-24 11:00:05 ERROR job=reports No files found for partition dt=2025-11-24\n```\n\n## Historical Matches (`history_query`)\n- none\n\n## Impacted Downstream (`lineage_query`)\n- `dashboard.reports` (dashboard) critical=False\n\n## Recommended Next Steps\n- Missing partition detected\n- Verify storage prefix exists and list objects: `aws s3 ls <prefix>` or `gsutil ls <prefix>` and confirm objects under <partition>.\n- If upstream should have produced this partition: re-run the upstream producer job for the <partition> window (job: reports).\n- If backup data exists: run a targeted backfill to restore <prefix>/<partition>.\n- If upstream intentionally skipped: mark the partition as 'no-data-expected' and configure downstream pipelines to soft-skip.\n- Confidence: High ‚Äî missing partition pattern found in logs.\n\n\nGenerated at: 2025-11-28 16:58:02"},"metadata":{}},{"name":"stdout","text":"Saved artifacts: MD -> submission_reports/pulsetrace_report_inc-57531b2a.md, JSON -> submission_reports/pulsetrace_report_inc-57531b2a.json\n\n--- Processing sample: invalid_values ---\nüìÑ Parsed CSV file.\nDetected job='pricing' for sample 'invalid_values' (parsed_job=pricing)\nüìå Signature built for job='pricing', error_class='ValueError', changed_fields=[]\nüîç Checking for existing incidents with text_hash=f9e58980ea...\n‚ùå No matching signature found.\n[emit] offline_runner -> pulse_detector | incident=inc-c043fed1\n[emit] offline_runner ‚Üí pulse_detector | type=detection\n‚ñ∂Ô∏è Router: starting event loop...\n[router] Dispatching to agent: pulse_detector\n[pulse_detector] Scenario=invalid_values, job=pricing\n[emit] pulse_detector ‚Üí root_cause_diagnoser | type=detection\n[router] Dispatching to agent: root_cause_diagnoser\nüîç Diagnoser triggered for incident: inc-c043fed1, scenario=invalid_values\nüìå Signature built for job='pricing', error_class='ValueError', changed_fields=[]\nüì¶ Diagnoser built signature & stored session for inc-c043fed1\n[emit] root_cause_diagnoser ‚Üí pattern_history_agent | type=signature\n[emit] root_cause_diagnoser ‚Üí impact_scope_agent | type=failure_point\n[router] Dispatching to agent: pattern_history_agent\nüìö History agent running for signature: inc-c043fed1\n[emit] pattern_history_agent ‚Üí pulse_advisor | type=history_matches\n[router] Dispatching to agent: impact_scope_agent\nüåê Impact agent running for job=pricing\n[emit] impact_scope_agent ‚Üí pulse_advisor | type=impact\n[router] Dispatching to agent: pulse_advisor\n[router] Dispatching to agent: pulse_advisor\nüö® Severity computed: 0.8\nüìä Confidence score computed: 0.42 (LOW)\n[emit] pulse_advisor ‚Üí ui | type=report_ready\n[emit] pulse_advisor ‚Üí save_report | type=save_pending\n[router] Dispatching to agent: ui\n[router] Dispatching to agent: save_report\n‚èπ Router: event queue empty.\n‚ñ∂Ô∏è Router: starting event loop...\n‚èπ Router: event queue empty.\n\n=== Draft report for sample 'invalid_values' (incident: inc-c043fed1) ===\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"---\nincident_id: inc-c043fed1\njob: pricing\nscenario: ValueError\ntext_hash: bd9555b803cb819c6072036eb090d993df1c0523a5fc9438e0760870bbda0d3b\nmode: gemini\ngenerated_at: 2025-11-28 16:58:02\nseeded: False\nsource: runtime_draft\n---\n\n# RCA Report - inc-c043fed1\n\n**Root Cause (hypothesis):** ValueError on job `pricing`\n\n**Severity:** HIGH (0.8)\n**Confidence:** LOW (0.42)\n\n**Confidence breakdown:**\n- final_score: 0.42 (LOW)\n- history_score: 0.6, field_score: 0.0, anomaly_score: 0.33\n\n## Evidence\n### Log excerpt (`log_fetch`)\n```\n  1: 2025-11-24 12:05:02 ERROR job=pricing transform ValueError: negative value found in price at row 524\n```\n### Sample anomalies (`sample_data`)\n|row|field|value|\n|--:|:--|:--|\n|524|price|`-12.5`|\n\n## Historical Matches (`history_query`)\n- matched past incident 2025-10-20 14:30:00 ‚Äî score 0.6 (job:0, fields:0, error_class:0)\n\n## Impacted Downstream (`lineage_query`)\n- `dashboard.sales_over_time` (dashboard) critical=True\n- `ml.revenue_forecast` (model) critical=True\n\n## Recommended Next Steps\n- Add validation rule at ingest: reject or flag negative prices; consider alerting on new validation failures.\n\n\nGenerated at: 2025-11-28 16:58:02"},"metadata":{}},{"name":"stdout","text":"Saved artifacts: MD -> submission_reports/pulsetrace_report_inc-c043fed1.md, JSON -> submission_reports/pulsetrace_report_inc-c043fed1.json\n\n‚úÖ Demo finished. 'demo_results' contains tuples (incident_id, md, json, saved_info).\nüîç validate_report -> ok=True, errors_count=0\n\nüîé Validation for inc-2f7a9d23: {'ok': True, 'errors': [], 'summary': {'meta': {'incident_id': 'inc-2f7a9d23', 'job': 'orders', 'scenario': 'TypeError', 'text_hash': '78eb9cde7a72074a5aa3dac445160041e34c72d864d5a1c54f9b7a3b2af21d1a', 'mode': 'gemini', 'generated_at': '2025-11-28 16:58:02', 'seeded': False, 'source': 'runtime_draft'}, 'severity': {'label': 'HIGH', 'score': 1.0}, 'confidence': 0.83, 'history_count': 1}}\nüîç validate_report -> ok=True, errors_count=0\n\nüîé Validation for inc-57531b2a: {'ok': True, 'errors': [], 'summary': {'meta': {'incident_id': 'inc-57531b2a', 'job': 'reports', 'scenario': 'No files found', 'text_hash': '5ee4b5a44c3e75c1d2494d5c8dca490c2d9be52bd03a5251356879b79129dffe', 'mode': 'gemini', 'generated_at': '2025-11-28 16:58:02', 'seeded': False, 'source': 'runtime_draft'}, 'severity': {'label': 'LOW', 'score': 0.2}, 'confidence': 0.1, 'history_count': 0}}\nüîç validate_report -> ok=True, errors_count=0\n\nüîé Validation for inc-c043fed1: {'ok': True, 'errors': [], 'summary': {'meta': {'incident_id': 'inc-c043fed1', 'job': 'pricing', 'scenario': 'ValueError', 'text_hash': 'bd9555b803cb819c6072036eb090d993df1c0523a5fc9438e0760870bbda0d3b', 'mode': 'gemini', 'generated_at': '2025-11-28 16:58:02', 'seeded': False, 'source': 'runtime_draft'}, 'severity': {'label': 'HIGH', 'score': 0.8}, 'confidence': 0.42, 'history_count': 1}}\nüèÅ Demo & validation complete.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### üõ∞Ô∏è Observability: Traces, Sessions & Memory\n\nPulseTrace exposes lightweight observability tools that help you inspect how the system behaves during an RCA run:\n\n- **`render_trace_store()`** ‚Äî displays recent agent-to-agent (A2A) events flowing through the message bus  \n- **`render_sessions()`** ‚Äî shows active incident sessions, including signatures, history usage, and impact metadata  \n- **Memory Bank Summary** ‚Äî lists long-term stored incident signatures so you can see which patterns and root causes were retained across runs(implemented later in code)\n\nThese views make it easy to understand agent communication, trace execution flow, and verify that multi-agent coordination is happening correctly.\n","metadata":{}},{"cell_type":"code","source":"# define render_trace_store and render_sessions\nprint(\"üîß defining render_trace_store and render_sessions...\")\n\nfrom IPython.display import display, HTML, Markdown, clear_output\nimport json\n\ndef render_trace_store(limit=200):\n    \"\"\"Display TRACE_STORE (most recent entries). Safe to call repeatedly.\"\"\"\n    try:\n        ts = globals().get(\"TRACE_STORE\", None)\n        if ts is None:\n            display(HTML(\"<i>TRACE_STORE is not defined in globals()</i>\"))\n            print(\"render_trace_store: TRACE_STORE not defined.\")\n            return\n        if not ts:\n            display(HTML(\"<i>No trace messages recorded.</i>\"))\n            print(\"render_trace_store: TRACE_STORE is empty.\")\n            return\n        rows = []\n        for m in ts[-limit:]:\n            payload_str = json.dumps(m.get('payload', {}), default=str, indent=2)\n            rows.append(\n                f\"<tr>\"\n                f\"<td style='vertical-align:top;padding:4px'>{m.get('ts')}</td>\"\n                f\"<td style='vertical-align:top;padding:4px'><b>{m.get('from')}</b> ‚Üí <b>{m.get('to')}</b></td>\"\n                f\"<td style='vertical-align:top;padding:4px'><pre style='white-space:pre-wrap;margin:0'>{payload_str}</pre></td>\"\n                f\"</tr>\"\n            )\n        html = (\n            \"<table style='width:100%;border-collapse:collapse' border=1>\"\n            \"<tr style='background:#f6f6f6'><th>ts</th><th>route</th><th>payload</th></tr>\"\n            + \"\".join(rows) + \"</table>\"\n        )\n        display(HTML(html))\n        print(f\"render_trace_store: displayed {min(len(ts), limit)} trace entries (total stored: {len(ts)}).\")\n    except Exception as e:\n        print(\"render_trace_store: error:\", e)\n        traceback.print_exc()\n\ndef render_sessions():\n    \"\"\"Display SESSIONS dict content (compact).\"\"\"\n    try:\n        sess = globals().get(\"SESSIONS\", None)\n        if sess is None:\n            display(HTML(\"<i>SESSIONS is not defined in globals()</i>\"))\n            print(\"render_sessions: SESSIONS not defined.\")\n            return\n        if not sess:\n            display(HTML(\"<i>No active sessions.</i>\"))\n            print(\"render_sessions: SESSIONS is empty.\")\n            return\n        for sid, s in sess.items():\n            hdr = f\"<h4>Session: {sid} ‚Äî stage: {s.get('stage')}</h4>\"\n            meta = {\n                \"signature\": s.get(\"signature\"),\n                \"history_len\": len(s.get(\"history\") or []),\n                \"impact\": s.get(\"impact\")\n            }\n            display(HTML(hdr))\n            display(HTML(f\"<pre>{json.dumps(meta, indent=2, default=str)}</pre>\"))\n        print(f\"render_sessions: displayed {len(sess)} sessions.\")\n    except Exception as e:\n        print(\"render_sessions: error:\", e)\n        traceback.print_exc()\n\n# expose to globals (redundant but explicit)\nglobals()['render_trace_store'] = render_trace_store\nglobals()['render_sessions'] = render_sessions\n\nprint(\"‚úÖ render_trace_store and render_sessions registered.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.737715Z","iopub.execute_input":"2025-11-28T16:58:02.738085Z","iopub.status.idle":"2025-11-28T16:58:02.756811Z","shell.execute_reply.started":"2025-11-28T16:58:02.738054Z","shell.execute_reply":"2025-11-28T16:58:02.755515Z"}},"outputs":[{"name":"stdout","text":"üîß defining render_trace_store and render_sessions...\n‚úÖ render_trace_store and render_sessions registered.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### UI dependencies & environment checks\n\nDetect & (only if missing) install `ipywidgets` and bring in display helpers.\nThis cell will not reinstall if `ipywidgets` is already available.\n","metadata":{}},{"cell_type":"code","source":"# UI dependencies & environment checks\nprint(\"üîé UI deps check: verifying ipywidgets and display utilities...\")\n\nimport importlib, sys, subprocess\n\ndef ensure_package(pkg_name, import_name=None, version_spec=None):\n    \"\"\"\n    Ensure a package is importable. If not installed, try pip installing it.\n    Returns True if import succeeded, False otherwise.\n    \"\"\"\n    import_name = import_name or pkg_name\n    try:\n        importlib.import_module(import_name)\n        print(f\"‚úÖ {import_name} already available.\")\n        return True\n    except Exception as e:\n        print(f\"‚ö†Ô∏è {import_name} not found ({e}). Attempting to install...\")\n        try:\n            cmd = [sys.executable, \"-m\", \"pip\", \"install\", pkg_name] + ([version_spec] if version_spec else [])\n            subprocess.check_call(cmd)\n            importlib.invalidate_caches()\n            importlib.import_module(import_name)\n            print(f\"‚úÖ Successfully installed and imported {import_name}.\")\n            return True\n        except Exception as ie:\n            print(f\"‚ùå Failed to install {pkg_name}: {ie}\")\n            return False\n\n# Only install ipywidgets if missing\n_ok_widgets = ensure_package(\"ipywidgets\", \"ipywidgets\", None)\n\n# Bring common display helpers into scope (safe repeated import)\ntry:\n    from IPython.display import display, Markdown, HTML, clear_output\n    from pathlib import Path\n    print(\"‚úÖ IPython.display and Path available.\")\nexcept Exception as e:\n    print(\"‚ùå Could not import IPython.display or Path:\", e)\n\n# Expose UI_AVAILABLE flag for downstream cells\nUI_AVAILABLE = _ok_widgets\nprint(f\"UI_AVAILABLE = {UI_AVAILABLE}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.758410Z","iopub.execute_input":"2025-11-28T16:58:02.758810Z","iopub.status.idle":"2025-11-28T16:58:02.785316Z","shell.execute_reply.started":"2025-11-28T16:58:02.758785Z","shell.execute_reply":"2025-11-28T16:58:02.784240Z"}},"outputs":[{"name":"stdout","text":"üîé UI deps check: verifying ipywidgets and display utilities...\n‚úÖ ipywidgets already available.\n‚úÖ IPython.display and Path available.\nUI_AVAILABLE = True\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Optional: Previously uploaded file tracking\n","metadata":{}},{"cell_type":"markdown","source":"### UI Controls & Helpers\n\nCreate widgets (dropdown, file uploader, run/inspect buttons) and small helpers:\n- make_save_path_for_uploaded()\n- approve_and_save_local()\nThis cell uses the same behavior as your original single-cell UI; it only defines helpers and widgets.\n","metadata":{}},{"cell_type":"code","source":"# UI Controls & Helpers (Clean Version)\nprint(\"üß© UI: creating controls & helpers (no duplicate imports)...\")\n\n# widgets are available only if ipywidgets import succeeded earlier\nif not UI_AVAILABLE:\n    print(\"‚ö†Ô∏è ipywidgets unavailable ‚Äî UI widgets will not be created.\")\nelse:\n    import ipywidgets as widgets  # safe even if already imported\n    from pathlib import Path\n\n    # UI controls (matching your original names)\n    sample_dropdown = widgets.Dropdown(\n        options=[\"-- select demo sample --\"] + list(SYNTHETIC_LOGS.keys()),\n        description=\"Demo:\"\n    )\n    file_uploader = widgets.FileUpload(accept=\".log,.txt,.json,.ndjson,.csv\", multiple=False)\n    run_button = widgets.Button(description=\"Run Diagnosis\", button_style=\"success\")\n    inspect_button = widgets.Button(description=\"Inspect\", button_style=\"\")\n    out_area = widgets.Output()\n\n    print(\"üîß Ensuring make_save_path_for_uploaded is available...\")\n    if 'make_save_path_for_uploaded' not in globals():\n        def make_save_path_for_uploaded(filename, out_dir=\"submission_reports\"):\n            safe = filename.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n            suffix = int(time.time())\n            os.makedirs(out_dir, exist_ok=True)\n            stem = Path(safe).stem\n            ext = Path(safe).suffix or \".log\"\n            fname = f\"{stem}_{suffix}{ext}\"\n            path = os.path.join(out_dir, fname)\n            return path\n\n    print(\"üîß Ensuring approve_and_save_local is available...\")\n    if 'approve_and_save_local' not in globals():\n        def approve_and_save_local(incident_id, uploaded_bytes=None, uploaded_filename=None, out_dir=\"submission_reports\"):\n            # Removed debug print: \"attempting to save...\"\n            sess = SESSIONS.get(incident_id)\n            if not sess or not sess.get(\"report\"):\n                print(\"Error: No session or report found to save.\")\n                return None\n\n            md = sess[\"report\"][\"md\"]\n            rep_json = sess[\"report\"][\"json\"]\n\n            out = TOOLS[\"save_report\"].run(\n                {\"report_md\": md, \"report_json\": rep_json, \"incident_id\": incident_id, \"out_dir\": out_dir}\n            )\n            sess[\"saved_info\"] = out\n            sess[\"stage\"] = \"approved\"\n            SESSIONS[incident_id] = sess\n            \n            # Removed debug print: \"report saved -> {dict}\"\n\n            uploaded_saved_path = None\n            if uploaded_bytes is not None:\n                try:\n                    uploaded_saved_path = make_save_path_for_uploaded(\n                        uploaded_filename or f\"uploaded_{incident_id}\",\n                        out_dir=out_dir\n                    )\n                    with open(uploaded_saved_path, \"wb\") as fh:\n                        fh.write(uploaded_bytes)\n                except Exception as e:\n                    uploaded_saved_path = f\"FAILED_TO_SAVE: {e}\"\n                    print(f\"Error saving uploaded file: {e}\")\n\n            return out, uploaded_saved_path\n\n    print(\"üîß Ensuring wire_save_controls is available...\")\n    def wire_save_controls(incident_id, uploaded_filename=None, uploaded_bytes=None):\n        try:\n            if not globals().get(\"UI_AVAILABLE\", False):\n                return\n\n            approval_checkbox = widgets.Checkbox(\n                description=\"I confirm and approve saving the report\",\n                indent=False,\n                value=False\n            )\n            save_btn = widgets.Button(description=\"Save Report\", button_style=\"primary\", disabled=True)\n\n            info_html = widgets.HTML(value=f\"<small>Incident id: <b>{incident_id}</b></small>\")\n            ctrl = widgets.HBox([approval_checkbox, save_btn])\n            display(info_html)\n            display(ctrl)\n\n            def on_check(change):\n                save_btn.disabled = not approval_checkbox.value\n\n            approval_checkbox.observe(on_check, names=\"value\")\n\n            def on_save(b):\n                with out_area:\n                    # Removed debug prints (\"handler triggered\", \"checkbox value\")\n                    \n                    # Enforce approval UI contract\n                    try:\n                        if not approval_checkbox.value:\n                            display(HTML(\"<b style='color:red'>Please check the approval box before saving.</b>\"))\n                            return\n                    except Exception as e:\n                        display(HTML(f\"<b>Save failed (internal error):</b> {e}\"))\n                        return\n\n                    # Attempt to save\n                    try:\n                        result = approve_and_save_local(\n                            incident_id,\n                            uploaded_bytes=uploaded_bytes,\n                            uploaded_filename=uploaded_filename,\n                            out_dir=\"submission_reports\"\n                        )\n                    except Exception as e:\n                        display(HTML(f\"<b>Save failed (exception):</b> {e}\"))\n                        return\n\n                    if not result:\n                        display(HTML(\"<b>Save failed:</b> no session or report found.\"))\n                        return\n\n                    out, uploaded_saved_path = result\n                    \n                    # ONLY display the clean success message\n                    success_msg = (\n                        f\"<div style='background-color:#e6fffa; padding:10px; border-radius:5px; border:1px solid #b2f5ea;'>\"\n                        f\"<b style='color:#285e61;'>‚úÖ Report saved successfully!</b><br/>\"\n                        f\"üìÑ MD: <code>{out.get('md_path')}</code><br/>\"\n                        f\"üìä JSON: <code>{out.get('json_path')}</code>\"\n                    )\n                    \n                    if uploaded_saved_path:\n                        success_msg += f\"<br/>üìÇ Uploaded File: <code>{uploaded_saved_path}</code>\"\n                    \n                    success_msg += \"</div>\"\n                    \n                    display(HTML(success_msg))\n\n                    save_btn.disabled = True\n                    approval_checkbox.disabled = True\n\n            save_btn.on_click(on_save)\n\n        except Exception as e:\n            print(\"‚ùå Failed to initialize wire_save_controls:\", e)\n\nprint(\"üß© UI Controls & Helpers ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.786666Z","iopub.execute_input":"2025-11-28T16:58:02.787026Z","iopub.status.idle":"2025-11-28T16:58:02.829287Z","shell.execute_reply.started":"2025-11-28T16:58:02.786996Z","shell.execute_reply":"2025-11-28T16:58:02.828097Z"}},"outputs":[{"name":"stdout","text":"üß© UI: creating controls & helpers (no duplicate imports)...\nüîß Ensuring make_save_path_for_uploaded is available...\nüîß Ensuring approve_and_save_local is available...\nüîß Ensuring wire_save_controls is available...\nüß© UI Controls & Helpers ready.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### Run handler, inspector & UI display\n\nDefines run_from_ui(), inspect_ui(), wires callbacks (idempotent), and renders the UI row.\nThis cell avoids re-importing modules, and prints debug lines as actions occur.\n","metadata":{}},{"cell_type":"code","source":"#### Run handler, inspector & UI display\nprint(\"‚ñ∂Ô∏è UI: setting up run handler, inspector, hooking callbacks, and rendering UI...\")\n\ndef _extract_uploaded_bytes_fallback(u):\n    \"\"\"\n    Robust, self-contained extractor for ipywidgets.FileUpload-like values.\n    \"\"\"\n    if u is None:\n        return None\n    try:\n        # direct bytes-like\n        if isinstance(u, (bytes, bytearray)):\n            return bytes(u)\n        if isinstance(u, memoryview):\n            return u.tobytes()\n\n        # dict mapping filename -> info (common FileUpload.value)\n        if isinstance(u, dict):\n            # pick first file\n            for fname, info in u.items():\n                if isinstance(info, dict):\n                    cont = info.get(\"content\") or info.get(\"data\") or info.get(\"content_bytes\")\n                    if isinstance(cont, (bytes, bytearray)):\n                        return bytes(cont)\n                    if isinstance(cont, memoryview):\n                        return cont.tobytes()\n                    for k in (\"content\",\"data\",\"body\"):\n                        val = info.get(k)\n                        if isinstance(val, (bytes, bytearray)):\n                            return bytes(val)\n                    if \"metadata\" in info and isinstance(info[\"metadata\"], dict):\n                        pass\n                if isinstance(info, (bytes, bytearray)):\n                    return bytes(info)\n            return None\n\n        # list/tuple possibilities\n        if isinstance(u, (list, tuple)) and len(u) > 0:\n            first = u[0]\n            if isinstance(first, (list, tuple)) and len(first) >= 2:\n                info = first[1]\n                if isinstance(info, dict):\n                    cont = info.get(\"content\") or info.get(\"data\")\n                    if isinstance(cont, (bytes, bytearray)):\n                        return bytes(cont)\n                if isinstance(info, (bytes, bytearray)):\n                    return bytes(info)\n            if isinstance(first, dict):\n                cont = first.get(\"content\") or first.get(\"data\")\n                if isinstance(cont, (bytes, bytearray)):\n                    return bytes(cont)\n            if isinstance(first, (bytes, bytearray)):\n                return bytes(first)\n\n        return str(u).encode(\"utf-8\")\n    except Exception as e:\n        print(\"‚ùó _extract_uploaded_bytes_fallback error:\", e)\n        try:\n            return str(u).encode(\"utf-8\")\n        except:\n            return None\n\n# Primary run handler (same behavior as original but robust extraction)\ndef run_from_ui(_):\n    out_area.clear_output()\n    with out_area:\n        chosen = sample_dropdown.value if 'sample_dropdown' in globals() else None\n        uploaded = file_uploader.value if 'file_uploader' in globals() else None\n\n        # Use local robust extractor first; fall back to previously-defined one if present\n        uploaded_bytes = None\n        try:\n            if uploaded:\n                if 'extract_bytes_from_upload' in globals() and callable(globals().get('extract_bytes_from_upload')):\n                    try:\n                        uploaded_bytes = extract_bytes_from_upload(uploaded)\n                        print(\"run_from_ui: used existing extract_bytes_from_upload ->\", \"present\" if uploaded_bytes else \"none\")\n                    except Exception as e:\n                        print(\"run_from_ui: existing extract_bytes_from_upload raised error, falling back:\", e)\n                        uploaded_bytes = _extract_uploaded_bytes_fallback(uploaded)\n                        print(\"run_from_ui: fallback extractor ->\", \"present\" if uploaded_bytes else \"none\")\n                else:\n                    uploaded_bytes = _extract_uploaded_bytes_fallback(uploaded)\n                    print(\"run_from_ui: fallback extractor ->\", \"present\" if uploaded_bytes else \"none\")\n            else:\n                print(\"run_from_ui: no uploaded value detected (file_uploader.value is empty)\")\n                uploaded_bytes = None\n        except Exception as e:\n            print(\"run_from_ui: extraction failed with unexpected error:\", e)\n            uploaded_bytes = None\n\n        uploaded_fname = None\n        try:\n            if isinstance(uploaded, dict):\n                uploaded_fname = next(iter(uploaded.keys()), None)\n            elif isinstance(uploaded, (list,tuple)) and len(uploaded) > 0:\n                first = uploaded[0]\n                if isinstance(first, (list,tuple)) and len(first) > 0:\n                    uploaded_fname = first[0]\n                elif isinstance(first, dict):\n                    uploaded_fname = first.get(\"name\") or first.get(\"filename\") or None\n            print(\"run_from_ui: uploaded filename resolved ->\", uploaded_fname)\n        except Exception:\n            uploaded_fname = None\n\n        # Main run logic\n        results = None\n        if uploaded_bytes:\n            try:\n                parsed_lines = parse_uploaded_file_bytes(uploaded_bytes) if 'parse_uploaded_file_bytes' in globals() else uploaded_bytes.decode(\"utf-8\", errors=\"replace\").splitlines()\n            except Exception as e:\n                print(\"run_from_ui: parse_uploaded_file_bytes failed, falling back to naive decode:\", e)\n                parsed_lines = uploaded_bytes.decode(\"utf-8\", errors=\"replace\").splitlines()\n\n            detected = detect_scenario_from_lines(parsed_lines) if 'detect_scenario_from_lines' in globals() else None\n\n            if detected:\n                scenario_key = detected\n            elif chosen and chosen in SYNTHETIC_LOGS and chosen != \"-- select demo sample --\":\n                scenario_key = chosen\n                detected = None\n            else:\n                scenario_key = next(iter(SYNTHETIC_LOGS.keys()))\n\n            print(f\"Running diagnosis on uploaded file '{uploaded_fname or 'uploaded'}' ‚Äî detected scenario: {detected or 'none'} (using scenario key: {scenario_key})\")\n            results = run_offline_samples(samples={scenario_key: uploaded_bytes}, show=False, save_reports=False, out_dir=\"submission_reports\")\n\n        elif chosen and chosen in SYNTHETIC_LOGS and chosen != \"-- select demo sample --\":\n            print(f\"Running demo sample: {chosen}\")\n            results = run_offline_samples(samples={chosen: (\"\\n\".join(SYNTHETIC_LOGS[chosen])).encode(\"utf-8\")}, show=False, save_reports=False, out_dir=\"submission_reports\")\n\n        else:\n            print(\"Select a demo sample or upload a log file.\")\n            prev_path = globals().get(\"UPLOADED_FILE_PATH\")\n            prev_url  = globals().get(\"UPLOADED_FILE_URL\")\n            if prev_path and prev_url and Path(prev_path).exists():\n                display(HTML(f\"<small>Previously uploaded file (for reference only): <a href='{prev_url}' target='_blank'>{prev_path}</a></small>\"))\n            else:\n                print(\"No previously uploaded file available.\")\n            return\n\n        if not results:\n            print(\"No results returned.\")\n            return\n\n        incident_id, md, rep_json, saved_info = results[0]\n\n        if not md:\n            print(\"No draft report produced.\")\n            return\n\n        print(\"\\nDraft produced. Click 'Save Report' below to persist the report (and uploaded file if present).\")\n        display(Markdown(md)) \n\n        # ---  EXPLICITLY RENDER PDF DOWNLOAD BUTTON ---\n        if 'render_pdf_download_button' in globals():\n            # UPDATED: Passing rep_json so we get the Professional Content\n            # The COLOR will be blue because we updated the utility function above.\n            pdf_button_html = render_pdf_download_button(incident_id, rep_json)\n            \n            display(HTML(pdf_button_html))\n            \n            if incident_id in SESSIONS and SESSIONS[incident_id].get(\"report\"):\n                SESSIONS[incident_id][\"report\"][\"md\"] += \"\\n\\n\" + pdf_button_html\n        # --- END ---\n        \n        # Wire save controls gracefully\n        if 'wire_save_controls' in globals() and callable(globals().get('wire_save_controls')):\n            try:\n                wire_save_controls(incident_id, uploaded_filename=uploaded_fname, uploaded_bytes=uploaded_bytes)\n            except Exception as e:\n                print(\"run_from_ui: wire_save_controls failed:\", e)\n                if 'approve_and_save_local' in globals() and callable(globals().get('approve_and_save_local')):\n                    print(\"run_from_ui: falling back to approve_and_save_local.\")\n                    out = approve_and_save_local(incident_id, uploaded_bytes=uploaded_bytes, uploaded_filename=uploaded_fname, out_dir=\"submission_reports\")\n                    print(\"approve_and_save_local result:\", out)\n                else:\n                    print(\"No fallback save function available. Please call approve_and_save_local(...) manually to save.\")\n        elif 'approve_and_save_local' in globals() and callable(globals().get('approve_and_save_local')):\n            print(\"run_from_ui: using approve_and_save_local directly (wire_save_controls not present).\")\n            out = approve_and_save_local(incident_id, uploaded_bytes=uploaded_bytes, uploaded_filename=uploaded_fname, out_dir=\"submission_reports\")\n            print(\"approve_and_save_local result:\", out)\n        else:\n            print(\"run_from_ui: neither wire_save_controls nor approve_and_save_local are available. Report cannot be saved via UI automatically.\")\n\n# Inspector UI (unchanged)\ndef inspect_ui(_):\n    out_area.clear_output()\n    with out_area:\n        print(\"=== TRACE STORE (most recent) ===\")\n        if 'render_trace_store' in globals():\n            render_trace_store()\n        else:\n            print(\"render_trace_store not available.\")\n        print(\"\\n=== SESSIONS ===\")\n        if 'render_sessions' in globals():\n            render_sessions()\n        else:\n            print(\"render_sessions not available.\")\n        print(\"\\n=== MEMORY_BANK SUMMARY ===\")\n        if not globals().get(\"MEMORY_BANK\"):\n            display(HTML(\"<i>Memory bank empty.</i>\"))\n        else:\n            for m in globals().get(\"MEMORY_BANK\",[])[-10:]:\n                display(HTML(f\"<pre>{json.dumps({'incident_id':m.get('incident_id'),'job':m.get('job'),'text_hash':m.get('text_hash'),'created_at':m.get('created_at')}, indent=2)}</pre>\"))\n\n# Hook up callbacks safely (idempotent)\ntry:\n    if 'run_button' in globals() and hasattr(run_button, \"on_click\"):\n        run_button.on_click(run_from_ui)\n    if 'inspect_button' in globals() and hasattr(inspect_button, \"on_click\"):\n        inspect_button.on_click(inspect_ui)\n    print(\"‚úÖ UI: callbacks hooked for run_button and inspect_button (if available).\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è UI: hooking callbacks failed (maybe already hooked):\", e)\n\n# Display UI row if widgets created\nif 'sample_dropdown' in globals() and globals().get(\"UI_AVAILABLE\"):\n    ui_row = widgets.VBox([\n        widgets.HTML(\"<h3>PulseTrace ‚Äî demo UI </h3>\"),\n        widgets.HBox([sample_dropdown, file_uploader, run_button, inspect_button]),\n        out_area,\n        (widgets.HTML(f\"<small>Previously uploaded file (for reference only): <a href='{globals().get('UPLOADED_FILE_URL')}' target='_blank'>{globals().get('UPLOADED_FILE_PATH')}</a></small>\")\n           if globals().get('UPLOADED_FILE_PATH') and globals().get('UPLOADED_FILE_URL') else widgets.HTML(\"<small>No previously uploaded file.</small>\"))\n    ])\n    display(ui_row)\n    print(\"‚úÖ UI: displayed.\")\nelse:\n    print(\"‚ÑπÔ∏è UI: widgets not available; UI row not rendered.\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T16:58:02.830630Z","iopub.execute_input":"2025-11-28T16:58:02.830895Z","iopub.status.idle":"2025-11-28T16:58:02.884223Z","shell.execute_reply.started":"2025-11-28T16:58:02.830874Z","shell.execute_reply":"2025-11-28T16:58:02.882952Z"},"trusted":true},"outputs":[{"name":"stdout","text":"‚ñ∂Ô∏è UI: setting up run handler, inspector, hooking callbacks, and rendering UI...\n‚úÖ UI: callbacks hooked for run_button and inspect_button (if available).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<h3>PulseTrace ‚Äî demo UI </h3>'), HBox(children=(Dropdown(description='Demo:', opti‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"699df9ea7f5841d78d4a03d9f2d5ea8b"}},"metadata":{}},{"name":"stdout","text":"‚úÖ UI: displayed.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## üöÄ Agent Deployment\n\nThe PulseTrace multi-agent system is designed with a **hybrid architecture**, leveraging **online Gemini reasoning** for complex tasks (like report synthesis) and **offline custom tools** for deterministic data operations (like log and schema lookups).\n\nWe are deploying this entire hybrid system to **Vertex AI Agent Engine** (Reasoning Engine). This specialized, fully managed runtime provides the necessary infrastructure for hosting reasoning agents, managing sessions, and providing deep observability, which is critical for a production RCA tool.\n\nWe configure the environment by relying on **Google Cloud SDK** credentials, which automatically handles project authentication based on environment variables (a standard practice for production code).","metadata":{}},{"cell_type":"markdown","source":"## ‚òÅÔ∏è Phase 1: Cloud Deployment Configuration\n\nNow that we have verified the PulseTrace logic locally using the interactive UI, we are ready to deploy it to **Google Cloud Vertex AI Agent Engine**.\n\nThis section configures the specific cloud resources required for hosting:\n1.  Verifying Cloud Credentials.\n2.  Setting the target Project ID and Region.\n3.  Enabling the specific APIs required for the Agent Engine runtime.","metadata":{}},{"cell_type":"code","source":"# Import Deployment Libraries & Verify Auth\nimport os\nimport time\nimport vertexai\nfrom kaggle_secrets import UserSecretsClient\nfrom vertexai.preview import reasoning_engines\n\nprint(\"‚úÖ Deployment libraries imported.\")\n\n# Verify Cloud Credentials are loaded\ntry:\n    user_secrets = UserSecretsClient()\n    user_credential = user_secrets.get_gcloud_credential()\n    user_secrets.set_tensorflow_credential(user_credential)\n    print(\"‚úÖ Cloud credentials verified.\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Authentication Error: {e}\")\n    print(\"Please ensure 'Google Cloud SDK' is enabled in Add-ons.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:02.885794Z","iopub.execute_input":"2025-11-28T16:58:02.887153Z","iopub.status.idle":"2025-11-28T16:58:12.624754Z","shell.execute_reply.started":"2025-11-28T16:58:02.887115Z","shell.execute_reply":"2025-11-28T16:58:12.623277Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Deployment libraries imported.\n‚úÖ Cloud credentials verified.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### 1.2 Configure Project & Region\nWe need to define which Google Cloud Project will host the agent. We also export these variables to the system environment so the deployment tools can access them automatically.","metadata":{}},{"cell_type":"code","source":"# Configure Project & Region\n# --- CONFIGURATION ---\nPROJECT_ID = \"your-project-id\"  # ADD your Project ID here\nREGION = \"us-central1\" # We use us-central1 for maximum stability\n# ---------------------\n\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\nos.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"global\"\n\nif PROJECT_ID == \"your-project-id\":\n    raise ValueError(\"‚ö†Ô∏è Please replace 'your-project-id' with your actual Google Cloud Project ID.\")\n\nprint(\"‚úÖ Target Project: Configured\")\nprint(f\"‚úÖ Target Region: {REGION}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T17:19:17.641746Z","iopub.execute_input":"2025-11-28T17:19:17.642102Z","iopub.status.idle":"2025-11-28T17:19:17.649029Z","shell.execute_reply.started":"2025-11-28T17:19:17.642077Z","shell.execute_reply":"2025-11-28T17:19:17.647758Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Target Project: Configured\n‚úÖ Target Region: us-central1\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"### 1.3 Enable Required Cloud APIs\nThe **Agent Engine** requires several specific Google Cloud APIs to be active. \n\nFor this tutorial, you'll need to enable the following APIs in the Google Cloud Console:\n* **Vertex AI API** (`aiplatform.googleapis.com`)\n* **Cloud Storage API** (`storage.googleapis.com`)\n* **Cloud Logging API** (`logging.googleapis.com`)\n* **Cloud Monitoring API** (`monitoring.googleapis.com`)\n* **Cloud Trace API** (`cloudtrace.googleapis.com`)\n* **Telemetry API** (`clouderrorreporting.googleapis.com`)\n\nYou can [use this link to open the Google Cloud Console](https://console.cloud.google.com/apis/library) and follow the steps there to enable these APIs manually.","metadata":{}},{"cell_type":"code","source":"# Initialize Vertex AI SDK\n# Once you have enabled the APIs above, run this cell to verify connectivity.\n\nprint(f\"‚öôÔ∏è Initializing Vertex AI SDK for project {PROJECT_ID} in {REGION}...\")\n\ntry:\n    vertexai.init(project=PROJECT_ID, location=REGION)\n    print(\"‚úÖ Vertex AI SDK Initialized successfully.\")\n    print(\"   (This confirms the Vertex AI API is enabled and accessible).\")\nexcept Exception as e:\n    print(f\"‚ùå Initialization failed: {e}\")\n    print(\"   Please double-check that the 'Vertex AI API' is enabled in the Console.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:12.634652Z","iopub.execute_input":"2025-11-28T16:58:12.634991Z","iopub.status.idle":"2025-11-28T16:58:12.662602Z","shell.execute_reply.started":"2025-11-28T16:58:12.634961Z","shell.execute_reply":"2025-11-28T16:58:12.661432Z"}},"outputs":[{"name":"stdout","text":"‚öôÔ∏è Initializing Vertex AI SDK for project project-9cada6ab-d137-4c1d-be0 in us-central1...\n‚úÖ Vertex AI SDK Initialized successfully.\n   (This confirms the Vertex AI API is enabled and accessible).\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## üèóÔ∏è Section 2: Create Your Agent with ADK\n\nBefore we deploy, we need a functional agent to host. In this section, we will package the **PulseTrace** logic into a standard ADK Agent format.\n\nThis agent is optimized for production deployment with the following configuration:\n* **Model:** Uses `gemini-2.0-flash-001` for low latency and high-speed reasoning.\n* **Tools:** Includes the `analyze_pipeline_logs` tool to deterministically parse errors.\n* **Persona:** Acts as a Site Reliability Engineer (SRE) specialized in Data Pipelines.\n\nWe'll create the following files and directory structure:\n```text\npulsetrace_adk/\n‚îú‚îÄ‚îÄ agent.py                  # The PulseTrace logic\n‚îú‚îÄ‚îÄ requirements.txt          # Dependencies (ADK, ReportLab)\n‚îú‚îÄ‚îÄ .env                      # Environment Config\n‚îî‚îÄ‚îÄ .agent_engine_config.json # Hardware specs","metadata":{}},{"cell_type":"markdown","source":"### 2.1: Create agent directory\nWe need a clean workspace to package our agent for deployment. We will create a directory named `pulsetrace_adk`.\nAll necessary files will be written into this folder to prepare it for the `adk deploy` command.","metadata":{}},{"cell_type":"code","source":"# Create agent directory\nimport os\n\n# Create the specific folder for your project\n!mkdir -p pulsetrace_adk\nprint(f\"‚úÖ PulseTrace Agent directory created: 'pulsetrace_adk/'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:12.663559Z","iopub.execute_input":"2025-11-28T16:58:12.663827Z","iopub.status.idle":"2025-11-28T16:58:12.815511Z","shell.execute_reply.started":"2025-11-28T16:58:12.663805Z","shell.execute_reply":"2025-11-28T16:58:12.813989Z"}},"outputs":[{"name":"stdout","text":"‚úÖ PulseTrace Agent directory created: 'pulsetrace_adk/'\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"### 2.2: Create requirements file\nThe Agent Engine builds a dedicated environment for your agent. To ensure it runs correctly, we must declare our dependencies.\n\nWe will write a `requirements.txt` file containing:\n* `google-adk`: The core agent framework.\n* `reportlab`: Required by PulseTrace for PDF generation.\n* `opentelemetry-instrumentation-google-genai`: For observability.","metadata":{}},{"cell_type":"code","source":"%%writefile pulsetrace_adk/requirements.txt\ngoogle-adk\nopentelemetry-instrumentation-google-genai\nreportlab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:12.817085Z","iopub.execute_input":"2025-11-28T16:58:12.817432Z","iopub.status.idle":"2025-11-28T16:58:12.826170Z","shell.execute_reply.started":"2025-11-28T16:58:12.817389Z","shell.execute_reply":"2025-11-28T16:58:12.824880Z"}},"outputs":[{"name":"stdout","text":"Overwriting pulsetrace_adk/requirements.txt\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"### 2.3: Create environment configuration\nWe need to provide the agent with the necessary cloud configuration settings.\n\nWe will write a `.env` file that sets the cloud location to `global` and explicitly enables the Vertex AI backend. Crucially, we inject your **Project ID** here so the agent can authenticate immediately.","metadata":{}},{"cell_type":"code","source":"# reate environment configuration\n# We use Python to write this file so we can inject the PROJECT_ID variable safely.\n\nenv_content = f\"\"\"\nGOOGLE_CLOUD_PROJECT=\"{PROJECT_ID}\"\nGOOGLE_CLOUD_LOCATION=\"global\"\nGOOGLE_GENAI_USE_VERTEXAI=1\n\"\"\"\n\nwith open(\"pulsetrace_adk/.env\", \"w\") as f:\n    f.write(env_content.strip())\n\nprint(\"‚úÖ .env configuration created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:12.827308Z","iopub.execute_input":"2025-11-28T16:58:12.827624Z","iopub.status.idle":"2025-11-28T16:58:12.848059Z","shell.execute_reply.started":"2025-11-28T16:58:12.827602Z","shell.execute_reply":"2025-11-28T16:58:12.846943Z"}},"outputs":[{"name":"stdout","text":"‚úÖ .env configuration created.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"### 2.4: Create agent code\nWe will now generate the `agent.py` file. This script defines the behavior of **PulseTrace**.\n\nIt includes:\n1.  **The Tool (`analyze_pipeline_logs`)**: Your custom logic that parses log lines to find schema drifts or missing partitions.\n2.  **The Agent (`root_agent`)**: The ADK wrapper that connects your tool to the Gemini 2.0 model.","metadata":{}},{"cell_type":"code","source":"%%writefile pulsetrace_adk/agent.py\nfrom google.adk.agents import Agent\nimport vertexai\nimport os\nimport re\nimport time\nimport uuid\nfrom collections import Counter\n\n# Initialize Vertex AI\nvertexai.init(\n    project=os.environ[\"GOOGLE_CLOUD_PROJECT\"],\n    location=os.environ[\"GOOGLE_CLOUD_LOCATION\"],\n)\n\n# --- 1. THE TOOL ---\ndef analyze_pipeline_logs(log_content: str) -> dict:\n    \"\"\"\n    Analyzes raw data pipeline logs to identify root causes of failures.\n    \n    This is a TOOL that the agent can call when users provide logs.\n    For this demo, it parses the input text directly using deterministic patterns.\n    \n    Args:\n        log_content: The raw text of the logs.\n        \n    Returns:\n        dict: A structured RCA report containing root cause and severity.\n    \"\"\"\n    # Logic to parse logs\n    log_lines = log_content.splitlines()\n    text_blob = \"\\n\".join(log_lines).lower()\n    \n    job = \"unknown\"\n    m = re.findall(r\"job=([A-Za-z0-9_\\-\\.]+)\", text_blob)\n    if m: job = Counter(m).most_common(1)[0][0]\n        \n    if \"cannot cast\" in text_blob:\n        rca = f\"Schema Drift in job '{job}': Type Mismatch\"\n        sev = \"MEDIUM\"\n        remediation = [\"Update schema definition\", \"Backfill affected partitions\"]\n    elif \"no files found\" in text_blob:\n        rca = f\"Missing Partition Data for job '{job}'\"\n        sev = \"HIGH\"\n        remediation = [\"Check upstream producer\", \"Verify S3 paths\"]\n    elif \"valueerror\" in text_blob:\n        rca = f\"Data Quality Violation in job '{job}'\"\n        sev = \"MEDIUM\"\n        remediation = [\"Add data validation rules\", \"Filter invalid rows\"]\n    else:\n        rca = \"Unknown Error - Pattern not recognized\"\n        sev = \"LOW\"\n        remediation = [\"Investigate logs manually\"]\n\n    return {\n        \"incident_id\": f\"cloud-{uuid.uuid4().hex[:6]}\",\n        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"job_detected\": job,\n        \"root_cause\": rca,\n        \"severity\": sev,\n        \"recommended_actions\": remediation\n    }\n\n# --- 2. THE AGENT DEFINITION ---\nroot_agent = Agent(\n    name=\"pulsetrace_agent\",\n    model=\"gemini-2.0-flash-001\", # Using the latest fast model\n    description=\"An AI Site Reliability Engineer that analyzes data pipeline logs.\",\n    instruction=\"\"\"\n    You are PulseTrace, an automated Root Cause Analysis system.\n    \n    When a user provides log data:\n    1. Call the 'analyze_pipeline_logs' tool with the log content.\n    2. Return the JSON summary provided by the tool directly to the user.\n    3. If the tool identifies a HIGH severity issue, emphasize the remediation steps.\n    \"\"\",\n    tools=[analyze_pipeline_logs]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:12.849177Z","iopub.execute_input":"2025-11-28T16:58:12.849472Z","iopub.status.idle":"2025-11-28T16:58:12.867781Z","shell.execute_reply.started":"2025-11-28T16:58:12.849434Z","shell.execute_reply":"2025-11-28T16:58:12.866500Z"}},"outputs":[{"name":"stdout","text":"Overwriting pulsetrace_adk/agent.py\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## ‚òÅÔ∏è Section 3: Deploy to Agent Engine\n\nADK supports multiple deployment platforms. You'll be deploying to **Vertex AI Agent Engine** in this notebook.\n\nThis fully managed service provides:\n* **Auto-scaling:** Scales to zero when not in use.\n* **Session management:** Built-in memory handling.\n* **Easy deployment:** Uses the simple `adk deploy` command.","metadata":{}},{"cell_type":"markdown","source":"### 3.1: Create deployment configuration\nThe `.agent_engine_config.json` file controls the deployment settings.\n\nWe define the resource limits here. For PulseTrace, we allocate **2Gi of memory** to ensure it can process log chunks efficiently.","metadata":{}},{"cell_type":"code","source":"%%writefile pulsetrace_adk/.agent_engine_config.json\n{\n    \"min_instances\": 0,\n    \"max_instances\": 1,\n    \"resource_limits\": {\"cpu\": \"1\", \"memory\": \"2Gi\"}\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:12.868943Z","iopub.execute_input":"2025-11-28T16:58:12.869364Z","iopub.status.idle":"2025-11-28T16:58:12.892079Z","shell.execute_reply.started":"2025-11-28T16:58:12.869337Z","shell.execute_reply":"2025-11-28T16:58:12.890846Z"}},"outputs":[{"name":"stdout","text":"Overwriting pulsetrace_adk/.agent_engine_config.json\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"### 3.2: Select deployment region\nAgent Engine is available in specific regions. We'll select `us-central1` for this deployment as it is the primary region for Reasoning Engine features.","metadata":{}},{"cell_type":"code","source":"# Select deployment region\nregions_list = [\"us-central1\", \"europe-west1\", \"europe-west4\", \"us-east4\", \"us-west1\"]\n\n# We select us-central1 for stability\ndeployed_region = \"us-central1\"\nprint(f\"‚úÖ Selected deployment region: {deployed_region}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:12.892834Z","iopub.execute_input":"2025-11-28T16:58:12.893100Z","iopub.status.idle":"2025-11-28T16:58:12.911732Z","shell.execute_reply.started":"2025-11-28T16:58:12.893080Z","shell.execute_reply":"2025-11-28T16:58:12.910680Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Selected deployment region: us-central1\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"### 3.3: Deploy the agent\nThis uses the **ADK CLI** to deploy your agent to Agent Engine.\n\n**Note:** This process typically takes **3-5 minutes**. Please wait until you see the message `‚úÖ Created agent engine` in the output logs.","metadata":{}},{"cell_type":"code","source":"# Deploy the agent\nprint(f\"üöÄ Deploying PulseTrace to: {deployed_region}...\")\n\n!adk deploy agent_engine \\\n    --project=$PROJECT_ID \\\n    --region=$deployed_region \\\n    pulsetrace_adk \\\n    --agent_engine_config_file=pulsetrace_adk/.agent_engine_config.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:58:12.912742Z","iopub.execute_input":"2025-11-28T16:58:12.913061Z","iopub.status.idle":"2025-11-28T17:02:10.071151Z","shell.execute_reply.started":"2025-11-28T16:58:12.913032Z","shell.execute_reply":"2025-11-28T17:02:10.069306Z"}},"outputs":[{"name":"stdout","text":"üöÄ Deploying PulseTrace to: us-central1...\nStaging all files in: /kaggle/working/pulsetrace_adk_tmp20251128_165839\nCopying agent source code...\nCopying agent source code complete.\nResolving files and dependencies...\nReading agent engine config from pulsetrace_adk/.agent_engine_config.json\nReading environment variables from /kaggle/working/pulsetrace_adk/.env\n\u001b[33mIgnoring GOOGLE_CLOUD_PROJECT in .env as `--project` was explicitly passed and takes precedence\u001b[0m\n\u001b[33mIgnoring GOOGLE_CLOUD_LOCATION in .env as `--region` was explicitly passed and takes precedence\u001b[0m\nInitializing Vertex AI...\nVertex AI initialized.\nCreated pulsetrace_adk_tmp20251128_165839/agent_engine_app.py\nFiles and dependencies resolved\nDeploying to agent engine...\nINFO:vertexai_genai.agentengines:Creating in-memory tarfile of source_packages\nINFO:vertexai_genai.agentengines:Using agent framework: google-adk\nINFO:vertexai_genai.agentengines:View progress and logs at https://console.cloud.google.com/logs/query?project=project-9cada6ab-d137-4c1d-be0.\nINFO:vertexai_genai.agentengines:Agent Engine created. To use it in another session:\nINFO:vertexai_genai.agentengines:agent_engine=client.agent_engines.get(name='projects/1088013112886/locations/us-central1/reasoningEngines/5571845542499057664')\n\u001b[32m‚úÖ Created agent engine: projects/1088013112886/locations/us-central1/reasoningEngines/5571845542499057664\u001b[0m\nCleaning up the temp folder: pulsetrace_adk_tmp20251128_165839\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"## ü§ñ Section 4: Retrieve and Test Your Deployed Agent\n\n### 4.1: Retrieve the deployed agent\nAfter deploying with the CLI, we need to retrieve the agent object in our Python session to interact with it.","metadata":{}},{"cell_type":"code","source":"# Retrieve the deployed agent\nimport vertexai\nfrom vertexai.preview import reasoning_engines\n\n# Initialize Vertex AI\nvertexai.init(project=PROJECT_ID, location=deployed_region)\n\n# List agents\nprint(\"‚è≥ Listing deployed agents...\")\ntry:\n    agents_list = list(reasoning_engines.ReasoningEngine.list())\n    \n    if agents_list:\n        remote_agent = agents_list[0]\n        print(f\"‚úÖ Connected to deployed agent: {remote_agent.resource_name}\")\n    else:\n        print(\"‚ùå No agents found. Please wait 1-2 minutes if deployment just finished.\")\nexcept Exception as e:\n    print(f\"‚ùå Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T17:12:30.029552Z","iopub.execute_input":"2025-11-28T17:12:30.030829Z","iopub.status.idle":"2025-11-28T17:12:30.856931Z","shell.execute_reply.started":"2025-11-28T17:12:30.030785Z","shell.execute_reply":"2025-11-28T17:12:30.855976Z"}},"outputs":[{"name":"stdout","text":"‚è≥ Listing deployed agents...\n‚úÖ Connected to deployed agent: projects/1088013112886/locations/us-central1/reasoningEngines/5571845542499057664\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"### 4.2: Test the deployed agent (REST + SSE)\n\nThis step sends a sample log line to the deployed PulseTrace agent using a REST-based SSE client.  \nThe response is streamed back in real time using `alt=sse`, allowing you to view partial outputs as they are generated.\n","metadata":{}},{"cell_type":"code","source":"# REST SSE client for Vertex AI Agent Engine (streamQuery)\n# Works in Jupyter / Kaggle. Requires google-auth installed (usually present).\nimport json, requests, time\nfrom google.auth.transport.requests import Request\nimport google.auth\n\n# ----- CONFIG -----\n# Replace with the resource_name you printed earlier (the full resource path)\nresource_name = \"resource_name\" #ADD your resource name here\n# Region endpoint (eg. us-central1)\nendpoint_base = \"https://us-central1-aiplatform.googleapis.com/v1\"\n# message to send\nmessage = \"Analyze these logs: 2025-11-24 10:03:02 ERROR job=orders transform TypeError: cannot cast '123.45' to INT\"\nuser_id = \"test_user_01\"\n# Optionally provide a session_id if you already created one; otherwise omit\nsession_id = None  # or \"your_session_id\"\n\n# ----- GET ACCESS TOKEN -----\ncreds, _ = google.auth.default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\ncreds.refresh(Request())\ntoken = creds.token\nif not token:\n    raise RuntimeError(\"Failed to obtain access token. Make sure credentials are configured.\")\n\n# ----- BUILD REQUEST -----\n# Use alt=sse to request Server-Sent Events streaming\nurl = f\"{endpoint_base}/{resource_name}:streamQuery?alt=sse\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {token}\",\n    \"Content-Type\": \"application/json; charset=utf-8\",\n    \"Accept\": \"text/event-stream\",\n}\n\n# The body format: include 'class_method' optionally and 'input' as a JSON struct.\n# ADK / community examples use 'class_method': 'stream_query' and input with user_id/session_id/message\nbody = {\n    \"class_method\": \"stream_query\",\n    \"input\": {\n        \"user_id\": user_id,\n        \"message\": message,\n    }\n}\nif session_id:\n    body[\"input\"][\"session_id\"] = session_id\n\nprint(\"‚û°Ô∏è POST\", url)\nprint(\"‚û°Ô∏è Sending payload:\", json.dumps(body)[:400], \"...\")\nprint(\"‚è≥ Waiting for streamed events... (Ctrl-C to cancel)\\n\")\n\n# ----- STREAM (SSE) -----\nwith requests.post(url, headers=headers, json=body, stream=True, timeout=120) as resp:\n    resp.raise_for_status()\n    # Iterate over the raw lines from the SSE stream\n    for raw_line in resp.iter_lines(decode_unicode=True):\n        # skip keep-alives or empty lines\n        if raw_line is None or raw_line.strip() == \"\":\n            continue\n        # SSE lines usually start with \"data: \"\n        line = raw_line.strip()\n        if line.startswith(\"data:\"):\n            payload = line[len(\"data:\"):].strip()\n        else:\n            payload = line\n\n        # Some server emits \"event: ...\", \"data: ...\", or plain JSON lines.\n        # Try to parse JSON; otherwise just print.\n        try:\n            obj = json.loads(payload)\n            # Pretty-print important fields (adjust as needed)\n            print(json.dumps(obj, indent=2, ensure_ascii=False))\n        except Exception:\n            # not JSON ‚Äî print raw payload (helps debug)\n            print(payload)\n\nprint(\"\\n‚úÖ Stream finished.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T17:12:33.572402Z","iopub.execute_input":"2025-11-28T17:12:33.572735Z","iopub.status.idle":"2025-11-28T17:13:07.914419Z","shell.execute_reply.started":"2025-11-28T17:12:33.572708Z","shell.execute_reply":"2025-11-28T17:13:07.913364Z"}},"outputs":[{"name":"stdout","text":"‚û°Ô∏è POST https://us-central1-aiplatform.googleapis.com/v1/projects/1088013112886/locations/us-central1/reasoningEngines/152220787795820544:streamQuery?alt=sse\n‚û°Ô∏è Sending payload: {\"class_method\": \"stream_query\", \"input\": {\"user_id\": \"test_user_01\", \"message\": \"Analyze these logs: 2025-11-24 10:03:02 ERROR job=orders transform TypeError: cannot cast '123.45' to INT\"}} ...\n‚è≥ Waiting for streamed events... (Ctrl-C to cancel)\n\n{\n  \"model_version\": \"gemini-2.0-flash-001\",\n  \"content\": {\n    \"parts\": [\n      {\n        \"function_call\": {\n          \"id\": \"adk-b41b8031-038c-4895-bdd0-26913db80db7\",\n          \"args\": {\n            \"log_content\": \"2025-11-24 10:03:02 ERROR job=orders transform TypeError: cannot cast '123.45' to INT\"\n          },\n          \"name\": \"analyze_pipeline_logs\"\n        }\n      }\n    ],\n    \"role\": \"model\"\n  },\n  \"finish_reason\": \"STOP\",\n  \"usage_metadata\": {\n    \"candidates_token_count\": 46,\n    \"candidates_tokens_details\": [\n      {\n        \"modality\": \"TEXT\",\n        \"token_count\": 46\n      }\n    ],\n    \"prompt_token_count\": 263,\n    \"prompt_tokens_details\": [\n      {\n        \"modality\": \"TEXT\",\n        \"token_count\": 263\n      }\n    ],\n    \"total_token_count\": 309,\n    \"traffic_type\": \"ON_DEMAND\"\n  },\n  \"avg_logprobs\": -9.942640368219302e-07,\n  \"invocation_id\": \"e-dd98ed33-201f-45ea-988c-1e226f708c9a\",\n  \"author\": \"pulsetrace_agent\",\n  \"actions\": {\n    \"state_delta\": {},\n    \"artifact_delta\": {},\n    \"requested_auth_configs\": {},\n    \"requested_tool_confirmations\": {}\n  },\n  \"long_running_tool_ids\": [],\n  \"id\": \"f1205e3d-2cb6-4591-94b7-7786994308be\",\n  \"timestamp\": 1764349985.817409\n}\n{\n  \"content\": {\n    \"parts\": [\n      {\n        \"function_response\": {\n          \"id\": \"adk-b41b8031-038c-4895-bdd0-26913db80db7\",\n          \"name\": \"analyze_pipeline_logs\",\n          \"response\": {\n            \"incident_id\": \"cloud-5a507f\",\n            \"timestamp\": \"2025-11-28 17:13:06\",\n            \"job_detected\": \"orders\",\n            \"root_cause\": \"Schema Drift in job 'orders': Type Mismatch\",\n            \"severity\": \"MEDIUM\",\n            \"recommended_actions\": [\n              \"Update schema definition\",\n              \"Backfill affected partitions\"\n            ]\n          }\n        }\n      }\n    ],\n    \"role\": \"user\"\n  },\n  \"invocation_id\": \"e-dd98ed33-201f-45ea-988c-1e226f708c9a\",\n  \"author\": \"pulsetrace_agent\",\n  \"actions\": {\n    \"state_delta\": {},\n    \"artifact_delta\": {},\n    \"requested_auth_configs\": {},\n    \"requested_tool_confirmations\": {}\n  },\n  \"id\": \"40aedbf5-a0c3-487b-8cbf-1a3f08df7471\",\n  \"timestamp\": 1764349986.730566\n}\n{\n  \"model_version\": \"gemini-2.0-flash-001\",\n  \"content\": {\n    \"parts\": [\n      {\n        \"text\": \"Here's a summary of the analysis:\\n\\n*   **Root Cause:** Schema Drift in job 'orders': Type Mismatch\\n*   **Severity:** MEDIUM\\n*   **Job Detected:** orders\\n*   **Recommended Actions:**\\n    *   Update schema definition\\n    *   Backfill affected partitions\"\n      }\n    ],\n    \"role\": \"model\"\n  },\n  \"finish_reason\": \"STOP\",\n  \"usage_metadata\": {\n    \"candidates_token_count\": 63,\n    \"candidates_tokens_details\": [\n      {\n        \"modality\": \"TEXT\",\n        \"token_count\": 63\n      }\n    ],\n    \"prompt_token_count\": 374,\n    \"prompt_tokens_details\": [\n      {\n        \"modality\": \"TEXT\",\n        \"token_count\": 374\n      }\n    ],\n    \"total_token_count\": 437,\n    \"traffic_type\": \"ON_DEMAND\"\n  },\n  \"avg_logprobs\": -0.0077755455932919946,\n  \"invocation_id\": \"e-dd98ed33-201f-45ea-988c-1e226f708c9a\",\n  \"author\": \"pulsetrace_agent\",\n  \"actions\": {\n    \"state_delta\": {},\n    \"artifact_delta\": {},\n    \"requested_auth_configs\": {},\n    \"requested_tool_confirmations\": {}\n  },\n  \"id\": \"5b24491f-1ff9-4cc5-bc23-84bc406fb25f\",\n  \"timestamp\": 1764349986.863241\n}\n\n‚úÖ Stream finished.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"## üßπ Section 6: Cleanup\n\n### ‚ö†Ô∏è Important: Prevent unexpected charges\nTo avoid incurring costs for the running agent engine, you should delete the resource when you are done testing.","metadata":{}},{"cell_type":"code","source":"# Cleanup resources\n# Uncomment the lines below to delete the agent\nif 'remote_agent' in locals():\n    print(f\"üóëÔ∏è Deleting Agent: {remote_agent.resource_name}...\")\n    try:\n        remote_agent.delete()\n        print(\"‚úÖ Agent successfully deleted\")\n    except Exception as e:\n        print(f\"‚ùå Deletion error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T17:15:09.416586Z","iopub.execute_input":"2025-11-28T17:15:09.416968Z","iopub.status.idle":"2025-11-28T17:15:10.087384Z","shell.execute_reply.started":"2025-11-28T17:15:09.416941Z","shell.execute_reply":"2025-11-28T17:15:10.086418Z"}},"outputs":[{"name":"stdout","text":"üóëÔ∏è Deleting Agent: projects/1088013112886/locations/us-central1/reasoningEngines/5571845542499057664...\n‚úÖ Agent successfully deleted\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"## üõ†Ô∏è Troubleshooting\n\nThis section helps diagnose issues that may appear while running the PulseTrace demo.  \nUse the points below to understand and resolve common problems.\n\n\n### ‚öôÔ∏è UI Panel Not Showing\nIf the UI dropdown, file uploader, or buttons do not appear:\n\n- `ipywidgets` may not be available in the environment.  \n- Ensure the `UI_AVAILABLE` flag printed during setup shows **True**.\n- If widgets cannot be installed, you can still run PulseTrace manually using:\n  ```python\n  run_offline_samples(...)\n  ```\n\n\n### üßæ No Draft Report Generated\nIf the UI prints **‚ÄúNo draft report produced.‚Äù**:\n\n- The Diagnoser may not have produced a signature (often due to log parsing failures).\n- Ensure your uploaded file contains readable text lines.\n- Use the Inspector UI to verify:\n  - **Trace Store** ‚Üí agent flow reached Advisor  \n  - **Sessions** ‚Üí signature, history, and impact exist\n\n\n### üì° Traces / Sessions Not Visible\nIf the Inspector UI shows:\n\n- `render_trace_store not available`  \n- `render_sessions not available`\n\nThen:\n\n- Confirm that the Trace & Session Rendering Helpers cell was executed.\n- Verify that `TRACE_STORE` and `SESSIONS` were initialized earlier.\n- You may also inspect manually:\n  ```python\n  TRACE_STORE[-5:]\n  SESSIONS.keys()\n  ```\n\n\n### ‚òÅÔ∏è Gemini / Hybrid Mode Errors\nIf Gemini calls fail or hybrid mode doesn‚Äôt activate:\n\n- Ensure `GOOGLE_API_KEY` is added to Kaggle Secrets.\n- Verify the environment setup printed **‚ÄúGemini Mode: ON‚Äù**.\n- In restricted environments, Gemini is automatically disabled.\n- You can also force offline mode by setting:\n  ```python\n  USE_GEMINI = False\n  ```\n\n\n### üìÅ Uploaded File Not Detected\nIf you see:\n\n- **‚Äúno uploaded value detected‚Äù**  \n- or  \n  **‚ÄúSelect a demo sample or upload a log file.‚Äù**\n\nThen:\n\n- Make sure you selected a file *and* clicked **Run Diagnosis** after uploading.\n- Some notebook environments return unusual `FileUpload.value` structures ‚Äî the fallback extractor handles most, but not all malformed objects.\n- Try uploading a simple `.log` or `.txt` file.\n\n\n### ‚õìÔ∏è General Diagnostics\nIf PulseTrace behaves unexpectedly:\n\n- Ensure all previous cells executed without errors.\n- Confirm router behavior by checking Trace Store.\n- Test a synthetic scenario manually:\n  ```python\n  run_offline_samples(show=True)\n  ```\n- If you modified agent code, ensure every agent still defines a valid `on_message()` method.\n\n\n### ‚úÖ Quick Troubleshooting Checklist\n\n- [ ] UI visible (`UI_AVAILABLE == True`)  \n- [ ] Uploaded file detected or sample selected  \n- [ ] Diagnoser produced a signature (`SESSIONS[...][\"signature\"]` exists)  \n- [ ] A2A traces appear in Trace Store  \n- [ ] Advisor produced Markdown draft (`SESSIONS[...][\"report\"][\"md\"]`)  \n- [ ] Gemini optional ‚Äî offline mode fully functional\n","metadata":{}},{"cell_type":"markdown","source":"## ‚ö†Ô∏è Current Limitations\n\nPulseTrace is a functional multi-agent RCA demo, but it operates within a few intentional constraints:\n\n- **Offline-first design**  \n  The system runs fully offline by default using synthetic logs, schemas, and lineage metadata. Real system integrations are not included.\n\n- **Simplified failure patterns**  \n  Scenario detection is optimized for structured log formats and may not generalize to noisy, multi-stage, or unstructured logs.\n\n- **No persistent storage layer**  \n  Reports are displayed in the UI but not permanently written to disk unless the save function is explicitly triggered.\n\n- **Basic agent memory**  \n  Memory stores lightweight incident fingerprints but does not include embeddings, similarity search, or long-term vector memory.\n\n- **Notebook UI dependence**  \n  The interactive UI relies on `ipywidgets`. Environments without widget support must use manual execution via code.\n\n- **Hybrid Gemini mode is optional**  \n  PulseTrace supports live online reasoning via Gemini when an API key is present, but only specific steps (like log summarization) use it currently.\n\nThese limitations allow PulseTrace to stay lightweight and responsive while still demonstrating a complete multi-agent RCA workflow.\n","metadata":{}},{"cell_type":"markdown","source":"## üöÄ What's Next for PulseTrace\n\nPulseTrace already demonstrates a complete multi-agent RCA workflow, but there are several exciting directions to expand the system:\n\n- **Add real integrations**  \n  Connect to actual log stores, schema registries, lineage tools, and monitoring systems instead of offline simulation.\n\n- **Strengthen Gemini hybrid mode**  \n  Route more agent reasoning through Gemini when available and add richer summaries or deeper log insights.\n\n- **Extend the Diagnoser**  \n  Implement anomaly detection, data quality checks, and graph-based propagation logic for more complex failures.\n\n- **Improve the UI**  \n  Add collapsible panels, richer report previews, and a timeline view of agent-to-agent messages.\n\n- **Model memory enhancements**  \n  Store richer historical fingerprints and use them to surface smarter, pattern-based suggestions.\n\nThese additions will help PulseTrace evolve from a demo into a robust, production-grade RCA assistant for data engineering workflows.\n","metadata":{}},{"cell_type":"markdown","source":"## üìù Conclusion\n\nPulseTrace demonstrates how a coordinated multi-agent system can streamline root cause analysis for data pipeline failures. By combining deterministic tools, message-based agent orchestration, hybrid Gemini reasoning, and optional UI interaction, it provides a clear blueprint for building intelligent, modular RCA systems.\n\nThe workflow‚ÄîDetector ‚Üí Diagnoser ‚Üí History Analyzer ‚Üí Impact Analyzer ‚Üí Advisor‚Äîshows how specialized agents can collaborate, exchange context, and synthesize a final explanation that is both actionable and transparent.\n\nAlthough this notebook runs on simulated data, the architecture is designed to extend naturally to real monitoring systems, log stores, lineage platforms, and large-scale data ecosystems. With further enhancements in memory, anomaly detection, and integrations, PulseTrace can evolve into a fully capable, production-ready RCA assistant for modern data engineering teams.\n\n**Thank you for exploring PulseTrace!**\n","metadata":{}}]}